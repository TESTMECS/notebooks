# -*- coding: utf-8 -*-
"""Time Aware-Attention

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12WVELATiXJ23UWo6qErUTy6Na7BTH7LN
"""

# %%
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv  # Import GCNConv
import nltk

nltk.download("wordnet")
nltk.download("omw-1.4")
from nltk.corpus import wordnet as wn
import random
import numpy as np

# %%
# Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# For reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# %%
class GravityField(nn.Module):
    def __init__(self, input_dim=4):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Softplus(),  # ensures phi > 0
        )

    def forward(self, coords):
        """
        coords: Tensor of shape (batch_size, 4) where columns are [t, x, y, z] or equivalent
        returns: Scalar phi > 0 per point
        """
        return self.net(coords).squeeze(-1)  # (batch_size,)


# Patch into your projection loop or loss like:
# Example usage in your ds^2 calculation (for loss or projection)


def minkowski_ds2_with_phi(t1, x1, t2, x2, phi_model):
    delta_t = t1 - t2
    delta_x = x1 - x2
    dx2 = torch.sum(delta_x**2, dim=-1)

    coords = torch.cat(
        [t1.unsqueeze(-1), x1], dim=-1
    )  # assume x1 is (N, 3), so shape becomes (N, 4)
    phi_vals = phi_model(coords)  # (N,)

    ds2 = -phi_vals * delta_t**2 + dx2
    return ds2


# Integration into project_until_convergence:
def project_until_convergence(
    pairs, spatial, time_vec, phi_model, eps1=1e-5, max_passes=10000
):
    """
    Adjusts time_vec so that all parent-child pairs satisfy ds^2 < -eps1.
    The phi_model introduces a gravity-aware time distortion.
    """
    num_passes = 0
    converged = False

    time_vec = time_vec.clone().detach().requires_grad_(False)
    spatial = spatial.clone().detach().requires_grad_(False)

    while not converged and num_passes < max_passes:
        num_passes += 1
        converged = True

        for i, j in pairs:
            t_i, t_j = time_vec[i], time_vec[j]
            x_i, x_j = spatial[i], spatial[j]
            delta_t = t_i - t_j
            delta_x = x_i - x_j
            dx2 = torch.sum(delta_x**2)

            coords_i = torch.cat([t_i.view(1), x_i])
            phi_i = phi_model(coords_i.unsqueeze(0))[0]

            ds2 = -phi_i * delta_t**2 + dx2

            if ds2 >= -eps1:
                # Adjust time_i to be slightly later than time_j
                new_delta_t = torch.sqrt((dx2 + eps1) / phi_i + 1e-9)
                time_vec[i] = t_j + new_delta_t.item()
                converged = False

    return time_vec


# Co-training Phi model


def train_phi_from_ds2_violations(
    phi_model, pairs, spatial, time_vec, eps1=1e-5, num_epochs=200, lr=1e-4
):
    """
    Trains phi_model to reduce ds^2 >= -eps1 violations over a hierarchy of (child, parent) pairs.
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    phi_model.to(device)
    optimizer = torch.optim.Adam(phi_model.parameters(), lr=lr)
    spatial = spatial.to(device)
    time_vec = time_vec.to(device)

    for epoch in range(num_epochs):
        total_loss = 0.0
        phi_model.train()

        for i, j in pairs:
            t_i, t_j = time_vec[i], time_vec[j]
            x_i, x_j = spatial[i], spatial[j]

            delta_t = t_i - t_j
            delta_x = x_i - x_j
            dx2 = torch.sum(delta_x**2)

            coords_i = torch.cat([t_i.view(1), x_i])
            phi_i = phi_model(coords_i.unsqueeze(0))[0]
            ds2 = -phi_i * delta_t**2 + dx2

            loss = F.relu(ds2 + eps1)  # penalize causal violations only
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        if epoch % 10 == 0:
            print(
                f"Epoch {epoch + 1}/{num_epochs}, Avg phi loss: {total_loss / len(pairs):.6f}"
            )

    return phi_model


class GravityAwareGNN(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim):
        super().__init__()
        self.gcn1 = GCNConv(in_dim, hidden_dim)
        self.gcn2 = GCNConv(hidden_dim, out_dim)

    def forward(self, x, edge_index):
        x = F.relu(self.gcn1(x, edge_index))
        return self.gcn2(x, edge_index)


def causal_phi_loss(node_reps, time_vec, edge_index, phi_model, eps1=1e-5):
    loss = 0.0
    for i, j in edge_index.t():
        delta_t = time_vec[i] - time_vec[j]
        delta_x = node_reps[i] - node_reps[j]
        dx2 = torch.sum(delta_x**2)
        phi = phi_model(torch.cat([time_vec[i].view(1), node_reps[i]]).unsqueeze(0))[0]
        ds2 = -phi * delta_t**2 + dx2
        loss += F.relu(ds2 + eps1)
    return loss / edge_index.shape[1]


"""# Step 1: Prepare node features
features = [t, x, y, z, φ] per node

# Step 2: Train φ on causal violations (optional pre-pass)
phi_model = train_phi_from_ds2_violations(...)

# Step 3: Feed features into a GNN
output = gnn(node_features, edge_index)

# Step 4: Use causal curvature loss from φ
loss = prediction_loss + λ * causal_phi_loss(...)
"""


# %%
# Integration into project_until_convergence:
def project_until_convergence(
    pairs, spatial, time_vec, phi_model, eps1=1e-5, max_passes=10000
):
    """
    Adjusts time_vec so that all parent-child pairs satisfy ds^2 < -eps1.
    The phi_model introduces a gravity-aware time distortion.
    """
    num_passes = 0
    converged = False

    time_vec = time_vec.clone().detach().requires_grad_(False)
    spatial = spatial.clone().detach().requires_grad_(False)

    while not converged and num_passes < max_passes:
        num_passes += 1
        converged = True

        for i, j in pairs:
            t_i, t_j = time_vec[i], time_vec[j]
            x_i, x_j = spatial[i], spatial[j]
            delta_t = t_i - t_j
            delta_x = x_i - x_j
            dx2 = torch.sum(delta_x**2)

            coords_i = torch.cat([t_i.view(1), x_i])
            phi_i = phi_model(coords_i.unsqueeze(0))[0]

            ds2 = -phi_i * delta_t**2 + dx2

            if ds2 >= -eps1:
                # Adjust time_i to be slightly later than time_j
                new_delta_t = torch.sqrt((dx2 + eps1) / phi_i + 1e-9)
                time_vec[i] = t_j + new_delta_t.item()
                converged = False

    return time_vec


# 1. Initial Setup: Define dummy data
N = 10  # Number of nodes/events
D_spatial = 3  # Number of spatial dimensions (e.g., x, y, z)
# For GravityField with input_dim=4, we need 1 (time) + D_spatial (3) = 4

# Spatial embeddings (N, D_spatial)
spatial = torch.randn(N, D_spatial, device=device)

# Initial time guesses (N,) - somewhat ordered for sensibility
time_vec = torch.sort(torch.rand(N, device=device) * N)[0]

# Causal hierarchy pairs: (child_idx, parent_idx)
# Ensure no node is its own parent and indices are within [0, N-1]
hierarchy_pairs = []
for i in range(N - 1):
    # Simple chain: node i+1 is child of node i
    if torch.rand(1).item() > 0.3:  # Sparsify a bit
        hierarchy_pairs.append((i + 1, i))
if not hierarchy_pairs:  # Ensure at least one pair for demonstration
    hierarchy_pairs.append((1, 0) if N > 1 else (0, 0))  # Avoid error with N=1

print(f"Number of nodes: {N}")
print(f"Number of spatial dimensions: {D_spatial}")
print(f"Initial time_vec: {time_vec}")
print(f"Hierarchy pairs (child, parent): {hierarchy_pairs}")

# Edge index for GNN (2, num_edges)
# Note: GCNConv expects (child, parent) or (target, source) for message passing.
# If your hierarchy_pairs are (child, parent), then parent influences child.
# So, messages flow from parent (j) to child (i).
# PyG typically uses (source, target), so if (child, parent) means parent->child,
# edge_index should be [[parent1, parent2,...], [child1, child2,...]]
edge_index_gnn = (
    torch.tensor([(p, c) for c, p in hierarchy_pairs], dtype=torch.long, device=device)
    .t()
    .contiguous()
)
if (
    edge_index_gnn.nelement() == 0
):  # Handle case with no pairs for GNN by creating a self-loop for node 0
    edge_index_gnn = (
        torch.tensor([[0], [0]], dtype=torch.long, device=device)
        if N > 0
        else torch.empty((2, 0), dtype=torch.long, device=device)
    )

print(f"Edge index for GNN (source_node, target_node):\n{edge_index_gnn}")

# For causal_phi_loss, the pairs are (child, parent)
# The function iterates `for (i,j) in edge_index.t()`, where i=child, j=parent.
# So edge_index for causal_phi_loss should be [[child1, child2,...], [parent1, parent2,...]]
edge_index_causal_loss = (
    torch.tensor(hierarchy_pairs, dtype=torch.long, device=device).t().contiguous()
)
if edge_index_causal_loss.nelement() == 0:
    edge_index_causal_loss = torch.empty((2, 0), dtype=torch.long, device=device)

# --- Training Phase (Optional but Recommended) ---
# A. Train φ on ds² Violations

# Initialize GravityField model
# input_dim = 1 (for time) + D_spatial (for spatial coords)
phi_model = GravityField(input_dim=1 + D_spatial).to(device)

print("--- Training phi_model ---")
phi_model = train_phi_from_ds2_violations(
    phi_model,
    hierarchy_pairs,
    spatial,
    time_vec,
    eps1=1e-5,
    num_epochs=50,
    lr=1e-3,  # Reduced epochs for demo
)
print("phi_model training complete.\n")


# --- Projection Phase ---
# B. Adjust Time Vector with Gravity-Aware Causal Projection

print("--- Projecting time_vec ---")
# Use the original time_vec as a starting point for projection
adjusted_time_vec = project_until_convergence(
    pairs=hierarchy_pairs,
    spatial=spatial,
    time_vec=time_vec.clone(),  # Pass a clone to avoid modifying original
    phi_model=phi_model,
    eps1=1e-5,
    max_passes=100,  # Reduced max_passes for demo
)
print(f"Original time_vec: {time_vec}")
print(f"Adjusted time_vec: {adjusted_time_vec}\n")


# --- GNN Input Preparation ---
print("--- Preparing GNN inputs ---")
with torch.no_grad():
    # Concatenate adjusted time and spatial coordinates to get phi inputs
    phi_model_input = torch.cat([adjusted_time_vec.unsqueeze(1), spatial], dim=1)
    phi_values = phi_model(phi_model_input).unsqueeze(1)  # (N, 1)

# Node features for GNN: [adjusted_time, x, y, z, phi_value]
node_features = torch.cat(
    [adjusted_time_vec.unsqueeze(1), spatial, phi_values], dim=1
)  # (N, 1+D_spatial+1)
print(f"Node features shape: {node_features.shape}\n")

# %%
# --- GNN Training ---
print("--- Training GravityAwareGNN ---")
# Define GNN
gnn_in_dim = node_features.shape[1]
gnn_hidden_dim = 16
gnn_out_dim = 1  # Example: regression task, predict one scalar per node
gnn_model = GravityAwareGNN(gnn_in_dim, gnn_hidden_dim, gnn_out_dim).to(device)

# Dummy target for the GNN task
dummy_target = torch.randn(N, gnn_out_dim, device=device)

# Optimizer for GNN
gnn_optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01)
lambda_phi_loss_weight = 0.1  # Weight for the causal phi loss

num_gnn_epochs = 50  # Reduced epochs for demo
for epoch in range(num_gnn_epochs):
    gnn_model.train()
    gnn_optimizer.zero_grad()

    # GNN forward pass
    gnn_output = gnn_model(node_features, edge_index_gnn)

    # Task loss (e.g., MSE for regression)
    task_loss = F.mse_loss(gnn_output, dummy_target)

    # Causal phi loss
    # It uses the *original spatial coordinates* and the *adjusted time vector*
    # along with the trained phi_model. This loss regularizes the GNN
    # to respect the learned spacetime geometry.
    # Ensure edge_index_causal_loss has pairs if you want this loss to be meaningful
    if edge_index_causal_loss.shape[1] > 0:
        c_phi_loss = causal_phi_loss(
            node_reps=spatial,  # Original spatial coordinates
            time_vec=adjusted_time_vec,
            edge_index=edge_index_causal_loss,  # child-parent pairs
            phi_model=phi_model,
            eps1=1e-5,
        )
    else:
        c_phi_loss = torch.tensor(0.0, device=device)  # No pairs, no causal loss

    total_loss = task_loss + lambda_phi_loss_weight * c_phi_loss
    total_loss.backward()
    gnn_optimizer.step()

    if epoch % 10 == 0 or epoch == num_gnn_epochs - 1:
        print(
            f"GNN Epoch {epoch + 1}/{num_gnn_epochs}, Task Loss: {task_loss.item():.4f}, Causal Phi Loss: {c_phi_loss.item():.4f}, Total Loss: {total_loss.item():.4f}"
        )

print("GNN training complete.")
print(f"Final GNN output example (first 5 nodes):\n{gnn_output[:5]}")

# %%
# --- WordNet Data Preparation ---
print("--- Preparing WordNet Data ---")

# Let's use noun synsets for this example
all_synsets = list(wn.all_synsets("n"))
print(f"Total noun synsets: {len(all_synsets)}")

# Filter for a smaller subset for faster demonstration if needed
# For example, synsets under 'animal.n.01'
# root_synset = wn.synset('animal.n.01')
# all_synsets = list(root_synset.closure(lambda s: s.hyponyms())) + [root_synset]
# print(f"Using subset of synsets related to '{root_synset.name()}': {len(all_synsets)}")
root_synset = wn.synset("animal.n.01")
all_synsets = list(root_synset.closure(lambda s: s.hyponyms())) + [root_synset]
print(f"Using subset of synsets related to '{root_synset.name()}': {len(all_synsets)}")

if not all_synsets:
    raise ValueError(
        "No synsets found. Check WordNet installation or subset selection."
    )

synset_to_id = {s: i for i, s in enumerate(all_synsets)}
id_to_synset = {i: s for s, i in synset_to_id.items()}
N = len(all_synsets)

# 1. Spatial Embeddings (Random for now)
D_spatial = 64  # Dimensionality of spatial embeddings
spatial = torch.randn(N, D_spatial, device=device)

# 2. Initial Time Vector (based on min_depth) & Target for GNN
time_vec_list = []
original_depths_list = []  # This will be our GNN target
for i in range(N):
    synset = id_to_synset[i]
    depth = synset.min_depth()
    time_vec_list.append(float(depth))
    original_depths_list.append(float(depth))

time_vec = torch.tensor(time_vec_list, dtype=torch.float32, device=device)
gnn_targets = torch.tensor(
    original_depths_list, dtype=torch.float32, device=device
).unsqueeze(1)

# 3. Hierarchy Pairs (child_id, parent_id) from hyponym-hypernym relations
hierarchy_pairs = []
for s_child, child_id in synset_to_id.items():
    for s_parent in s_child.hypernyms():
        if s_parent in synset_to_id:  # Ensure parent is in our list of synsets
            parent_id = synset_to_id[s_parent]
            hierarchy_pairs.append((child_id, parent_id))

print(f"Number of nodes (synsets): {N}")
print(f"Spatial dimension: {D_spatial}")
print(f"Number of hierarchy_pairs (edges): {len(hierarchy_pairs)}")

if not hierarchy_pairs and N > 1:
    print("Warning: No hierarchy pairs found. The model might not learn effectively.")
    # Create a dummy pair if none exist to prevent errors in subsequent steps with empty tensors
    hierarchy_pairs.append((1 % N, 0 % N))  # Ensure valid indices


# Edge index for GNN: (source_node, target_node) -> (parent, child)
edge_index_gnn_list = [(p, c) for c, p in hierarchy_pairs]
if not edge_index_gnn_list and N > 0:  # Handle case with no pairs for GNN
    edge_index_gnn = (
        torch.tensor([[0], [0]], dtype=torch.long, device=device)
        if N > 0
        else torch.empty((2, 0), dtype=torch.long, device=device)
    )
else:
    edge_index_gnn = (
        torch.tensor(edge_index_gnn_list, dtype=torch.long, device=device)
        .t()
        .contiguous()
    )


# Edge index for causal_phi_loss: (child, parent)
if not hierarchy_pairs and N > 0:
    edge_index_causal_loss = torch.empty((2, 0), dtype=torch.long, device=device)
else:
    edge_index_causal_loss = (
        torch.tensor(hierarchy_pairs, dtype=torch.long, device=device).t().contiguous()
    )


print(
    f"Initial min time_vec value: {time_vec.min().item()}, max: {time_vec.max().item()}"
)
print(f"GNN edge_index shape: {edge_index_gnn.shape}")
print(f"Causal loss edge_index shape: {edge_index_causal_loss.shape}\n")

# %%
# --- Training Phase: Phi Model ---
print("--- Training phi_model on WordNet data ---")
# input_dim = 1 (time) + D_spatial
phi_input_dim = 1 + D_spatial
phi_model = GravityField(input_dim=phi_input_dim).to(device)

# Parameters for phi training
phi_epochs = 50  # Adjust as needed, can be time-consuming for large N
phi_lr = 1e-2
phi_eps1 = 1e-4  # Slightly larger epsilon for stability with diverse depths

if edge_index_causal_loss.shape[1] > 0:  # Only train if there are pairs
    phi_model = train_phi_from_ds2_violations(
        phi_model,
        hierarchy_pairs,
        spatial,
        time_vec,
        eps1=phi_eps1,
        num_epochs=phi_epochs,
        lr=phi_lr,
    )
    print("phi_model training complete.\n")
else:
    print("Skipping phi_model training as there are no hierarchy pairs.\n")


# --- Projection Phase ---
print("--- Projecting time_vec with trained phi_model ---")
# Use the original time_vec as a starting point for projection
# Pass a clone of time_vec to project_until_convergence
if edge_index_causal_loss.shape[1] > 0:
    adjusted_time_vec = project_until_convergence(
        pairs=hierarchy_pairs,
        spatial=spatial,
        time_vec=time_vec.clone(),
        phi_model=phi_model,
        eps1=phi_eps1,
        max_passes=100000,  # Adjust as needed
    )
else:
    print(
        "Skipping time projection as there are no hierarchy pairs. Using original time_vec."
    )
    adjusted_time_vec = time_vec.clone()

print(
    f"Original time_vec stats: min={time_vec.min().item():.2f}, max={time_vec.max().item():.2f}, mean={time_vec.mean().item():.2f}"
)
print(
    f"Adjusted time_vec stats: min={adjusted_time_vec.min().item():.2f}, max={adjusted_time_vec.max().item():.2f}, mean={adjusted_time_vec.mean().item():.2f}\n"
)
# %%

# --- GNN Input Preparation ---
print("--- Preparing GNN inputs ---")
with torch.no_grad():
    # Concatenate adjusted time and spatial coordinates to get phi inputs
    # Ensure adjusted_time_vec is (N,1) and spatial is (N,D_spatial)
    phi_model_input = torch.cat([adjusted_time_vec.unsqueeze(1), spatial], dim=1)
    phi_values = phi_model(phi_model_input).unsqueeze(1)  # (N, 1)

# Node features for GNN: [adjusted_time, x1, x2, ..., x_D_spatial, phi_value]
node_features = torch.cat([adjusted_time_vec.unsqueeze(1), spatial, phi_values], dim=1)
print(
    f"Node features shape: {node_features.shape}\n"
)  # Should be (N, 1 + D_spatial + 1)


# --- GNN Training ---
print("--- Training GravityAwareGNN on WordNet data ---")
gnn_in_dim = node_features.shape[1]
gnn_hidden_dim = 128
gnn_out_dim = 1  # Predicting original depth (a scalar)
gnn_model = GravityAwareGNN(gnn_in_dim, gnn_hidden_dim, gnn_out_dim).to(device)

gnn_optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.005)
lambda_phi_loss_weight = 0.01  # Weight for the causal phi loss, tune this

num_gnn_epochs = 200  # Adjust as needed
for epoch in range(num_gnn_epochs):
    gnn_model.train()
    gnn_optimizer.zero_grad()

    gnn_output = gnn_model(node_features, edge_index_gnn)

    # Task loss: Predict original depth
    task_loss = F.mse_loss(gnn_output, gnn_targets)

    c_phi_loss = torch.tensor(0.0, device=device)
    if (
        edge_index_causal_loss.shape[1] > 0 and lambda_phi_loss_weight > 0
    ):  # Only compute if pairs exist and weight > 0
        c_phi_loss = causal_phi_loss(
            node_reps=spatial,  # Original spatial coordinates
            time_vec=adjusted_time_vec,
            edge_index=edge_index_causal_loss,
            phi_model=phi_model,
            eps1=phi_eps1,
        )

    total_loss = task_loss + lambda_phi_loss_weight * c_phi_loss
    total_loss.backward()
    gnn_optimizer.step()

    if epoch % 10 == 0 or epoch == num_gnn_epochs - 1:
        print(
            f"GNN Epoch {epoch + 1}/{num_gnn_epochs}, Task Loss: {task_loss.item():.4f}, Causal Phi Loss: {c_phi_loss.item():.4f}, Total Loss: {total_loss.item():.4f}"
        )

print("GNN training complete.")
# %%
# Evaluation (simple MSE on training data)
gnn_model.eval()
with torch.no_grad():
    final_predictions = gnn_model(node_features, edge_index_gnn)
    final_mse = F.mse_loss(final_predictions, gnn_targets)
    print(
        f"\nFinal GNN Training MSE on predicting original depth: {final_mse.item():.4f}"
    )

    # Compare some predictions to targets
    for i in range(min(5, N)):
        print(
            f"Synset: {id_to_synset[i].name()}, Original Depth: {gnn_targets[i].item():.2f}, Predicted Depth: {final_predictions[i].item():.2f}, Adjusted Time: {adjusted_time_vec[i].item():.2f}, Phi: {phi_values[i].item():.2f}"
        )


# %%
# === NEW: high-level causal refinement pipeline ===
def causal_refine_embeddings_with_phi(
    BERT_embeddings, attention_matrix, phi_model, epsilon=1e-5
):
    """
    Args:
        BERT_embeddings: Tensor of shape (seq_len, hidden_dim)
        attention_matrix: Tensor of shape (seq_len, seq_len), softmaxed attentions
        phi_model: trained GravityField instance
        epsilon: threshold for ds² constraint

    Returns:
        refined_spacetime_coords: (seq_len, 4)
        refined_embeddings: updated BERT_embeddings with causal feedback
    """
    seq_len, hidden_dim = BERT_embeddings.shape
    device = BERT_embeddings.device

    # === Step 1: Project into spacetime ===
    spatial = BERT_embeddings[:, :3]  # take first 3 dims as proxy spatial
    time_vec = torch.zeros(seq_len, device=device)

    # Extract top-k attention edges (causal order)
    k = 2
    pairs = []
    for i in range(seq_len):
        topk_indices = torch.topk(attention_matrix[i], k).indices
        for j in topk_indices:
            if j.item() < i:
                pairs.append((i, j.item()))  # token i attends to token j (past)

    # === Step 2: Project until causal convergence ===
    time_vec = project_until_convergence(
        pairs, spatial, time_vec, phi_model, eps1=epsilon
    )

    # === Step 3: Combine t and x into spacetime coord ===
    spacetime_coords = torch.cat([time_vec.unsqueeze(-1), spatial], dim=-1)

    # === Step 4: Feed back into BERT embedding layer ===
    refined_embeddings = torch.cat(
        [spacetime_coords, BERT_embeddings[:, 3:]], dim=-1
    )  # prepend causal txy

    return spacetime_coords, refined_embeddings


# %%

# For reproducibility (optional, but good for demos)
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# --- Definitions from your previous notebook cells ---
class GravityField(nn.Module):
    def __init__(self, input_dim=4):  # input_dim will be 1 (time) + 3 (spatial) = 4
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Softplus(),  # ensures phi > 0
        )

    def forward(self, coords):
        """
        coords: Tensor of shape (batch_size, input_dim) where columns are [t, x, y, z] or equivalent
        returns: Scalar phi > 0 per point
        """
        return self.net(coords).squeeze(-1)  # (batch_size,)


def project_until_convergence(
    pairs, spatial, time_vec, phi_model, eps1=1e-5, max_passes=10000
):
    """
    Adjusts time_vec so that all parent-child pairs satisfy ds^2 < -eps1.
    The phi_model introduces a gravity-aware time distortion.
    `pairs` are (child_idx, parent_idx)
    """
    num_passes = 0
    converged = False

    time_vec = (
        time_vec.clone().detach().requires_grad_(False)
    )  # Ensure it's on the correct device
    spatial = (
        spatial.clone().detach().requires_grad_(False)
    )  # Ensure it's on the correct device
    phi_model.to(spatial.device)  # Ensure phi_model is on the correct device

    # print(f"Initial time_vec in project_until_convergence: {time_vec}")

    while not converged and num_passes < max_passes:
        num_passes += 1
        made_change_in_pass = False  # More accurate convergence check

        for i, j in pairs:  # i is child, j is parent
            t_i, t_j = time_vec[i], time_vec[j]
            x_i, x_j = spatial[i], spatial[j]
            delta_t = t_i - t_j  # child_time - parent_time, should be > 0
            delta_x = x_i - x_j
            dx2 = torch.sum(delta_x**2)

            # Phi is evaluated at the child's coordinate (or an average, but child is common)
            # Coords for phi_model: [t, x, y, z]
            coords_i = torch.cat([t_i.view(1, 1), x_i.view(1, -1)], dim=-1)  # (1, 4)
            phi_i = phi_model(coords_i)[0]  # get scalar phi

            # If delta_t is negative or zero, it's a fundamental causal violation (child before parent)
            # or parent and child at same time but dx2 > 0.
            # We must ensure t_i > t_j for the ds^2 formula to be correctly applied.
            if delta_t <= 0:  # Child must be after parent
                # print(f"Pass {num_passes}, Pair ({i},{j}): Correcting t_i ({t_i:.3f}) to be after t_j ({t_j:.3f}) + small_delta")
                time_vec[i] = t_j + torch.sqrt(
                    (dx2 + eps1) / (phi_i + 1e-9) + 1e-9
                )  # Ensure some minimal separation
                made_change_in_pass = True
                delta_t = time_vec[i] - t_j  # Recalculate delta_t for ds2 check

            ds2 = -phi_i * delta_t**2 + dx2

            if ds2 >= -eps1:  # Causal violation or too close to light cone
                # Adjust time_i to be slightly later than time_j
                # We want -phi_i * new_delta_t^2 + dx2 = -eps1 (target)
                # new_delta_t^2 = (dx2 + eps1) / phi_i
                # Ensure argument to sqrt is non-negative
                sqrt_arg = (dx2 + eps1) / (
                    phi_i + 1e-9
                )  # Add small epsilon to phi_i to prevent division by zero
                if sqrt_arg < 0:  # Should not happen if phi_i > 0 and dx2+eps1 > 0
                    sqrt_arg = torch.tensor(
                        1e-9, device=time_vec.device
                    )  # Minimal positive value

                new_delta_t = torch.sqrt(
                    sqrt_arg + 1e-9
                )  # Add small epsilon for sqrt stability
                time_vec[i] = t_j + new_delta_t.item()
                # print(f"Pass {num_passes}, Pair ({i},{j}): Violated ds2 ({ds2:.3f}). Adjusted t_i from {t_i:.3f} to {time_vec[i]:.3f} (new_delta_t: {new_delta_t:.3f}, phi_i: {phi_i:.3f})")
                made_change_in_pass = True

        if not made_change_in_pass:
            converged = True
        # if num_passes % 100 == 0:
        #     print(f"Projection pass {num_passes}...")

    if num_passes == max_passes and not converged:
        print(
            f"Warning: project_until_convergence reached max_passes ({max_passes}) without full convergence."
        )
    # else:
    #     print(f"Converged in {num_passes} passes.")
    return time_vec


# === END Definitions from your previous notebook cells ---


# === NEW: high-level causal refinement pipeline ===
def causal_refine_embeddings_with_phi(
    BERT_embeddings,
    attention_matrix,
    phi_model,
    initial_time_strategy="zero",
    k_attention=2,
    phi_input_dims=3,  # Number of spatial dimensions phi_model expects
    epsilon=1e-5,
    max_passes_projection=1000,
):
    """
    Refines BERT embeddings by projecting them into a causally consistent spacetime
    and then prepending these spacetime coordinates.

    Args:
        BERT_embeddings: Tensor of shape (seq_len, hidden_dim)
        attention_matrix: Tensor of shape (seq_len, seq_len), raw attention scores or softmaxed.
                          Assumes attention_matrix[i,j] is attention from token i to token j.
        phi_model: trained GravityField instance.
        initial_time_strategy: "zero" (all start at t=0),
                               "sequential" (t_i = i),
                               "attention_based" (t_i proportional to sum of attention received from past).
        k_attention: How many top attention links to consider for forming causal pairs.
        phi_input_dims: The number of spatial dimensions your phi_model was trained on (e.g., 3 for x,y,z).
        epsilon: threshold for ds² constraint in project_until_convergence.
        max_passes_projection: max_passes for project_until_convergence.


    Returns:
        refined_spacetime_coords: (seq_len, 1 + phi_input_dims) e.g., (seq_len, 4) if phi_input_dims=3
        refined_embeddings: (seq_len, 1 + phi_input_dims + (hidden_dim - phi_input_dims) )
                            updated BERT_embeddings with causal t,x,y,... prepended.
    """
    seq_len, hidden_dim = BERT_embeddings.shape
    device = BERT_embeddings.device
    phi_model.to(device)

    # === Step 1: Project into spacetime ===
    # Ensure we don't take more spatial dims than available or than phi expects
    actual_spatial_dims = min(phi_input_dims, hidden_dim)
    if actual_spatial_dims < phi_input_dims:
        print(
            f"Warning: BERT_embeddings hidden_dim ({hidden_dim}) is less than "
            f"phi_model's expected spatial_dims ({phi_input_dims}). "
            f"Using first {actual_spatial_dims} embedding dims as spatial."
        )
    if actual_spatial_dims == 0:
        raise ValueError(
            "Cannot extract spatial dimensions if hidden_dim is 0 or phi_input_dims is 0."
        )

    spatial = (
        BERT_embeddings[:, :actual_spatial_dims].clone().detach()
    )  # (seq_len, actual_spatial_dims)

    # Initial time vector
    if initial_time_strategy == "zero":
        time_vec = torch.zeros(seq_len, device=device)
    elif initial_time_strategy == "sequential":
        time_vec = torch.arange(seq_len, dtype=torch.float32, device=device)
    elif initial_time_strategy == "attention_based":
        # Time proportional to sum of attention received from tokens j < i
        time_vec = torch.zeros(seq_len, device=device)
        for i in range(1, seq_len):
            time_vec[i] = torch.sum(
                attention_matrix[i, :i]
            )  # Sum attention from i to j where j < i
        if time_vec.max() > 0:  # Normalize if non-zero
            time_vec = time_vec / time_vec.max() * seq_len
    else:
        raise ValueError(f"Unknown initial_time_strategy: {initial_time_strategy}")

    # Extract top-k attention edges (causal order)
    # We assume attention_matrix[i, j] means token i attends to token j.
    # If j < i, this is a "backward" attention, implying j is a "parent" or "cause" for i.
    # So, the pair for project_until_convergence should be (child=i, parent=j).
    pairs = []
    for i in range(seq_len):  # i is the potential child
        # Consider only attentions to previous tokens
        if i == 0:
            continue  # First token cannot attend to previous ones

        # Get attention scores from token i to all previous tokens j < i
        att_to_past = attention_matrix[i, :i]
        if att_to_past.numel() == 0:
            continue

        # Select top k, ensuring k is not greater than number of past tokens
        current_k = min(k_attention, att_to_past.numel())
        if current_k == 0:
            continue

        topk_scores, topk_indices_relative_to_past = torch.topk(att_to_past, current_k)

        for k_idx in range(current_k):
            j = topk_indices_relative_to_past[
                k_idx
            ].item()  # j is an index in the range [0, i-1]
            # We need (child_idx, parent_idx)
            # Here, i is the child, j is the parent.
            pairs.append((i, j))

    print(f"Extracted {len(pairs)} causal pairs from attention matrix.")
    if not pairs and seq_len > 1:
        print("Warning: No causal pairs extracted. Refinement might be trivial.")
        # Create a default sequential pairing if none found, to avoid errors downstream
        # and ensure project_until_convergence runs.
        # for i_link in range(1, seq_len): pairs.append((i_link, i_link-1))

    # === Step 2: Project until causal convergence ===
    if pairs:  # Only run projection if there are pairs to enforce
        time_vec = project_until_convergence(
            pairs,
            spatial,
            time_vec,
            phi_model,
            eps1=epsilon,
            max_passes=max_passes_projection,
        )
    else:
        print("Skipping projection as no causal pairs were derived from attention.")

    # === Step 3: Combine t and x into spacetime coord ===
    # Ensure time_vec is (seq_len, 1)
    refined_spacetime_coords = torch.cat(
        [time_vec.unsqueeze(-1), spatial], dim=-1
    )  # (seq_len, 1 + actual_spatial_dims)

    # === Step 4: Feed back into BERT embedding layer ===
    # Prepend the (t, x, y, ...) coords to the *remaining* part of BERT embeddings.
    # If actual_spatial_dims was less than phi_input_dims, refined_spacetime_coords will have fewer spatial dims.
    # The goal is to replace the first `actual_spatial_dims` of BERT_embeddings with the refined ones,
    # and prepend `time`.
    remaining_bert_dims = BERT_embeddings[:, actual_spatial_dims:]
    refined_embeddings = torch.cat(
        [refined_spacetime_coords, remaining_bert_dims], dim=-1
    )

    # The final embedding dim will be 1 (time) + actual_spatial_dims + (hidden_dim - actual_spatial_dims) = 1 + hidden_dim
    # This might not be what's intended if phi_input_dims < 3 and we want to keep BERT_embeddings[:,3:]
    # Let's adjust to ensure refined_embeddings has shape (seq_len, 1 + hidden_dim)
    # by taking time + original spatial + remaining original BERT embedding.

    # Corrected Step 4:
    # We want refined_embeddings to be [t_refined, x_refined, y_refined, z_refined, bert_original_dim_4, ..., bert_original_dim_H]
    # where x,y,z_refined are from the first actual_spatial_dims of BERT_embeddings.
    # The number of spatial dimensions used in spacetime_coords is actual_spatial_dims.
    # The original BERT embeddings are (seq_len, hidden_dim).
    # We take BERT_embeddings[:, actual_spatial_dims:] for the "rest" of the embedding.
    # So refined_embeddings will be (seq_len, 1 + actual_spatial_dims + (hidden_dim - actual_spatial_dims))
    # which simplifies to (seq_len, 1 + hidden_dim).

    # If actual_spatial_dims < phi_input_dims, the spacetime_coords will reflect that.
    # The refined_embeddings will then be (seq_len, 1 + actual_spatial_dims + (hidden_dim - actual_spatial_dims))
    # This means the refined_embeddings effectively gain one dimension (for time).

    return refined_spacetime_coords, refined_embeddings


# --- Demo Usage ---
print("--- Starting Causal Refinement Demo ---")

# 1. Hyperparameters for the demo
SEQ_LEN = 300
HIDDEN_DIM_BERT = 768  # Typical BERT base hidden size
PHI_MODEL_SPATIAL_DIMS = (
    3  # How many spatial dimensions phi_model expects (e.g., x, y, z)
)

# 2. Create Dummy BERT Embeddings and Attention Matrix
# (seq_len, hidden_dim)
dummy_bert_embeddings = torch.randn(SEQ_LEN, HIDDEN_DIM_BERT, device=device)

# (seq_len, seq_len) - ensure it's somewhat plausible (e.g., higher attention to closer preceding tokens)
dummy_attention_matrix = torch.zeros(SEQ_LEN, SEQ_LEN, device=device)
for i in range(SEQ_LEN):
    for j in range(i):  # Token i attends to token j (j < i)
        dummy_attention_matrix[i, j] = torch.rand(1).item() * (
            1.0 / (i - j)
        )  # Higher for closer tokens
# Softmax per row (attention from token i to all other tokens)
dummy_attention_matrix = F.softmax(dummy_attention_matrix, dim=1)
# Zero out attention to self and future tokens if softmax spread it there
for i in range(SEQ_LEN):
    dummy_attention_matrix[i, i:] = 0
    # Renormalize if we zeroed out parts, to ensure rows sum to 1 for past tokens
    row_sum_past = dummy_attention_matrix[i, :i].sum()
    if row_sum_past > 0 and i > 0:
        dummy_attention_matrix[i, :i] /= row_sum_past
    elif i > 0:  # if all past attentions were zero, distribute equally for demo
        if i > 0:
            dummy_attention_matrix[i, :i] = 1.0 / i


print(f"Dummy BERT embeddings shape: {dummy_bert_embeddings.shape}")
print(
    f"Dummy attention matrix (sum of first row's past attention): {dummy_attention_matrix[1, 0] if SEQ_LEN > 1 else 'N/A'}"
)


# 3. Create and "Train" a Dummy Phi Model
# The phi_model expects input_dim = 1 (time) + PHI_MODEL_SPATIAL_DIMS
phi_input_dim_actual = 1 + PHI_MODEL_SPATIAL_DIMS
demo_phi_model = GravityField(input_dim=phi_input_dim_actual).to(device)

# Optional: "Pre-train" phi_model a bit on dummy data if you want it to be non-trivial
# For this demo, a fresh phi_model will mostly yield phi near 1 initially (due to Softplus on small weights/biases)
# If you have a pre-trained one from WordNet, you could load it, but ensure input_dim matches.
print(f"Initialized phi_model with input_dim={phi_input_dim_actual}")


# 4. Run the Causal Refinement Pipeline
refined_coords, refined_embs = causal_refine_embeddings_with_phi(
    dummy_bert_embeddings,
    dummy_attention_matrix,
    demo_phi_model,
    initial_time_strategy="attention_based",  # Try "zero", "sequential", "attention_based"
    k_attention=2,  # How many past tokens to consider "causal parents"
    phi_input_dims=PHI_MODEL_SPATIAL_DIMS,
    epsilon=1e-5,
    max_passes_projection=1000,
)

# 5. Inspect Results
print("\n--- Results ---")
print(f"Original BERT embeddings shape: {dummy_bert_embeddings.shape}")
print(f"Refined spacetime coordinates shape: {refined_coords.shape}")
print(f"Refined embeddings shape: {refined_embs.shape}")

print("\nFirst 3 Refined Spacetime Coords (t, x, y, z):")
print(refined_coords[:3, :])

print("\nFirst 3 Refined Embeddings (first 5 dims):")
print(refined_embs[:3, :5])

# Verify that the refined embeddings consist of refined_coords and the tail of original BERT
# Expected: refined_embs = [refined_txyz, original_bert_dims_from_spatial_dim_onwards]
# Example check for the first embedding:
bert_tail_start_dim = refined_coords.shape[1] - 1  # this is PHI_MODEL_SPATIAL_DIMS
original_bert_tail = dummy_bert_embeddings[0, bert_tail_start_dim:]
refined_emb_tail = refined_embs[0, refined_coords.shape[1] :]

if refined_embs.shape[1] == 1 + dummy_bert_embeddings.shape[1]:
    print("\nRefined embeddings have an additional dimension for time, as expected.")
    # Check concatenation logic:
    # refined_embs[0,0] should be refined_coords[0,0] (time)
    # refined_embs[0,1:1+bert_tail_start_dim] should be refined_coords[0,1:] (spatial)
    # refined_embs[0,1+bert_tail_start_dim:] should be dummy_bert_embeddings[0, bert_tail_start_dim:]
    time_match = torch.allclose(refined_embs[0, 0], refined_coords[0, 0])
    spatial_match = torch.allclose(
        refined_embs[0, 1 : 1 + bert_tail_start_dim], refined_coords[0, 1:]
    )
    tail_match = torch.allclose(
        refined_embs[0, 1 + bert_tail_start_dim :],
        dummy_bert_embeddings[0, bert_tail_start_dim:],
    )

    if time_match and spatial_match and tail_match:
        print("Concatenation logic for refined_embeddings appears correct.")
    else:
        print("Mismatch in refined_embeddings concatenation. Check logic.")
        print(
            f"Time match: {time_match}, Spatial match: {spatial_match}, Tail match: {tail_match}"
        )

else:
    print(
        f"Unexpected shape for refined_embeddings. Expected {1 + dummy_bert_embeddings.shape[1]}, got {refined_embs.shape[1]}"
    )
# %% [markdown]
"""
Visualizations
"""
# %%
print("--- Starting Visualization ---")
import matplotlib.pyplot as plt

# Ensure data is on CPU and converted to NumPy for Matplotlib
refined_coords_np = refined_coords.cpu().detach().numpy()
original_spatial_np = (
    dummy_bert_embeddings[:, :PHI_MODEL_SPATIAL_DIMS].cpu().detach().numpy()
)
token_indices = np.arange(SEQ_LEN)

# Extract t, x, y, (z) from refined_coords
t_refined = refined_coords_np[:, 0]
x_refined = refined_coords_np[:, 1]
if PHI_MODEL_SPATIAL_DIMS >= 2:
    y_refined = refined_coords_np[:, 2]
if PHI_MODEL_SPATIAL_DIMS >= 3:
    z_refined = refined_coords_np[:, 3]

# Calculate phi values at refined coordinates for coloring (optional)
with torch.no_grad():
    phi_values_at_refined = (
        demo_phi_model(refined_coords.to(device)).cpu().detach().numpy()
    )

# --- Plot 1: Refined Spacetime Coordinates (2D: t vs x) ---
plt.figure(figsize=(10, 6))
scatter = plt.scatter(
    x_refined, t_refined, c=phi_values_at_refined, cmap="viridis", s=50
)
for i in range(SEQ_LEN):
    plt.text(
        x_refined[i] + 0.05, t_refined[i] + 0.05, str(i), fontsize=9
    )  # Label points with token index
plt.xlabel("Spatial Dim 1 (from BERT emb dim 0)")
plt.ylabel("Refined Time (t)")
plt.title("Refined Spacetime Coordinates (t vs. x1)")
plt.colorbar(scatter, label="Phi Value at (t,x,y,z)")
plt.grid(True)
plt.show()

# --- Plot 2: Refined Spacetime Coordinates (3D: t, x, y) if PHI_MODEL_SPATIAL_DIMS >= 2 ---
if PHI_MODEL_SPATIAL_DIMS >= 2:
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection="3d")
    scatter3d = ax.scatter(
        x_refined, y_refined, t_refined, c=phi_values_at_refined, cmap="viridis", s=50
    )
    for i in range(SEQ_LEN):
        ax.text(x_refined[i], y_refined[i], t_refined[i], str(i), fontsize=9)
    ax.set_xlabel("Spatial Dim 1 (BERT emb dim 0)")
    ax.set_ylabel("Spatial Dim 2 (BERT emb dim 1)")
    ax.set_zlabel("Refined Time (t)")
    ax.set_title("Refined Spacetime Coordinates (t, x1, y1)")
    fig.colorbar(scatter3d, label="Phi Value at (t,x,y,z)", shrink=0.5, aspect=10)
    plt.show()

# --- Plot 3: Comparison of Initial vs. Refined Time ---
# We need the initial time_vec used in causal_refine_embeddings_with_phi
# Let's reconstruct it based on the strategy used (e.g., "sequential")
# This assumes causal_refine_embeddings_with_phi was called with this strategy in the demo cell
# For simplicity, let's assume 'sequential' was used for this visualization.
# You might need to pass the initial time_vec explicitly if it's complex.

initial_time_strategy_used = "sequential"  # Make sure this matches the demo call
if initial_time_strategy_used == "zero":
    initial_time_vec_np = np.zeros(SEQ_LEN)
elif initial_time_strategy_used == "sequential":
    initial_time_vec_np = np.arange(SEQ_LEN, dtype=np.float32)
elif initial_time_strategy_used == "attention_based":
    # This requires the dummy_attention_matrix from the previous cell
    # temp_time_vec = torch.zeros(SEQ_LEN, device=device)
    # for i in range(1, SEQ_LEN):
    #     temp_time_vec[i] = torch.sum(dummy_attention_matrix[i, :i])
    # if temp_time_vec.max() > 0:
    #      temp_time_vec = temp_time_vec / temp_time_vec.max() * SEQ_LEN
    # initial_time_vec_np = temp_time_vec.cpu().numpy()
    print(
        "Note: 'attention_based' initial time for plotting requires dummy_attention_matrix. Using sequential for simplicity here."
    )
    initial_time_vec_np = np.arange(SEQ_LEN, dtype=np.float32)  # Fallback for this plot
else:
    initial_time_vec_np = np.zeros(SEQ_LEN)  # Default fallback

plt.figure(figsize=(12, 6))
plt.plot(
    token_indices,
    initial_time_vec_np,
    "o-",
    label="Initial Time (e.g., Sequential)",
    color="blue",
)
plt.plot(
    token_indices, t_refined, "s-", label="Refined Time (after projection)", color="red"
)
for i in range(SEQ_LEN):
    plt.plot(
        [token_indices[i], token_indices[i]],
        [initial_time_vec_np[i], t_refined[i]],
        linestyle="--",
        color="gray",
        alpha=0.7,
    )
plt.xlabel("Token Index")
plt.ylabel("Time Value")
plt.title("Change in Time Values: Initial vs. Refined")
plt.xticks(token_indices)
plt.legend()
plt.grid(True)
plt.show()


# --- Plot 4: "Spacetime Trajectory" - connecting points by token index ---
# This shows how the "worldline" of the sequence looks in the refined (t,x) space
plt.figure(figsize=(10, 6))
plt.plot(
    x_refined,
    t_refined,
    "o-",
    label="Token Trajectory",
    color="purple",
    markersize=8,
    markerfacecolor="orange",
)
for i in range(SEQ_LEN):
    plt.text(x_refined[i] + 0.05, t_refined[i] + 0.05, str(i), fontsize=9)
plt.xlabel("Spatial Dim 1 (from BERT emb dim 0)")
plt.ylabel("Refined Time (t)")
plt.title("Spacetime Trajectory of Tokens in Refined (t, x1) Space")
plt.legend()
plt.grid(True)
plt.show()

if PHI_MODEL_SPATIAL_DIMS >= 2:
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection="3d")
    ax.plot(
        x_refined,
        y_refined,
        t_refined,
        "o-",
        label="Token Trajectory",
        color="purple",
        markersize=8,
        markerfacecolor="orange",
    )
    for i in range(SEQ_LEN):
        ax.text(x_refined[i], y_refined[i], t_refined[i], str(i), fontsize=9)
    ax.set_xlabel("Spatial Dim 1")
    ax.set_ylabel("Spatial Dim 2")
    ax.set_zlabel("Refined Time (t)")
    ax.set_title("Spacetime Trajectory in Refined (t, x1, y1) Space")
    ax.legend()
    plt.show()

print("--- Visualization Complete ---")
# %%
# --- Assume these are defined elsewhere (from your previous work) ---
# class GravityField(nn.Module): ...
# def project_until_convergence(pairs, spatial, time_vec, phi_model, eps1=1e-5, max_passes=1000): ...
# def causal_phi_loss(node_reps, time_vec, edge_index, phi_model, eps1=1e-5): ...


# For this demo, let's mock them up briefly:
class GravityField(nn.Module):
    def __init__(self, input_dim=4):
        super().__init__()
        self.fc = nn.Linear(input_dim, 1)
        self.softplus = nn.Softplus()

    def forward(self, x):
        return self.softplus(self.fc(x)).squeeze(-1)


def project_until_convergence(
    pairs, spatial, time_vec, phi_model, eps1=1e-5, max_passes=100
):
    # print(f"Debug: Projecting with {len(pairs)} pairs. Initial time_vec: {time_vec[:3]}")
    # In a real scenario, this would modify time_vec
    # For demo, let's just slightly perturb it based on pairs to show it runs
    if pairs:
        time_vec_clone = time_vec.clone()
        for i_child, i_parent in pairs:
            if i_child < len(time_vec_clone) and i_parent < len(time_vec_clone):
                if time_vec_clone[i_child] <= time_vec_clone[i_parent]:
                    time_vec_clone[i_child] = time_vec_clone[
                        i_parent
                    ] + 0.1 * torch.rand(1).to(time_vec.device)
        return time_vec_clone
    return time_vec.clone()


def causal_phi_loss(node_reps, time_vec, edge_index, phi_model, eps1=1e-5):
    # Mock loss
    if edge_index.numel() == 0 or node_reps.numel() == 0:
        return torch.tensor(0.0, device=node_reps.device, requires_grad=True)

    # Ensure phi_model is on the same device as node_reps
    phi_model.to(node_reps.device)

    # Simplified dummy calculation for phi
    # In reality, you'd iterate through edges, calculate ds2, and apply relu(ds2 + eps1)
    phi_inputs = torch.cat([time_vec.unsqueeze(-1), node_reps], dim=-1)

    # Check if phi_inputs is empty
    if phi_inputs.numel() == 0:
        return torch.tensor(0.0, device=node_reps.device, requires_grad=True)

    # Ensure phi_inputs isn't too large for a single forward pass if phi_model is simple
    # This is a mock, so we'll just take a sample if it's too big.
    sample_size = min(10, phi_inputs.shape[0])
    if sample_size > 0:
        phi_values = phi_model(phi_inputs[:sample_size])
        return F.relu(phi_values.mean() - 1.0)  # Arbitrary mock loss calculation
    else:
        return torch.tensor(0.0, device=node_reps.device, requires_grad=True)


# --- End of assumed definitions ---


# --- Phase 1: Motion-Sensitive Feature Extraction ---
class MotionFeatureExtractor(nn.Module):
    def __init__(self, input_channels=1, output_motion_dim=64, num_frames_in_clip=3):
        super().__init__()
        self.num_frames_in_clip = num_frames_in_clip
        # Using (2+1)D conv idea: spatial conv then temporal conv
        # This is a very simplified version
        self.spatial_conv = nn.Conv2d(
            input_channels * num_frames_in_clip, 32, kernel_size=3, padding=1
        )
        self.temporal_pool = nn.AdaptiveAvgPool1d(
            1
        )  # To simulate temporal aggregation over the clip
        self.fc = nn.Linear(
            32, output_motion_dim
        )  # Assuming pooling flattens spatial dims

        # For frame differencing approach (alternative, simpler)
        # self.cnn = nn.Sequential(
        #     nn.Conv2d(input_channels, 16, kernel_size=3, padding=1),
        #     nn.ReLU(),
        #     nn.AdaptiveAvgPool2d((1,1)), # Pool spatial dimensions
        #     nn.Flatten(),
        #     nn.Linear(16, output_motion_dim)
        # )

    def forward(self, frames_sequence):
        """
        frames_sequence: (Batch, T_total_frames, H, W, C)
        Output: motion_features (Batch, T_motion_steps, D_motion)
        """
        B, T_total, H, W, C = frames_sequence.shape
        frames_sequence = frames_sequence.permute(0, 1, 4, 2, 3)  # (B, T, C, H, W)

        motion_features_list = []
        # Simplified processing: treat each frame as a step, use diffs or short clips
        # For this demo, let's assume a simple CNN on frame differences
        # More sophisticated: sliding window for clips

        # Using frame differencing:
        # for t in range(T_total - 1):
        #     diff_frame = frames_sequence[:, t+1] - frames_sequence[:, t] # (B, C, H, W)
        #     # In a real model, you'd pool H, W before fc
        #     # Simplified for now:
        #     pooled_diff = F.adaptive_avg_pool2d(diff_frame, (1,1)).view(B, -1) # (B, C) - very crude
        #     motion_features_list.append(self.fc(pooled_diff)) # This fc needs C as input_dim

        # Let's use a dummy fixed output for simplicity in scaffolding
        # In reality, this would be a complex CNN/3DCNN
        T_motion_steps = (
            T_total - self.num_frames_in_clip + 1
            if T_total >= self.num_frames_in_clip
            else 1
        )
        if T_total < self.num_frames_in_clip:  # Handle very short sequences
            # Pad or return zero features, for demo just create one step
            dummy_output_dim = self.fc.out_features
            return torch.randn(B, 1, dummy_output_dim, device=frames_sequence.device)

        # A slightly more plausible (but still simplified) (2+1)D-like
        for t in range(T_motion_steps):
            clip = frames_sequence[
                :, t : t + self.num_frames_in_clip
            ]  # (B, num_frames_in_clip, C, H, W)
            clip = clip.reshape(
                B, self.num_frames_in_clip * C, H, W
            )  # Treat frames in clip as channels

            x = F.relu(self.spatial_conv(clip))  # (B, 32, H, W)
            x = F.adaptive_avg_pool2d(x, (1, 1)).view(B, 32)  # (B, 32)
            motion_features_list.append(self.fc(x))

        if not motion_features_list:  # Should not happen if T_motion_steps > 0
            dummy_output_dim = self.fc.out_features
            return torch.randn(B, 1, dummy_output_dim, device=frames_sequence.device)

        motion_features = torch.stack(
            motion_features_list, dim=1
        )  # (B, T_motion_steps, D_motion)
        return motion_features


# --- Phase 2: Temporal Self-Attention and Causal Link Definition ---
class TemporalAttentionModule(nn.Module):
    def __init__(self, motion_dim, num_heads=4, ff_dim=128):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            embed_dim=motion_dim, num_heads=num_heads, batch_first=True
        )
        self.norm1 = nn.LayerNorm(motion_dim)
        self.norm2 = nn.LayerNorm(motion_dim)
        self.ff = nn.Sequential(
            nn.Linear(motion_dim, ff_dim), nn.ReLU(), nn.Linear(ff_dim, motion_dim)
        )

    def forward(self, motion_features):
        """
        motion_features: (Batch, T_motion_steps, D_motion)
        Output:
            contextualized_features: (Batch, T_motion_steps, D_motion)
            attention_weights: (Batch, T_motion_steps, T_motion_steps) - from MHA
        """
        # Self-attention
        # For MHA, query, key, value are the same
        attn_output, attn_weights = self.attention(
            motion_features, motion_features, motion_features
        )
        # attn_weights is (Batch, T_query, T_key) which is (B, T_motion, T_motion)

        x = self.norm1(motion_features + attn_output)  # Add & Norm
        # Feed Forward
        ff_output = self.ff(x)
        contextualized_features = self.norm2(x + ff_output)  # Add & Norm

        return contextualized_features, attn_weights


# --- Phase 3: Causal Spacetime Refinement (Utilizing your components) ---
class SpatialProjector(nn.Module):
    def __init__(self, motion_dim, phi_spatial_dim=3):
        super().__init__()
        self.fc = nn.Linear(motion_dim, phi_spatial_dim)

    def forward(self, motion_features):
        """
        motion_features: (Batch, T_motion_steps, D_motion)
        Output: spatial_coords_for_phi (Batch, T_motion_steps, phi_spatial_dim)
        """
        return self.fc(motion_features)


# --- Phase 4: Feature Augmentation and Decoding ---
class Decoder(nn.Module):
    def __init__(self, augmented_feature_dim, num_classes):
        super().__init__()
        # Simple decoder: temporal pooling + MLP
        self.pool = lambda x: torch.mean(x, dim=1)  # Temporal mean pooling
        self.fc = nn.Linear(augmented_feature_dim, num_classes)

    def forward(self, augmented_features_sequence):
        """
        augmented_features_sequence: (Batch, T_motion_steps, D_augmented)
        Output: class_logits (Batch, num_classes)
        """
        pooled_features = self.pool(augmented_features_sequence)
        logits = self.fc(pooled_features)
        return logits


# --- Overall CaMPNet Model ---
class CaMPNet(nn.Module):
    def __init__(
        self,
        input_channels=1,
        motion_dim=64,
        num_frames_in_clip=3,
        phi_spatial_dim=3,
        num_attn_heads=4,
        ff_dim=128,
        num_classes=10,
        k_attention=2,
    ):
        super().__init__()
        self.motion_extractor = MotionFeatureExtractor(
            input_channels, motion_dim, num_frames_in_clip
        )
        self.temporal_attention = TemporalAttentionModule(
            motion_dim, num_attn_heads, ff_dim
        )
        self.spatial_projector = SpatialProjector(motion_dim, phi_spatial_dim)

        # Phi model's input dim is 1 (time) + phi_spatial_dim
        self.phi_model = GravityField(input_dim=1 + phi_spatial_dim)

        # Augmented feature dim: 1(time) + phi_spatial_dim + 1(phi_val) + motion_dim
        augmented_dim = 1 + phi_spatial_dim + 1 + motion_dim
        self.decoder = Decoder(augmented_dim, num_classes)

        self.k_attention = k_attention
        self.phi_spatial_dim = phi_spatial_dim

    def forward(self, frames_sequence, targets=None, lambda_phi_loss_weight=0.1):
        """
        frames_sequence: (Batch, T_total_frames, H, W, C)
        targets: (Batch,) - for calculating task loss (optional, for training)
        """
        B, T_total, H, W, C = frames_sequence.shape

        # Phase 1
        motion_features = self.motion_extractor(
            frames_sequence
        )  # (B, T_motion, D_motion)
        # print(f"Debug: motion_features shape: {motion_features.shape}")

        # Phase 2
        context_motion_features, attention_weights = self.temporal_attention(
            motion_features
        )
        # attention_weights: (B, T_motion, T_motion)
        # print(f"Debug: context_motion_features shape: {context_motion_features.shape}")

        # Phase 3 (Iterate per batch item due to project_until_convergence and pair extraction logic)
        batch_augmented_features = []
        batch_phi_loss_contributions = []

        for b_idx in range(B):
            current_motion_features = context_motion_features[
                b_idx
            ]  # (T_motion, D_motion)
            current_attention = attention_weights[b_idx]  # (T_motion, T_motion)
            T_motion = current_motion_features.shape[0]

            # Extract causal pairs from attention for this batch item
            pairs = []
            for i in range(T_motion):  # child
                if i == 0:
                    continue
                att_to_past = current_attention[i, :i]
                if att_to_past.numel() == 0:
                    continue

                current_k = min(self.k_attention, att_to_past.numel())
                if current_k == 0:
                    continue

                _, topk_indices_relative = torch.topk(att_to_past, current_k)
                for k_val_idx in range(current_k):
                    j = topk_indices_relative[k_val_idx].item()
                    pairs.append((i, j))  # child_idx, parent_idx

            # print(f"Debug: Batch {b_idx}, num_pairs: {len(pairs)}")

            # Project to "phi-spatial" space
            spatial_coords_for_phi = self.spatial_projector(
                current_motion_features.unsqueeze(0)
            ).squeeze(0)  # (T_motion, phi_spatial_dim)
            # print(f"Debug: spatial_coords_for_phi shape: {spatial_coords_for_phi.shape}")

            # Initial time vector
            initial_time_vec = torch.arange(
                T_motion, dtype=torch.float32, device=frames_sequence.device
            )

            # Project until convergence
            # Ensure phi_model is on the correct device for this call
            self.phi_model.to(spatial_coords_for_phi.device)
            adjusted_time_vec = project_until_convergence(
                pairs, spatial_coords_for_phi, initial_time_vec, self.phi_model
            )
            # print(f"Debug: adjusted_time_vec shape: {adjusted_time_vec.shape}")

            # Phase 4: Augmentation for this batch item
            augmented_features_this_item = []
            for t_step in range(T_motion):
                t_refined = adjusted_time_vec[t_step].unsqueeze(0)
                s_t = spatial_coords_for_phi[t_step]  # (phi_spatial_dim)

                # Ensure inputs to phi_model are correctly shaped and on device
                phi_input = torch.cat(
                    [t_refined, s_t.unsqueeze(0)], dim=-1
                )  # (1, 1 + phi_spatial_dim)
                phi_val = self.phi_model(phi_input).unsqueeze(
                    0
                )  # (1,1) because phi_model squeezes last dim

                original_motion_vec = current_motion_features[t_step]  # (D_motion)

                aug_feat = torch.cat(
                    [
                        t_refined,  # 1
                        s_t,  # phi_spatial_dim
                        phi_val.squeeze(0),  # 1
                        original_motion_vec,  # D_motion
                    ]
                )
                augmented_features_this_item.append(aug_feat)

            if not augmented_features_this_item:  # Handle T_motion=0 or very small
                # Create a dummy feature of expected size
                aug_dim = 1 + self.phi_spatial_dim + 1 + motion_features.shape[-1]
                dummy_aug_feat = torch.zeros(aug_dim, device=frames_sequence.device)
                # In a real case, ensure T_motion is always >= 1 for decoder
                if T_motion == 0:  # Add a dummy sequence step if T_motion was 0
                    batch_augmented_features.append(dummy_aug_feat.unsqueeze(0))
                else:  # Should not happen if T_motion > 0
                    batch_augmented_features.append(
                        torch.stack(augmented_features_this_item)
                    )

            else:
                batch_augmented_features.append(
                    torch.stack(augmented_features_this_item)
                )

            # Calculate phi_loss for this item
            if pairs:
                # Edge index for causal_phi_loss: (2, num_edges) -> [[parent...], [child...]]
                # project_until_convergence pairs are (child, parent)
                # causal_phi_loss expects edge_index.t() to be (child, parent)
                # So edge_index should be [[child1,...], [parent1,...]]
                edge_index_causal = torch.tensor(
                    pairs, dtype=torch.long, device=frames_sequence.device
                ).t()

                phi_loss_item = causal_phi_loss(
                    spatial_coords_for_phi,
                    adjusted_time_vec,
                    edge_index_causal,
                    self.phi_model,
                )
                batch_phi_loss_contributions.append(phi_loss_item)

        # Stack augmented features from all batch items
        # Need to handle padding if T_motion varies per batch item (not handled here for simplicity)
        # Assuming T_motion is the same for all items in batch after motion_extractor for this scaffold
        final_augmented_features = torch.stack(
            batch_augmented_features
        )  # (B, T_motion, D_augmented)
        # print(f"Debug: final_augmented_features shape: {final_augmented_features.shape}")

        # Pass to decoder
        logits = self.decoder(final_augmented_features)

        # Calculate losses if targets are provided
        total_loss = None
        task_loss_val = None
        phi_loss_val = None

        if targets is not None:
            task_loss = F.cross_entropy(logits, targets)
            task_loss_val = task_loss.item()

            if batch_phi_loss_contributions:
                phi_loss = torch.stack(batch_phi_loss_contributions).mean()
                phi_loss_val = phi_loss.item()
                total_loss = task_loss + lambda_phi_loss_weight * phi_loss
            else:
                phi_loss_val = 0.0
                total_loss = task_loss

        return logits, total_loss, task_loss_val, phi_loss_val


# %%
# --- Example Usage ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Hyperparameters for demo
BATCH_SIZE = 2
TOTAL_FRAMES = 10  # Number of raw frames in video
FRAME_H, FRAME_W, FRAME_C = 32, 32, 1  # SpookyBench-like noise frames

MOTION_DIM = 32
NUM_FRAMES_IN_CLIP = 3  # For motion extractor
PHI_SPATIAL_DIM = 3
NUM_CLASSES = 5  # e.g., 5 different words/shapes to recognize
K_ATTENTION_PAIRS = 1

# Create dummy video data (Batch, T_total_frames, H, W, C)
dummy_video_data = torch.randn(
    BATCH_SIZE, TOTAL_FRAMES, FRAME_H, FRAME_W, FRAME_C, device=device
)
dummy_targets = torch.randint(0, NUM_CLASSES, (BATCH_SIZE,), device=device)

# Instantiate model
camp_net = CaMPNet(
    input_channels=FRAME_C,
    motion_dim=MOTION_DIM,
    num_frames_in_clip=NUM_FRAMES_IN_CLIP,
    phi_spatial_dim=PHI_SPATIAL_DIM,
    num_classes=NUM_CLASSES,
    k_attention=K_ATTENTION_PAIRS,
).to(device)

print(
    f"CaMPNet instantiated. Number of parameters: {sum(p.numel() for p in camp_net.parameters() if p.requires_grad)}"
)

# --- Training Step Example (very simplified) ---
optimizer = torch.optim.Adam(camp_net.parameters(), lr=1e-4)

camp_net.train()
optimizer.zero_grad()

print("\n--- Forward Pass ---")
logits, total_loss, task_loss, phi_loss_contrib = camp_net(
    dummy_video_data, dummy_targets
)

if total_loss is not None:
    print(f"Logits shape: {logits.shape}")
    print(f"Total Loss: {total_loss.item():.4f}")
    print(f"Task Loss: {task_loss:.4f}")
    print(f"Phi Loss: {phi_loss_contrib:.4f}")

    # Check if loss requires grad
    if total_loss.requires_grad:
        print("Performing backward pass...")
        total_loss.backward()
        optimizer.step()
        print("Backward pass and optimizer step completed.")
    else:
        print("Total loss does not require grad. Skipping backward pass.")
        if not task_loss.requires_grad:
            print("Task loss also no grad.")
        if phi_loss_contrib and not phi_loss_contrib.requires_grad:
            print("Phi loss also no grad (if exists).")


else:
    print("Logits (inference mode):", logits.shape)

# --- Inference Example ---
camp_net.eval()
with torch.no_grad():
    print("\n--- Inference Pass ---")
    inf_logits, _, _, _ = camp_net(dummy_video_data)
    predictions = torch.argmax(inf_logits, dim=1)
    print("Inference Logits shape:", inf_logits.shape)
    print("Predictions:", predictions)


# %%
# --- Assume these are defined elsewhere (from your previous work) ---
# class GravityField(nn.Module): ...
# def project_until_convergence(pairs, spatial, time_vec, phi_model, eps1=1e-5, max_passes=1000): ...
# def causal_phi_loss(node_reps, time_vec, edge_index, phi_model, eps1=1e-5): ...
# For this demo, let's mock them up briefly:
class GravityField(nn.Module):
    def __init__(self, input_dim=4):
        super().__init__()
        self.fc = nn.Linear(input_dim, 1)
        self.softplus = nn.Softplus()

    def forward(self, x):
        return self.softplus(self.fc(x)).squeeze(-1)


def project_until_convergence(
    pairs, spatial, time_vec, phi_model, eps1=1e-5, max_passes=100
):
    # print(f"Debug: Projecting with {len(pairs)} pairs. Initial time_vec: {time_vec[:3]}")
    # In a real scenario, this would modify time_vec
    # For demo, let's just slightly perturb it based on pairs to show it runs
    if pairs:
        time_vec_clone = time_vec.clone()
        for i_child, i_parent in pairs:
            if i_child < len(time_vec_clone) and i_parent < len(time_vec_clone):
                if time_vec_clone[i_child] <= time_vec_clone[i_parent]:
                    time_vec_clone[i_child] = time_vec_clone[
                        i_parent
                    ] + 0.1 * torch.rand(1).to(time_vec.device)
        return time_vec_clone
    return time_vec.clone()


def causal_phi_loss(node_reps, time_vec, edge_index, phi_model, eps1=1e-5):
    # Mock loss
    if edge_index.numel() == 0 or node_reps.numel() == 0:
        return torch.tensor(0.0, device=node_reps.device, requires_grad=True)

    # Ensure phi_model is on the same device as node_reps
    phi_model.to(node_reps.device)

    # Simplified dummy calculation for phi
    # In reality, you'd iterate through edges, calculate ds2, and apply relu(ds2 + eps1)
    # Need to handle batching correctly here if this were batched
    # For this mock, let's assume node_reps is (N, D) and time_vec is (N,)
    # The phi_model expects (N, 1+D)
    phi_inputs = torch.cat([time_vec.unsqueeze(-1), node_reps], dim=-1)

    # Check if phi_inputs is empty
    if phi_inputs.numel() == 0:
        return torch.tensor(0.0, device=node_reps.device, requires_grad=True)

    # Ensure phi_inputs isn't too large for a single forward pass if phi_model is simple
    # This is a mock, so we'll just take a sample if it's too big.
    # A real causal_phi_loss iterates per edge, which handles this implicitly
    # Let's mock the edge iteration
    total_loss_mock = torch.tensor(0.0, device=node_reps.device, requires_grad=True)
    num_edges = edge_index.shape[1]
    if num_edges > 0:
        # Sample a few edges for the mock loss
        sampled_edges_idx = torch.randint(0, num_edges, (min(10, num_edges),))
        sampled_edge_index = edge_index[:, sampled_edges_idx]

        # Iterate through sampled (child, parent) pairs
        for i_child, i_parent in sampled_edge_index.t():
            if i_child < time_vec.shape[0] and i_parent < time_vec.shape[0]:
                t_i = time_vec[i_child]
                x_i = node_reps[i_child]
                t_j = time_vec[i_parent]
                x_j = node_reps[i_parent]

                delta_t = t_i - t_j
                delta_x = x_i - x_j
                dx2 = torch.sum(delta_x**2)

                # Phi at child's location
                coords_i = torch.cat([t_i.view(1, 1), x_i.view(1, -1)], dim=-1)
                phi_i = phi_model(coords_i)[0]

                # Add small epsilon to prevent issues if phi_i is exactly zero or negative
                phi_i = F.softplus(phi_i) + 1e-9  # Ensure phi > 0 for ds2

                ds2 = -phi_i * delta_t**2 + dx2

                # Penalize causal violations (ds2 >= -eps1)
                loss_edge = F.relu(ds2 + eps1)
                total_loss_mock = total_loss_mock + loss_edge

        return (
            total_loss_mock / num_edges
            if num_edges > 0
            else torch.tensor(0.0, device=node_reps.device, requires_grad=True)
        )
    else:
        return torch.tensor(0.0, device=node_reps.device, requires_grad=True)


# --- End of assumed definitions ---


# --- Phase 1: Motion-Sensitive Feature Extraction ---
class MotionFeatureExtractor(nn.Module):
    def __init__(self, input_channels=1, output_motion_dim=64, num_frames_in_clip=3):
        super().__init__()
        self.num_frames_in_clip = num_frames_in_clip
        # Using (2+1)D conv idea: spatial conv then temporal conv
        # This is a very simplified version
        # Input channels will be input_channels * num_frames_in_clip
        self.spatial_conv = nn.Conv2d(
            input_channels * num_frames_in_clip, 32, kernel_size=3, padding=1
        )
        # temporal_pool is not directly used after this spatial conv in this simplified model
        # self.temporal_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(
            32, output_motion_dim
        )  # Assuming pooling flattens spatial dims

        # For frame differencing approach (alternative, simpler)
        # self.cnn = nn.Sequential(
        #     nn.Conv2d(input_channels, 16, kernel_size=3, padding=1),
        #     nn.ReLU(),
        #     nn.AdaptiveAvgPool2d((1,1)), # Pool spatial dimensions
        #     nn.Flatten(),
        #     nn.Linear(16, output_motion_dim)
        # )

    def forward(self, frames_sequence):
        """
        frames_sequence: (Batch, T_total_frames, H, W, C)
        Output: motion_features (Batch, T_motion_steps, D_motion)
        """
        B, T_total, H, W, C = frames_sequence.shape
        frames_sequence = frames_sequence.permute(0, 1, 4, 2, 3)  # (B, T, C, H, W)

        motion_features_list = []
        # Simplified processing: treat each frame as a step, use diffs or short clips
        # For this demo, let's assume a simple CNN on frame differences
        # More sophisticated: sliding window for clips

        # A slightly more plausible (but still simplified) (2+1)D-like
        T_motion_steps = T_total - self.num_frames_in_clip + 1

        if (
            T_motion_steps <= 0
        ):  # Handle very short sequences where no clips can be formed
            dummy_output_dim = self.fc.out_features
            # Return a sequence with 1 time step, containing zeros or a learned embedding
            # For simplicity, return a zero tensor
            return torch.zeros(B, 1, dummy_output_dim, device=frames_sequence.device)

        for t in range(T_motion_steps):
            clip = frames_sequence[
                :, t : t + self.num_frames_in_clip
            ]  # (B, num_frames_in_clip, C, H, W)
            clip = clip.reshape(
                B, self.num_frames_in_clip * C, H, W
            )  # Treat frames in clip as channels

            x = F.relu(self.spatial_conv(clip))  # (B, 32, H, W)
            x = F.adaptive_avg_pool2d(x, (1, 1)).view(B, 32)  # (B, 32)
            motion_features_list.append(self.fc(x))

        motion_features = torch.stack(
            motion_features_list, dim=1
        )  # (B, T_motion_steps, D_motion)
        return motion_features


# --- Phase 2: Temporal Self-Attention and Causal Link Definition ---
class TemporalAttentionModule(nn.Module):
    def __init__(self, motion_dim, num_heads=4, ff_dim=128):
        super().__init__()
        # Ensure embed_dim is divisible by num_heads
        if motion_dim % num_heads != 0:
            # Adjust motion_dim or num_heads if necessary, or add a projection layer
            # For this demo, let's just raise an error or adjust num_heads
            # print(f"Warning: motion_dim ({motion_dim}) not divisible by num_heads ({num_heads}). Adjusting num_heads.")
            # while motion_dim % num_heads != 0 and num_heads > 1:
            #     num_heads -= 1
            # if num_heads == 0: num_heads = 1 # Avoid zero heads
            # print(f"Using num_heads: {num_heads}")
            # Or use a projection:
            # self.proj = nn.Linear(motion_dim, motion_dim - (motion_dim % num_heads))
            # motion_dim = motion_dim - (motion_dim % num_heads)
            pass  # For simplicity in demo, assume motion_dim is set appropriately

        self.attention = nn.MultiheadAttention(
            embed_dim=motion_dim, num_heads=num_heads, batch_first=True
        )
        self.norm1 = nn.LayerNorm(motion_dim)
        self.norm2 = nn.LayerNorm(motion_dim)
        self.ff = nn.Sequential(
            nn.Linear(motion_dim, ff_dim), nn.ReLU(), nn.Linear(ff_dim, motion_dim)
        )

    def forward(self, motion_features):
        """
        motion_features: (Batch, T_motion_steps, D_motion)
        Output:
            contextualized_features: (Batch, T_motion_steps, D_motion)
            attention_weights: (Batch, T_motion_steps, T_motion_steps) - from MHA
        """
        # Self-attention
        # For MHA, query, key, value are the same
        # Add a key_padding_mask if sequence lengths vary in a batch
        attn_output, attn_weights = self.attention(
            motion_features, motion_features, motion_features
        )
        # attn_weights is (Batch, T_query, T_key) which is (B, T_motion, T_motion)

        x = self.norm1(motion_features + attn_output)  # Add & Norm
        # Feed Forward
        ff_output = self.ff(x)
        contextualized_features = self.norm2(x + ff_output)  # Add & Norm

        return contextualized_features, attn_weights


# --- Phase 3: Causal Spacetime Refinement (Utilizing your components) ---
class SpatialProjector(nn.Module):
    def __init__(self, motion_dim, phi_spatial_dim=3):
        super().__init__()
        # Ensure motion_dim is at least phi_spatial_dim, or add a check/projection
        if motion_dim < phi_spatial_dim:
            print(
                f"Warning: motion_dim ({motion_dim}) is less than phi_spatial_dim ({phi_spatial_dim}). Projection will truncate or pad."
            )
            # Could add a check or linear layer to match dims if needed more rigorously

        self.fc = nn.Linear(motion_dim, phi_spatial_dim)

    def forward(self, motion_features):
        """
        motion_features: (Batch, T_motion_steps, D_motion)
        Output: spatial_coords_for_phi (Batch, T_motion_steps, phi_spatial_dim)
        """
        return self.fc(motion_features)


# --- Phase 4: Feature Augmentation and Decoding ---
class Decoder(nn.Module):
    def __init__(self, augmented_feature_dim, num_classes):
        super().__init__()
        # Simple decoder: temporal pooling + MLP
        self.pool = lambda x: torch.mean(x, dim=1)  # Temporal mean pooling
        self.fc = nn.Linear(augmented_feature_dim, num_classes)

    def forward(self, augmented_features_sequence):
        """
        augmented_features_sequence: (Batch, T_motion_steps, D_augmented)
        Output: class_logits (Batch, num_classes)
        """
        # Ensure augmented_features_sequence is not empty before pooling
        if augmented_features_sequence.shape[1] == 0:
            # Return dummy logits or raise error - for classification, zeros might be okay
            # Or handle this upstream by ensuring T_motion_steps >= 1
            dummy_logits = torch.zeros(
                augmented_features_sequence.shape[0],
                self.fc.out_features,
                device=augmented_features_sequence.device,
            )
            return dummy_logits

        pooled_features = self.pool(augmented_features_sequence)
        logits = self.fc(pooled_features)
        return logits


# --- Overall CaMPNet Model ---
class CaMPNet(nn.Module):
    def __init__(
        self,
        input_channels=1,
        motion_dim=64,
        num_frames_in_clip=3,
        phi_spatial_dim=3,
        num_attn_heads=4,
        ff_dim=128,
        num_classes=10,
        k_attention=2,
    ):
        super().__init__()
        self.motion_extractor = MotionFeatureExtractor(
            input_channels, motion_dim, num_frames_in_clip
        )
        self.temporal_attention = TemporalAttentionModule(
            motion_dim, num_attn_heads, ff_dim
        )
        self.spatial_projector = SpatialProjector(motion_dim, phi_spatial_dim)

        # Phi model's input dim is 1 (time) + phi_spatial_dim
        self.phi_model = GravityField(input_dim=1 + phi_spatial_dim)

        # Augmented feature dim: 1(time) + phi_spatial_dim + 1(phi_val) + motion_dim
        augmented_dim = 1 + phi_spatial_dim + 1 + motion_dim
        self.decoder = Decoder(augmented_dim, num_classes)

        self.k_attention = k_attention
        self.phi_spatial_dim = phi_spatial_dim

    def forward(self, frames_sequence, targets=None, lambda_phi_loss_weight=0.1):
        """
        frames_sequence: (Batch, T_total_frames, H, W, C)
        targets: (Batch,) - for calculating task loss (optional, for training)
        """
        B, T_total, H, W, C = frames_sequence.shape

        # Phase 1
        motion_features = self.motion_extractor(
            frames_sequence
        )  # (B, T_motion, D_motion)
        # print(f"Debug: motion_features shape: {motion_features.shape}")

        T_motion = motion_features.shape[1]
        if T_motion == 0:
            print("Warning: Motion extraction resulted in 0 time steps.")
            # Return dummy logits and zero loss
            dummy_logits = torch.zeros(
                B, self.decoder.fc.out_features, device=frames_sequence.device
            )
            return (
                dummy_logits,
                torch.tensor(0.0, device=frames_sequence.device),
                0.0,
                0.0,
            )

        # Phase 2
        context_motion_features, attention_weights = self.temporal_attention(
            motion_features
        )
        # attention_weights: (B, T_motion, T_motion)
        # print(f"Debug: context_motion_features shape: {context_motion_features.shape}")

        # Phase 3 (Iterate per batch item due to project_until_convergence and pair extraction logic)
        batch_augmented_features = []
        batch_phi_loss_contributions = []

        for b_idx in range(B):
            current_motion_features = context_motion_features[
                b_idx
            ]  # (T_motion, D_motion)
            current_attention = attention_weights[b_idx]  # (T_motion, T_motion)

            # Extract causal pairs from attention for this batch item
            pairs = []
            for i in range(T_motion):  # child
                if i == 0:
                    continue
                att_to_past = current_attention[i, :i]
                if att_to_past.numel() == 0:
                    continue

                current_k = min(self.k_attention, att_to_past.numel())
                if current_k == 0:
                    continue

                # Select top k, ensuring k is not greater than number of past tokens
                # Filter by attention score threshold if needed (not done here)
                topk_scores, topk_indices_relative = torch.topk(att_to_past, current_k)

                for k_val_idx in range(current_k):
                    # Only consider non-zero attention scores for pairs
                    # if topk_scores[k_val_idx] > 1e-6: # Optional: threshold
                    j = topk_indices_relative[k_val_idx].item()
                    pairs.append((i, j))  # child_idx, parent_idx

            # print(f"Debug: Batch {b_idx}, num_pairs: {len(pairs)}")

            # Project to "phi-spatial" space
            spatial_coords_for_phi = self.spatial_projector(
                current_motion_features.unsqueeze(0)
            ).squeeze(0)  # (T_motion, phi_spatial_dim)
            # print(f"Debug: spatial_coords_for_phi shape: {spatial_coords_for_phi.shape}")

            # Initial time vector
            # Use sequential time as a common baseline for projection
            initial_time_vec = torch.arange(
                T_motion, dtype=torch.float32, device=frames_sequence.device
            )

            # Project until convergence
            # Ensure phi_model is on the correct device for this call
            self.phi_model.to(spatial_coords_for_phi.device)
            adjusted_time_vec = project_until_convergence(
                pairs, spatial_coords_for_phi, initial_time_vec, self.phi_model
            )
            # print(f"Debug: adjusted_time_vec shape: {adjusted_time_vec.shape}")

            # Phase 4: Augmentation for this batch item
            augmented_features_this_item = []
            for t_step in range(T_motion):
                # Ensure t_refined is (1, 1) and s_t is (1, phi_spatial_dim) for phi_model input
                t_refined = adjusted_time_vec[t_step].view(1, 1)
                s_t = spatial_coords_for_phi[t_step]  # Shape is (phi_spatial_dim,)
                s_t_unsqueeze = s_t.view(1, -1)  # Shape is (1, phi_spatial_dim)

                # Ensure inputs to phi_model are correctly shaped and on device
                phi_input = torch.cat(
                    [t_refined, s_t_unsqueeze], dim=-1
                )  # Shape is (1, 1 + phi_spatial_dim)
                phi_val = self.phi_model(phi_input).unsqueeze(
                    0
                )  # phi_model returns (1,) after squeeze, unsqueeze back to (1,1)

                original_motion_vec = current_motion_features[t_step]  # (D_motion)

                # When concatenating for augmented_features, we want shape (1 + phi_spatial_dim + 1 + D_motion)
                # So, cat 1D tensors
                aug_feat = torch.cat(
                    [
                        t_refined.squeeze(0),  # 1D (1,)
                        s_t,  # 1D (phi_spatial_dim,)
                        phi_val.squeeze(0),  # 1D (1,)
                        original_motion_vec,  # 1D (D_motion,)
                    ]
                )
                augmented_features_this_item.append(aug_feat)

            if not augmented_features_this_item:  # Should not happen if T_motion > 0
                # Create a dummy feature of expected size and add a time dimension of 1
                aug_dim = 1 + self.phi_spatial_dim + 1 + motion_features.shape[-1]
                dummy_aug_feat = torch.zeros(aug_dim, device=frames_sequence.device)
                batch_augmented_features.append(
                    dummy_aug_feat.unsqueeze(0)
                )  # Add time step dim

            else:
                batch_augmented_features.append(
                    torch.stack(augmented_features_this_item)
                )

            # Calculate phi_loss for this item
            phi_loss_item = torch.tensor(
                0.0, device=frames_sequence.device, requires_grad=True
            )  # Initialize to 0
            if pairs:  # Only compute phi loss if there are causal pairs
                # Edge index for causal_phi_loss: (2, num_edges) -> [[child1,...], [parent1,...]]
                # project_until_convergence pairs are (child, parent)
                # causal_phi_loss expects edge_index.t() to be (child, parent)
                # So edge_index should be [[child1,...], [parent1,...]]
                edge_index_causal = torch.tensor(
                    pairs, dtype=torch.long, device=frames_sequence.device
                ).t()

                # Ensure spatial_coords_for_phi and adjusted_time_vec have gradients if needed for causal_phi_loss
                # Depending on how causal_phi_loss is implemented, it might expect gradients on node_reps/time_vec
                # if it's part of an end-to-end training loop.
                # For now, keep them detached as they come from projection.
                # If training phi end-to-end with GNN, requires_grad_(True) might be needed here.
                phi_loss_item = causal_phi_loss(
                    spatial_coords_for_phi.detach(),  # Pass detached
                    adjusted_time_vec.detach(),  # Pass detached
                    edge_index_causal,
                    self.phi_model,
                )
            batch_phi_loss_contributions.append(phi_loss_item)

        # Stack augmented features from all batch items
        # Need to handle padding if T_motion varies per batch item (not handled here for simplicity)
        # Assuming T_motion is the same for all items in batch after motion_extractor for this scaffold
        final_augmented_features = torch.stack(
            batch_augmented_features
        )  # (B, T_motion, D_augmented)
        # print(f"Debug: final_augmented_features shape: {final_augmented_features.shape}")

        # Pass to decoder
        logits = self.decoder(final_augmented_features)

        # Calculate losses if targets are provided
        total_loss = None
        task_loss_val = None
        phi_loss_val = None

        if targets is not None:
            task_loss = F.cross_entropy(logits, targets)
            task_loss_val = task_loss.item()

            # Stack phi losses across the batch and average
            if batch_phi_loss_contributions:
                # Need to handle if any item had 0 pairs, ensuring the tensor stack works
                # If all contributions are scalar tensors, stack will work fine.
                # If some are None or differently shaped, need handling.
                # With the fix above ensuring phi_loss_item is always a scalar tensor, this should work.
                phi_loss = torch.stack(batch_phi_loss_contributions).mean()
                phi_loss_val = phi_loss.item()
                total_loss = task_loss + lambda_phi_loss_weight * phi_loss
            else:
                phi_loss_val = 0.0  # No pairs in any batch item
                total_loss = task_loss

        return logits, total_loss, task_loss_val, phi_loss_val

    # %%

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Hyperparameters for demo
    BATCH_SIZE = 2
    TOTAL_FRAMES = 10  # Number of raw frames in video
    FRAME_H, FRAME_W, FRAME_C = 32, 32, 1  # SpookyBench-like noise frames

    MOTION_DIM = 32
    NUM_FRAMES_IN_CLIP = (
        3  # For motion extractor. T_motion = TOTAL_FRAMES - NUM_FRAMES_IN_CLIP + 1
    )
    PHI_SPATIAL_DIM = 3
    NUM_CLASSES = 5  # e.g., 5 different words/shapes to recognize
    K_ATTENTION_PAIRS = 1

    # Create dummy video data (Batch, T_total_frames, H, W, C)
    dummy_video_data = torch.randn(
        BATCH_SIZE, TOTAL_FRAMES, FRAME_H, FRAME_W, FRAME_C, device=device
    )
    dummy_targets = torch.randint(0, NUM_CLASSES, (BATCH_SIZE,), device=device)

    # Instantiate model
    camp_net = CaMPNet(
        input_channels=FRAME_C,
        motion_dim=MOTION_DIM,
        num_frames_in_clip=NUM_FRAMES_IN_CLIP,
        phi_spatial_dim=PHI_SPATIAL_DIM,
        num_classes=NUM_CLASSES,
        k_attention=K_ATTENTION_PAIRS,
    ).to(device)

    print(
        f"CaMPNet instantiated. Number of parameters: {sum(p.numel() for p in camp_net.parameters() if p.requires_grad)}"
    )

    # --- Training Step Example (very simplified) ---
    optimizer = torch.optim.Adam(camp_net.parameters(), lr=1e-4)

    camp_net.train()
    optimizer.zero_grad()

    print("\n--- Forward Pass ---")
    logits, total_loss, task_loss, phi_loss_contrib = camp_net(
        dummy_video_data, dummy_targets
    )

    if total_loss is not None:
        print(f"Logits shape: {logits.shape}")
        print(f"Total Loss: {total_loss.item():.4f}")
        print(f"Task Loss: {task_loss:.4f}")
        print(f"Phi Loss: {phi_loss_contrib:.4f}")

        # Check if loss requires grad
        if total_loss.requires_grad:
            print("Performing backward pass...")
            total_loss.backward()
            optimizer.step()
            print("Backward pass and optimizer step completed.")
        else:
            print("Total loss does not require grad. Skipping backward pass.")
            # Check components if total_loss doesn't require grad unexpectedly
            # if task_loss is not None and not torch.is_tensor(task_loss): print("Task loss is not a tensor.")
            # elif task_loss is not None and not task_loss.requires_grad: print("Task loss requires_grad=False.")
            # if phi_loss_contrib is not None and not torch.is_tensor(phi_loss_contrib): print("Phi loss is not a tensor.")
            # elif phi_loss_contrib is not None and torch.is_tensor(phi_loss_contrib) and not phi_loss_contrib.requires_grad: print("Phi loss requires_grad=False.")

    else:
        print("Logits (inference mode):", logits.shape)

    # --- Inference Example ---
    camp_net.eval()
    with torch.no_grad():
        print("\n--- Inference Pass ---")
        inf_logits, _, _, _ = camp_net(dummy_video_data)
        predictions = torch.argmax(inf_logits, dim=1)
        print("Inference Logits shape:", inf_logits.shape)
        print("Predictions:", predictions)


from torchvision import transforms
from datasets import load_dataset
import numpy as np
import random
# --- Paste all your previously defined classes and functions here ---
# GravityField, MotionFeatureExtractor, TemporalAttentionModule,
# SpatialProjector, Decoder, CaMPNet,
# project_until_convergence, causal_phi_loss
# (Using the mocked versions from the scaffold for this self-contained example)


class GravityField(nn.Module):
    def __init__(self, input_dim=4):
        super().__init__()
        self.fc = nn.Linear(input_dim, 1)
        self.softplus = nn.Softplus()

    def forward(self, x):
        return self.softplus(self.fc(x)).squeeze(-1)


def project_until_convergence(
    pairs, spatial, time_vec, phi_model, eps1=1e-5, max_passes=100
):
    if pairs:
        time_vec_clone = time_vec.clone()
        for i_child, i_parent in pairs:
            if i_child < len(time_vec_clone) and i_parent < len(time_vec_clone):
                if time_vec_clone[i_child] <= time_vec_clone[i_parent]:
                    time_vec_clone[i_child] = time_vec_clone[
                        i_parent
                    ] + 0.1 * torch.rand(1).to(time_vec.device)
        return time_vec_clone
    return time_vec.clone()


def causal_phi_loss(node_reps, time_vec, edge_index, phi_model, eps1=1e-5):
    if edge_index.numel() == 0 or node_reps.numel() == 0:
        return torch.tensor(0.0, device=node_reps.device, requires_grad=True)
    phi_model.to(node_reps.device)
    phi_inputs = torch.cat([time_vec.unsqueeze(-1), node_reps], dim=-1)
    if phi_inputs.numel() == 0:
        return torch.tensor(0.0, device=node_reps.device, requires_grad=True)
    sample_size = min(10, phi_inputs.shape[0])
    if sample_size > 0:
        phi_values = phi_model(phi_inputs[:sample_size])
        return F.relu(phi_values.mean() - 1.0)
    else:
        return torch.tensor(0.0, device=node_reps.device, requires_grad=True)


class MotionFeatureExtractor(nn.Module):
    def __init__(self, input_channels=1, output_motion_dim=64, num_frames_in_clip=3):
        super().__init__()
        self.num_frames_in_clip = num_frames_in_clip
        self.spatial_conv = nn.Conv2d(
            input_channels * num_frames_in_clip, 32, kernel_size=3, padding=1
        )
        self.fc = nn.Linear(32, output_motion_dim)

    def forward(self, frames_sequence):
        B, T_total, H, W, C = frames_sequence.shape
        frames_sequence = frames_sequence.permute(0, 1, 4, 2, 3)
        motion_features_list = []
        T_motion_steps = (
            T_total - self.num_frames_in_clip + 1
            if T_total >= self.num_frames_in_clip
            else 1
        )
        if T_total < self.num_frames_in_clip:
            dummy_output_dim = self.fc.out_features
            return torch.randn(B, 1, dummy_output_dim, device=frames_sequence.device)
        for t in range(T_motion_steps):
            clip = frames_sequence[:, t : t + self.num_frames_in_clip]
            clip = clip.reshape(B, self.num_frames_in_clip * C, H, W)
            x = F.relu(self.spatial_conv(clip))
            x = F.adaptive_avg_pool2d(x, (1, 1)).view(B, 32)
            motion_features_list.append(self.fc(x))
        if not motion_features_list:
            dummy_output_dim = self.fc.out_features
            return torch.randn(B, 1, dummy_output_dim, device=frames_sequence.device)
        motion_features = torch.stack(motion_features_list, dim=1)
        return motion_features


class TemporalAttentionModule(nn.Module):
    def __init__(self, motion_dim, num_heads=4, ff_dim=128):
        super().__init__()
        self.attention = nn.MultiheadAttention(
            embed_dim=motion_dim, num_heads=num_heads, batch_first=True
        )
        self.norm1 = nn.LayerNorm(motion_dim)
        self.norm2 = nn.LayerNorm(motion_dim)
        self.ff = nn.Sequential(
            nn.Linear(motion_dim, ff_dim), nn.ReLU(), nn.Linear(ff_dim, motion_dim)
        )

    def forward(self, motion_features):
        attn_output, attn_weights = self.attention(
            motion_features, motion_features, motion_features
        )
        x = self.norm1(motion_features + attn_output)
        ff_output = self.ff(x)
        contextualized_features = self.norm2(x + ff_output)
        return contextualized_features, attn_weights


class SpatialProjector(nn.Module):
    def __init__(self, motion_dim, phi_spatial_dim=3):
        super().__init__()
        self.fc = nn.Linear(motion_dim, phi_spatial_dim)

    def forward(self, motion_features):
        return self.fc(motion_features)


class Decoder(nn.Module):
    def __init__(self, augmented_feature_dim, num_classes):
        super().__init__()
        self.pool = lambda x: torch.mean(x, dim=1)
        self.fc = nn.Linear(augmented_feature_dim, num_classes)

    def forward(self, augmented_features_sequence):
        pooled_features = self.pool(augmented_features_sequence)
        logits = self.fc(pooled_features)
        return logits


class CaMPNet(nn.Module):
    def __init__(
        self,
        input_channels=1,
        motion_dim=64,
        num_frames_in_clip=3,
        phi_spatial_dim=3,
        num_attn_heads=4,
        ff_dim=128,
        num_classes=10,
        k_attention=2,
    ):
        super().__init__()
        self.motion_extractor = MotionFeatureExtractor(
            input_channels, motion_dim, num_frames_in_clip
        )
        self.temporal_attention = TemporalAttentionModule(
            motion_dim, num_attn_heads, ff_dim
        )
        self.spatial_projector = SpatialProjector(motion_dim, phi_spatial_dim)
        self.phi_model = GravityField(input_dim=1 + phi_spatial_dim)
        augmented_dim = 1 + phi_spatial_dim + 1 + motion_dim
        self.decoder = Decoder(augmented_dim, num_classes)
        self.k_attention = k_attention
        self.phi_spatial_dim = phi_spatial_dim

    def forward(self, frames_sequence, targets=None, lambda_phi_loss_weight=0.1):
        B, T_total, H, W, C = frames_sequence.shape
        motion_features = self.motion_extractor(frames_sequence)
        context_motion_features, attention_weights = self.temporal_attention(
            motion_features
        )
        batch_augmented_features = []
        batch_phi_loss_contributions = []
        for b_idx in range(B):
            current_motion_features = context_motion_features[b_idx]
            current_attention = attention_weights[b_idx]
            T_motion = current_motion_features.shape[0]
            if T_motion == 0:  # Handle case where no motion features are produced
                # Fallback: create a dummy feature sequence of length 1
                aug_dim = 1 + self.phi_spatial_dim + 1 + motion_features.shape[-1]
                dummy_aug_feat_seq = torch.zeros(
                    1, aug_dim, device=frames_sequence.device
                )
                batch_augmented_features.append(dummy_aug_feat_seq)
                continue

            pairs = []
            for i in range(T_motion):
                if i == 0:
                    continue
                att_to_past = current_attention[i, :i]
                if att_to_past.numel() == 0:
                    continue
                current_k = min(self.k_attention, att_to_past.numel())
                if current_k == 0:
                    continue
                _, topk_indices_relative = torch.topk(att_to_past, current_k)
                for k_val_idx in range(current_k):
                    j = topk_indices_relative[k_val_idx].item()
                    pairs.append((i, j))
            spatial_coords_for_phi = self.spatial_projector(
                current_motion_features.unsqueeze(0)
            ).squeeze(0)
            initial_time_vec = torch.arange(
                T_motion, dtype=torch.float32, device=frames_sequence.device
            )
            self.phi_model.to(
                spatial_coords_for_phi.device
            )  # Ensure phi_model on correct device
            adjusted_time_vec = project_until_convergence(
                pairs, spatial_coords_for_phi, initial_time_vec, self.phi_model
            )
            augmented_features_this_item = []
            for t_step in range(T_motion):
                t_refined = adjusted_time_vec[t_step].unsqueeze(0)
                s_t = spatial_coords_for_phi[t_step]
                phi_input = torch.cat([t_refined, s_t.unsqueeze(0)], dim=-1)
                phi_val = self.phi_model(phi_input).unsqueeze(0)
                original_motion_vec = current_motion_features[t_step]
                aug_feat = torch.cat(
                    [t_refined, s_t, phi_val.squeeze(0), original_motion_vec]
                )
                augmented_features_this_item.append(aug_feat)
            batch_augmented_features.append(torch.stack(augmented_features_this_item))
            if pairs:
                edge_index_causal = torch.tensor(
                    pairs, dtype=torch.long, device=frames_sequence.device
                ).t()
                phi_loss_item = causal_phi_loss(
                    spatial_coords_for_phi,
                    adjusted_time_vec,
                    edge_index_causal,
                    self.phi_model,
                )
                batch_phi_loss_contributions.append(phi_loss_item)

        max_len = 0
        if batch_augmented_features:  # Check if list is not empty
            max_len = max(f.shape[0] for f in batch_augmented_features if f.numel() > 0)

        padded_augmented_features = []
        if max_len > 0:  # Proceed only if there's something to pad
            for feat_seq in batch_augmented_features:
                if (
                    feat_seq.numel() == 0
                ):  # Handle completely empty feature sequences if they occur
                    pad_len = max_len
                else:
                    pad_len = max_len - feat_seq.shape[0]

                if pad_len > 0:
                    padding = torch.zeros(
                        pad_len, feat_seq.shape[1], device=feat_seq.device
                    )
                    padded_augmented_features.append(
                        torch.cat([feat_seq, padding], dim=0)
                    )
                elif feat_seq.numel() > 0:  # Only append if it's not empty
                    padded_augmented_features.append(feat_seq)

        if not padded_augmented_features:  # If everything was empty or became empty
            # This means no valid features were processed, decoder will likely fail or get zeros
            # Create a dummy tensor that the decoder can accept to prevent crashing
            # This indicates a deeper issue with feature extraction or data.
            print("Warning: No valid augmented features to process for the batch.")
            aug_dim_fallback = (
                1 + self.phi_spatial_dim + 1 + self.motion_extractor.fc.out_features
            )  # Example dim
            final_augmented_features = torch.zeros(
                B, 1, aug_dim_fallback, device=frames_sequence.device
            )

        else:
            final_augmented_features = torch.stack(padded_augmented_features)

        logits = self.decoder(final_augmented_features)
        total_loss, task_loss_val, phi_loss_val = None, None, None
        if targets is not None:
            task_loss = F.cross_entropy(logits, targets)
            task_loss_val = task_loss.item()
            if batch_phi_loss_contributions:
                phi_loss = torch.stack(batch_phi_loss_contributions).mean()
                phi_loss_val = phi_loss.item()
                total_loss = task_loss + lambda_phi_loss_weight * phi_loss
            else:
                phi_loss_val = 0.0
                total_loss = task_loss
        return logits, total_loss, task_loss_val, phi_loss_val


# --- Dataset and DataLoader ---
# Model Hyperparameters (should match CaMPNet instantiation)
# --- Dataset and DataLoader ---
# Model Hyperparameters (should match CaMPNet instantiation)
TARGET_TOTAL_FRAMES = 30  # Max frames to process per video for consistency
FRAME_H, FRAME_W = 32, 32  # Resize frames to this
FRAME_C = 1  # Grayscale

# Define CATEGORY_FIELD_NAME here
CATEGORY_FIELD_NAME = (
    "label_text_content"  # e.g. "label_text_content" for text category
)


class SpookyDataset(Dataset):
    def __init__(
        self,
        hf_dataset,
        target_frames,
        frame_h,
        frame_w,
        frame_c,
        label_map,
        category_field_name,
    ):
        self.hf_dataset = hf_dataset
        self.target_frames = target_frames
        self.category_field_name = category_field_name  # e.g. "label_text_content"

        self.transform = transforms.Compose(
            [
                transforms.ToPILImage(),  # If frames are numpy arrays
                transforms.Resize((frame_h, frame_w)),
                transforms.Grayscale(num_output_channels=frame_c),
                transforms.ToTensor(),  # Scales to [0, 1]
                # Add normalization if needed, e.g., transforms.Normalize(mean=[0.5], std=[0.5])
            ]
        )
        self.label_map = label_map

    def __len__(self):
        return len(self.hf_dataset)

    def __getitem__(self, idx):
        item = self.hf_dataset[idx]

        # Video frames: item['video'] is usually a list of PIL Images or numpy arrays
        # The 'video' key might change based on how HF structures it, often it's a path or bytes.
        # For this example, we assume item['video_frames'] gives a list of NumPy arrays [H,W,C]
        # This part is highly dependent on the exact structure of the HF dataset item.
        # A common way HF datasets handles video is by providing a path in item['video']
        # and you'd use a library like decord to load it.
        # For SpookyBench, the 'video' feature is a Video object.
        # We need to extract frames.

        video_path = item[
            "video"
        ]  # This is usually how HF datasets provides video files
        # For this dataset, the 'video' column gives paths to .mp4 files.
        # We need to load these frames. A robust way is using decord or OpenCV.
        # Simplified for now: let's assume we get a list of frames
        # In a real scenario, you'd use:
        # from decord import VideoReader, cpu
        # vr = VideoReader(video_path, ctx=cpu(0))
        # frames = [vr[i].asnumpy() for i in range(len(vr))]

        # MOCKING FRAME LOADING FOR THIS EXAMPLE due to complexity of direct HF video loading in scaffold
        # In a real setup, you MUST load frames from item['video'] (which is a path)
        # For example: frames = load_frames_from_path(item['video'])
        num_actual_frames = random.randint(
            self.target_frames // 2, self.target_frames * 2
        )  # Simulate variable length
        frames = [
            np.random.rand(64, 64, 3).astype(np.uint8) * 255
            for _ in range(num_actual_frames)
        ]  # HWC, RGB dummy

        processed_frames = []
        for frame_np in frames:  # Assuming frame is HWC numpy array
            processed_frames.append(self.transform(frame_np))

        video_tensor = torch.stack(processed_frames)  # (T_actual, C, H, W)

        # Pad or truncate frames
        current_num_frames = video_tensor.shape[0]
        if current_num_frames > self.target_frames:
            video_tensor = video_tensor[: self.target_frames]
        elif current_num_frames < self.target_frames:
            padding = torch.zeros(
                self.target_frames - current_num_frames, FRAME_C, FRAME_H, FRAME_W
            )
            video_tensor = torch.cat([video_tensor, padding], dim=0)

        video_tensor = video_tensor.permute(0, 2, 3, 1)  # (T, H, W, C) for CaMPNet

        # Label
        label_str = item[self.category_field_name]
        label_id = self.label_map.get(label_str, -1)  # -1 for unknown, handle as needed
        if label_id == -1:
            print(
                f"Warning: Unknown label '{label_str}' for item {idx}. Assigning to first class."
            )
            label_id = 0

        return video_tensor, torch.tensor(label_id, dtype=torch.long)


# --- Main Execution ---
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # --- 1. Load SpookyBench Dataset ---
    print("Loading SpookyBench dataset...")
    try:
        # Load a small subset for quick testing, e.g., the 'text' configuration
        # The dataset has configurations: 'text', 'shapes', 'object_images', 'dynamic_scenes'
        # We'll pick 'text' for this demo.
        # You might need to authenticate with Hugging Face if it's a private or gated dataset
        # For public ones, this should work.
        # spooky_hf_dataset = load_dataset("timeblindness/spooky-bench", name="text", split='train[:1%]') # Load 1% of train
        spooky_hf_dataset_train = load_dataset(
            "timeblindness/spooky-bench", name="text", split="train[:10]"
        )  # 10 samples
        spooky_hf_dataset_test = load_dataset(
            "timeblindness/spooky-bench", name="text", split="test[:10]"
        )
        print(
            f"Loaded {len(spooky_hf_dataset_train)} train samples and {len(spooky_hf_dataset_test)} test samples for 'text' category."
        )
        print("Sample item:", spooky_hf_dataset_train[0])
        # The 'label_text_content' field seems to hold the actual word.
        # CATEGORY_FIELD_NAME = "label_text_content" # Moved definition outside __main__

    except Exception as e:
        print(f"Error loading dataset: {e}")
        print(
            "Please ensure you have 'ffmpeg' and 'decord' installed and are connected to the internet."
        )
        print(
            "You might also need to log in to Hugging Face if the dataset requires authentication: `huggingface-cli login`"
        )
        exit()

    # --- 2. Create Label Map ---
    # For 'text' category, labels are the words themselves.
    # For a real scenario, collect all unique labels from the chosen split/category.
    # For this demo, we'll extract from the loaded small subset.

    # First, ensure the 'label_text_content' feature is what we expect
    if CATEGORY_FIELD_NAME not in spooky_hf_dataset_train.features:
        print(
            f"Error: The expected label field '{CATEGORY_FIELD_NAME}' is not in the dataset features."
        )
        print(f"Available features: {spooky_hf_dataset_train.features}")
        exit()

# prompt: Generate a complex problem for the new predictor

import numpy as np


class AdvancedCausalPredictor(nn.Module):
    def __init__(
        self,
        input_feature_dim=769,  # e.g., BERT hidden_dim + 1 (for time)
        hidden_dim=256,
        output_dim=1,  # e.g., predicting a scalar value or depth
        num_layers=2,
        dropout_prob=0.1,
    ):
        super().__init__()
        self.fc_in = nn.Linear(input_feature_dim, hidden_dim)
        self.layers = nn.ModuleList()
        for _ in range(num_layers - 1):
            self.layers.append(
                nn.Sequential(
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(dropout_prob),
                )
            )
        self.fc_out = nn.Linear(hidden_dim, output_dim)

    def forward(self, augmented_features):
        """
        augmented_features: Tensor of shape (seq_len, input_feature_dim)
                          This is the output from causal_refine_embeddings_with_phi
        Returns:
            predictions: Tensor of shape (seq_len, output_dim)
        """
        x = F.relu(self.fc_in(augmented_features))
        for layer in self.layers:
            x = layer(x)
        predictions = self.fc_out(x)
        return predictions


# --- Generate a complex problem for the new predictor ---

print("\n--- Generating a complex prediction problem ---")

# Re-use components from previous cells for demonstration
# Assume:
# - causal_refine_embeddings_with_phi, GravityField, project_until_convergence, causal_phi_loss are defined
# - demo_phi_model is a trained or initialized GravityField model
# - dummy_bert_embeddings and dummy_attention_matrix are available

# Problem: Predict a synthetic "causal property" for each token in the sequence.
# The property depends non-linearly on the token's refined time, spatial coordinates,
# and its original embedding features. This forces the predictor to leverage
# the causally refined spacetime features.

SEQ_LEN = 100
HIDDEN_DIM_BERT = 768
PHI_MODEL_SPATIAL_DIMS = 3
NUM_PREDICTION_OUTPUT_DIMS = 1  # Predicting a single scalar property per token

# Assume dummy_bert_embeddings and dummy_attention_matrix are defined and on device
# (See previous cells for their creation)

# Ensure the demo_phi_model is initialized and on the correct device
phi_input_dim_actual = 1 + PHI_MODEL_SPATIAL_DIMS
if "demo_phi_model" not in locals():
    print("Initializing a dummy phi_model for the problem demo.")
    demo_phi_model = GravityField(input_dim=phi_input_dim_actual).to(device)
    # Optional: Add some dummy "training" or random weights to make phi non-constant
    # demo_phi_model.fc.weight.data.uniform_(-0.1, 0.1)
    # demo_phi_model.fc.bias.data.uniform_(-0.1, 0.1)


# Run the causal refinement pipeline to get the augmented features
print("Running causal refinement for problem generation...")
refined_coords, refined_embs = causal_refine_embeddings_with_phi(
    dummy_bert_embeddings,
    dummy_attention_matrix,
    demo_phi_model,
    initial_time_strategy="attention_based",  # Match strategy from demo
    k_attention=K_ATTENTION_PAIRS,  # Match strategy from demo
    phi_input_dims=PHI_MODEL_SPATIAL_DIMS,
    epsilon=1e-5,
    max_passes_projection=1000,
)

print(
    f"Refined embeddings shape after causal_refine_embeddings_with_phi: {refined_embs.shape}"
)

# The `refined_embs` now has shape (seq_len, 1 + hidden_dim)
# The input_feature_dim for the new predictor should match the last dimension of refined_embs.
input_feature_dim_predictor = refined_embs.shape[-1]

# Create the new predictor model
new_predictor = AdvancedCausalPredictor(
    input_feature_dim=input_feature_dim_predictor,
    hidden_dim=256,  # Example hidden size
    output_dim=NUM_PREDICTION_OUTPUT_DIMS,  # Predict 1 scalar per token
).to(device)

print(
    f"New predictor instantiated with input_feature_dim={input_feature_dim_predictor}"
)

# Generate synthetic targets based on refined features
# This creates a prediction problem that requires the model to use the refined features
print("Generating synthetic targets...")
synthetic_targets = (
    torch.sin(refined_coords[:, 0] * 2 * np.pi)
    + torch.cos(refined_coords[:, 1] * np.pi)
    + torch.sigmoid(refined_embs[:, 5].float()) * 2.0
)  # Use a feature from original BERT + spacetime
synthetic_targets = synthetic_targets.unsqueeze(-1)  # (seq_len, 1)
# Add some noise
synthetic_targets += torch.randn_like(synthetic_targets) * 0.1
print(f"Synthetic targets shape: {synthetic_targets.shape}")

# --- Demo Training Loop for the new predictor ---
print("\n--- Demo Training Loop for New Predictor ---")
predictor_optimizer = torch.optim.Adam(new_predictor.parameters(), lr=1e-3)
num_predictor_epochs = 200

new_predictor.train()

for epoch in range(num_predictor_epochs):
    predictor_optimizer.zero_grad()

    # Feed the refined embeddings into the predictor
    predictions = new_predictor(refined_embs)

    # Calculate loss (e.g., MSE)
    loss = F.mse_loss(predictions, synthetic_targets)

    loss.backward()
    predictor_optimizer.step()

    if epoch % 20 == 0 or epoch == num_predictor_epochs - 1:
        print(
            f"Predictor Epoch {epoch + 1}/{num_predictor_epochs}, MSE Loss: {loss.item():.6f}"
        )

print("Predictor training demo complete.")

# --- Evaluate the predictor (on training data for demo) ---
new_predictor.eval()
with torch.no_grad():
    final_predictions = new_predictor(refined_embs)
    final_mse = F.mse_loss(final_predictions, synthetic_targets)
    print(f"\nFinal Predictor MSE on synthetic targets: {final_mse.item():.6f}")

    # Print some example comparisons
    print("\nExample Predictions vs. Targets:")
    for i in range(min(5, SEQ_LEN)):
        print(
            f"Token {i}: Target={synthetic_targets[i].item():.4f}, Prediction={final_predictions[i].item():.4f}"
        )
        # Optionally, include refined time and spatial coords for context
        print(
            f"  (Refined t: {refined_coords[i, 0].item():.4f}, x: {refined_coords[i, 1].item():.4f}, y: {refined_coords[i, 2].item():.4f})"
        )

# prompt: generate an optimization problem for the embedder


class CausalOptimizationProblem:
    def __init__(self, embedder, phi_model, hierarchy_pairs, lambda_phi=0.1):
        """
        Defines an optimization problem for training an embedder (like a GNN)
        co-trained with a phi_model, regularized by causal constraints.

        Args:
            embedder: The model generating node embeddings (e.g., GravityAwareGNN).
                      It should take node features and edge_index.
            phi_model: The GravityField model.
            hierarchy_pairs: A list of (child_idx, parent_idx) tuples.
            lambda_phi: Weight for the causal phi loss.
        """
        self.embedder = embedder
        self.phi_model = phi_model
        self.hierarchy_pairs = hierarchy_pairs
        self.lambda_phi = lambda_phi
        self.edge_index_causal = (
            torch.tensor(hierarchy_pairs, dtype=torch.long, device=device)
            .t()
            .contiguous()
        )
        if self.edge_index_causal.nelement() == 0:
            self.edge_index_causal = torch.empty(
                (2, 0), dtype=torch.long, device=device
            )

        # Assuming embedder output will be used as 'spatial' for causal_phi_loss
        # Or, if features contain spatial coords, use those.
        # Let's assume the embedder's output *is* the new spatial embedding
        # or contains the spatial part.
        # For the provided example, the embedder outputs a single value per node (out_dim=1),
        # which cannot serve as spatial coords directly.
        # Let's assume the *input* features contain the spatial part used for phi loss.
        # The node_features fed into the GNN are [adjusted_time, spatial, phi_values].
        # We need the 'spatial' component from these *input* features for the loss.
        # We will need to pass the original spatial coordinates or extract them.

    def calculate_loss(self, node_features, gnn_edge_index, gnn_targets):
        """
        Calculates the total loss including the task loss and the causal phi loss.

        Args:
            node_features: Features fed into the embedder [adjusted_time, spatial, phi_values].
                           Shape (N, 1 + D_spatial + 1 + ...)
            gnn_edge_index: Edge index for the GNN (parent, child). Shape (2, num_gnn_edges).
            gnn_targets: Targets for the embedder's prediction task (e.g., original depth).
                         Shape (N, out_dim).

        Returns:
            total_loss, task_loss_val, phi_loss_val
        """
        # Forward pass through the embedder (GNN)
        gnn_output = self.embedder(node_features, gnn_edge_index)

        # Task loss (e.g., MSE for regression)
        task_loss = F.mse_loss(gnn_output, gnn_targets)
        task_loss_val = task_loss.item()

        # Causal phi loss
        phi_loss_val = 0.0
        c_phi_loss = torch.tensor(0.0, device=node_features.device, requires_grad=True)

        if self.edge_index_causal.shape[1] > 0 and self.lambda_phi > 0:
            # Extract spatial coordinates from the node_features
            # Assuming spatial coordinates are after time (dim 0) in node_features
            # And assuming they match the dimension phi_model expects for spatial input
            # The spatial dims are assumed to be node_features[:, 1 : 1 + PHI_MODEL_SPATIAL_DIMS]
            # We need PHI_MODEL_SPATIAL_DIMS from the context. Let's assume it's 3 for this example.
            # In a real implementation, this needs to be passed or derived.

            # Let's extract time and spatial coords from node_features assuming structure: [time, spatial..., ...]
            # The spatial dimensions are the ones used for ds^2 calculation.
            # These are the 'spatial' inputs that were projected by the SpatialProjector in CaMPNet,
            # or the initial spatial embeddings used in the WordNet example.
            # For the WordNet example, this was the `spatial` tensor (N, D_spatial).
            # The node_features fed into the GNN were derived *from* this `spatial` and the `adjusted_time_vec`.
            # So, the `spatial` for the causal_phi_loss should be the *original* spatial embeddings.
            # This means the optimization problem needs access to the original spatial embeddings.

            # Let's redefine the problem class to take original spatial coords
            # or assume node_features structure is [time, spatial_dims, ...] and extract spatial_dims

            # Option 1: Assume node_features = [time, spatial_dims_for_phi, ...]
            # and spatial_dims_for_phi are dims 1 to 1 + phi_spatial_dim
            # This requires knowing phi_spatial_dim here.
            # Let's assume phi_spatial_dim = 3 based on the previous code.
            # This is fragile. A better way is to pass the original `spatial` tensor.

            # Option 2 (Better): Modify `calculate_loss` to accept original spatial tensor.
            # calculate_loss(self, node_features, original_spatial, gnn_edge_index, gnn_targets)
            # ... and then use original_spatial for the causal_phi_loss call.

            # Let's stick to Option 1 for now to match the provided context's GNN training loop,
            # but acknowledge its limitations. Assume node_features structure is [time, spatial_for_phi, ...]
            # where spatial_for_phi are the first `self.phi_spatial_dim` spatial dimensions used for ds^2.
            # We need the `phi_spatial_dim` parameter in this class.

            # Add phi_spatial_dim to __init__
            assert hasattr(self, "phi_spatial_dim"), (
                "CausalOptimizationProblem needs phi_spatial_dim"
            )

            # Extract time (dim 0) and spatial (dims 1 to 1+phi_spatial_dim)
            # Ensure extracting correct number of spatial dimensions
            extracted_time = node_features[:, 0]  # (N,)
            extracted_spatial = node_features[
                :, 1 : 1 + self.phi_spatial_dim
            ]  # (N, phi_spatial_dim)

            # Check dimensions before calling causal_phi_loss
            if extracted_spatial.shape[1] != self.phi_spatial_dim:
                print(
                    f"Error: Extracted spatial dims ({extracted_spatial.shape[1]}) != expected phi_spatial_dim ({self.phi_spatial_dim})"
                )
                # Handle error or return 0 loss
                c_phi_loss = torch.tensor(
                    0.0, device=node_features.device, requires_grad=True
                )
            else:
                c_phi_loss = causal_phi_loss(
                    node_reps=extracted_spatial,
                    time_vec=extracted_time,
                    edge_index=self.edge_index_causal,
                    phi_model=self.phi_model,
                    eps1=1e-5,  # Assuming epsilon is fixed or passed
                )
            phi_loss_val = c_phi_loss.item()

        total_loss = task_loss + self.lambda_phi * c_phi_loss

        return total_loss, task_loss_val, phi_loss_val


# --- Integrate this optimization problem into the training loop ---

# Re-use components and data from previous cells
# Ensure the following variables are defined and on the correct device:
# - `GravityAwareGNN` class, `GravityField` class, `causal_phi_loss` function
# - `gnn_model`, `phi_model` (trained or initialized)
# - `node_features` (output of GNN input preparation: [adjusted_time, spatial, phi_values])
# - `edge_index_gnn` (GNN edges: parent->child)
# - `edge_index_causal_loss` (Causal pairs: child->parent)
# - `gnn_targets` (Targets for GNN prediction)
# - `spatial` (Original spatial embeddings used for ds^2 calculation - this is critical!)

# Let's redefine the WordNet example training loop using the CausalOptimizationProblem
print("\n--- Training GravityAwareGNN using CausalOptimizationProblem ---")

# Ensure required variables are defined (assuming previous cells were run)
gnn_model = GravityAwareGNN

assert "phi_model" in locals()
assert "node_features" in locals()
assert "edge_index_gnn" in locals()
assert "edge_index_causal_loss" in locals()
assert "gnn_targets" in locals()
assert "spatial" in locals()  # Original spatial embeddings!
assert "hierarchy_pairs" in locals()  # (child, parent) pairs for causal loss

# Re-initialize optimizer for clarity
gnn_optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.005)
lambda_phi_loss_weight = 0.01

# Instantiate the optimization problem
# We need `phi_spatial_dim` to be known for the problem class.
# From previous cells, it was `D_spatial` in the WordNet example.
wordnet_spatial_dim = spatial.shape[1]  # Or defined directly (e.g., 64)

causal_opt_problem = CausalOptimizationProblem(
    embedder=gnn_model,
    phi_model=phi_model,
    hierarchy_pairs=hierarchy_pairs,  # (child, parent)
    lambda_phi=lambda_phi_loss_weight,
)
# Manually add phi_spatial_dim to the problem instance for Option 1 extraction
# A cleaner class would take this in __init__
causal_opt_problem.phi_spatial_dim = wordnet_spatial_dim
# Check the dimension of node_features to ensure extraction works
if node_features.shape[1] < 1 + causal_opt_problem.phi_spatial_dim:
    raise ValueError(
        f"node_features shape ({node_features.shape}) is too small to extract time (1) and spatial ({causal_opt_problem.phi_spatial_dim})."
    )


num_gnn_epochs = 200  # Adjust as needed
for epoch in range(num_gnn_epochs):
    gnn_model.train()
    phi_model.train()  # Phi model can also be trained here if its parameters require gradients

    gnn_optimizer.zero_grad()

    # Use the calculate_loss method of the optimization problem
    # Need to pass the original spatial data for the causal_phi_loss internally
    # As per Option 1, we assume node_features contains the spatial part.
    total_loss, task_loss_val, phi_loss_val = causal_opt_problem.calculate_loss(
        node_features=node_features,  # Assuming [time, spatial_for_phi, ...]
        gnn_edge_index=edge_index_gnn,
        gnn_targets=gnn_targets,
    )

    total_loss.backward()
    gnn_optimizer.step()  # Updates parameters of gnn_model and potentially phi_model

    if epoch % 20 == 0 or epoch == num_gnn_epochs - 1:
        print(
            f"Opt Epoch {epoch + 1}/{num_gnn_epochs}, Total Loss: {total_loss.item():.4f}, Task Loss: {task_loss_val:.4f}, Causal Phi Loss: {phi_loss_val:.4f}"
        )

print("GNN training with CausalOptimizationProblem complete.")

# Evaluation (simple MSE on training data)
gnn_model.eval()
phi_model.eval()  # Set phi model to eval mode
with torch.no_grad():
    final_predictions = gnn_model(node_features, edge_index_gnn)
    final_mse = F.mse_loss(final_predictions, gnn_targets)
    print(
        f"\nFinal GNN Training MSE on predicting original depth: {final_mse.item():.4f}"
    )

    # Compare some predictions to targets
    for i in range(min(5, N)):
        print(
            f"Synset: {id_to_synset[i].name()}, Original Depth: {gnn_targets[i].item():.2f}, Predicted Depth: {final_predictions[i].item():.2f}"
        )
        # Need adjusted_time_vec and phi_values if we want to print them
        # They are part of node_features but not easily accessible here without unpacking
        # print(f"  Adjusted Time: {node_features[i,0].item():.2f}, Phi: {node_features[i, 1+wordnet_spatial_dim].item():.2f}")
