# -*- coding: utf-8 -*-
"""Curved Concept Spacetime

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_zk_wT9VkyFU0WqOnTGgFO5TUhBNEINk

AI is a treasure map.

### The Treasure we found.

1.  **The "Magic Map"**
    * Our thoughts and ideas (like "dog" and "mammal") aren't just a flat list. They actually live on a special, squishy, bendy "magic map"
    * On this map, how ideas are connected (like "dog is a type of mammal") is shown by special lines that are almost like light beams. This is called "causality" â€“ meaning one idea leads to another.

2.  **We Can Put Ideas on a Flat Map (First Step Working):**
    * First, we built a way to put our ideas on a normal, flat map (like a simple drawing on the floor) using a special math trick. We made sure the "light beam" connections were straight on this flat map. This flat map uses something called "Minkowski spacetime".

    * We even found that our flat map of ideas looks a lot like how smart computer brains (like BERT) store their ideas! This is a really big deal because it means our geometric idea might be a hidden secret to how these big AIs work.

3.  **We Got Smarter Magic Glasses (The INN for $\phi$):**
    * To make our ideas live on a *truly* squishy, bendy map, we needed "magic glasses" ($\phi$) that could bend the flat map. We decided to make these glasses super smart by training a special computer brain (an "Invertible Neural Network" or INN) to be our $\phi$

### What We Can Do With Our Current Progress (Playing on the Map!)

1.  **Explain AI Better:** Since our idea map (even the flat one) looks like what smart AIs do, we can use our map to understand *why* AIs think certain things or connect ideas in certain ways. It's like having a window into their brain!
2.  **Make AI Smarter:** We can use our map to teach AIs how to be more factual and avoid making up nonsense, by guiding their ideas to stay on the "good, valid paths" of our magic map.
3.  **See Our Ideas Clearly:** We can actually draw and look at our ideas on this 3D map. We can see which ideas are close, which are far, and how they flow into each other, even if the map itself is now bendy.
4.  **A Stepping Stone for True "Meaning-Gravity":** The fact that we can create a squishy, bendy map that preserves its bigness means we're much closer to seeing if some ideas are so "heavy" they actually *bend the map around them*, just like gravity bends space around planets!
"""

import math
import numpy as np
import torch
import nltk
from nltk.corpus import wordnet as wn
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from transformers import BertTokenizer, BertModel
from sklearn.decomposition import PCA
from collections import defaultdict
from typing import List, Tuple

# --- Ensure NLTK WordNet is available ---
try:
    wn.ensure_loaded()
except LookupError:
    nltk.download('wordnet')
    nltk.download('omw-1.4') # For broader compatibility
    wn.ensure_loaded()

"""import bert basically"""

# === CONFIGURATION ==================================================
BERT_MODEL_NAME = 'bert-base-uncased'
NUM_DIMENSIONS = 2
print("Loading BERT model and tokenizer (if not already loaded)...")
tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)
bert_model = BertModel.from_pretrained(BERT_MODEL_NAME)
bert_model.eval()
print("BERT loaded.")
# === Helper functions ===============================================
def get_bert_embedding(text, model, tokenizer):
  inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
  with torch.no_grad():
    outputs = model(**inputs)
  return outputs.last_hidden_state[:, 0, :].squeeze().numpy()
# --- WordNet Mammal Subtree Function ---
def get_mammal_concepts(depth_limit=3, start_synset_name='mammal.n.01'):
    concepts = {}
    try:
        mammal_root_synset = wn.synset(start_synset_name)
    except Exception as e:
        print(f"Error accessing WordNet synset '{start_synset_name}': {e}")
        print("Make sure NLTK WordNet data is downloaded and the synset name is correct.")
        return {}

    queue = [(mammal_root_synset, 0, None)]
    visited_synsets = set()
    processed_names = set() # To handle cases where different synsets might yield same primary lemma

    while queue:
        current_synset, depth, parent_name = queue.pop(0)

        if current_synset in visited_synsets or depth > depth_limit:
            continue
        visited_synsets.add(current_synset)

        # Try to get a clean name, prefer single words if possible
        name = None
        lemmas = current_synset.lemmas()
        if lemmas:
            name = lemmas[0].name().replace('_', ' ')
            # Prefer shorter names or names without hyphens if multiple lemmas give same concept
            for lem in lemmas[1:]:
                temp_name = lem.name().replace('_', ' ')
                if len(temp_name) < len(name) and '-' not in temp_name:
                    name = temp_name

        if name is None or name in processed_names: # Skip if no good name or name already processed from another synset
            continue
        processed_names.add(name)

        concepts[name] = {'synset': current_synset, 'depth': depth, 'parents': set(), 'children': set()}

        if parent_name and parent_name in concepts: # Ensure parent was successfully added
            concepts[name]['parents'].add(parent_name)
            concepts[parent_name]['children'].add(name)

        for hyponym in current_synset.hyponyms():
            queue.append((hyponym, depth + 1, name))
    return concepts

print("=== Minkowski Embedding Demo with WordNet Mammals and BERT ===")
# === 1. Load Mammal Concepts from WordNet =====================================================
with get_mammal_concepts(depth_limit=NUM_DIMENSIONS, start_synset_name='mammal.n.0!') as carnivore_data_wn:
    print("\nLoading mammal concepts from WordNet...")
    if carnivore_data_wn:
        print(f"Loaded {len(carnivore_data_wn)} carnivore")
# ================================================================================================

# 2. === Generate Actual BERT Embeddings =========================================================
concept_bert_embeddings_wn = {}
if mammal_data_wn:
    print("\nGenerating BERT embeddings for WordNet concepts...")
    for name in mammal_data_wn.keys():
        try:
            concept_bert_embeddings_wn[name] = get_bert_embedding(name, bert_model, tokenizer)
        except Exception as e:
            print(f"Error getting BERT embedding for '{name}': {e}")
    print(f"Generated {len(concept_bert_embeddings_wn)} BERT embeddings.")
    if len(concept_bert_embeddings_wn) < len(mammal_data_wn):
        print("Warning: Not all WordNet concepts received a BERT embedding.")
        # Filter mammal_data_wn to only include concepts for which we have embeddings
        mammal_data_wn = {name: data for name, data in mammal_data_wn.items() if name in concept_bert_embeddings_wn}
        print(f"Proceeding with {len(mammal_data_wn)} concepts that have embeddings.")


# 3. === Define Hierarchy from WordNet Data ========================================================
hierarchy_wn = []
root_wn_name = None
min_depth_wn = float('inf')

if mammal_data_wn:
    for name, data in mammal_data_wn.items():
        if not data['parents']: # Potential root
            if data['depth'] <= min_depth_wn: # Find the highest root
                min_depth_wn = data['depth']
                root_wn_name = name
        for parent_name in data['parents']:
            if parent_name in mammal_data_wn: # Ensure parent is also in our selected concept set
                hierarchy_wn.append((name, parent_name))

    if root_wn_name:
        hierarchy_wn.append((root_wn_name, None))
        print(f"Identified WordNet hierarchy root: {root_wn_name}")
    elif mammal_data_wn: # Fallback if no explicit root was easily found (e.g. if start_synset had parents outside depth_limit)
        # Find any node with no parents WITHIN the current mammal_data_wn set
        all_children_in_set = {child for child,parent in hierarchy_wn}
        potential_roots = [name for name in mammal_data_wn.keys() if name not in all_children_in_set]
        if potential_roots:
            root_wn_name = min(potential_roots, key=lambda r: mammal_data_wn[r]['depth']) # Pick highest one
            hierarchy_wn.append((root_wn_name, None))
            print(f"Fallback WordNet hierarchy root: {root_wn_name}")
        else:
            print("Warning: Could not definitively identify a single root for the WordNet hierarchy subset.")

    # Ensure hierarchy_wn contains only concepts present in concept_bert_embeddings_wn
    valid_concepts_for_hierarchy = set(concept_bert_embeddings_wn.keys())
    hierarchy_wn_filtered = []
    for child, parent in hierarchy_wn:
        if child in valid_concepts_for_hierarchy and (parent is None or parent in valid_concepts_for_hierarchy):
            hierarchy_wn_filtered.append((child,parent))
    hierarchy_wn = sorted(list(set(hierarchy_wn_filtered))) # Remove duplicates and sort

    print(f"Constructed hierarchy with {len(hierarchy_wn)} pairs for embedding.")


# === 4. Set Parameters ====================================================================================================
spatial_dimensions_wn = 2 # (t, x, y)
epsilon_wn = 1e-5 # make this smaller for more challenging strict inequality.

# 5. === Run the LLM's embedding function ==================================================================================
if concept_bert_embeddings_wn and hierarchy_wn:
    print("\nComputing Minkowski Embedding for WordNet data...")
    minkowski_coordinates_wn = compute_minkowski_embedding(
        concept_bert_embeddings_wn,
        hierarchy_wn,
        spatial_dimensions_wn,
        epsilon_wn
    )

    print("\n--- Computed Minkowski Coordinates for WordNet concepts (sample) ---")
    if minkowski_coordinates_wn:
        for i, (name, coords) in enumerate(minkowski_coordinates_wn.items()):
            if i < 10: # Print first 10
                 print(f"{name}: [{coords[0]:.3f}, " + ", ".join([f"{c:.3f}" for c in coords[1:]]) + "]")
            elif i == 10:
                 print("...")
    else:
        print("No WordNet coordinates were computed.")

    # 6. === Basic Causal Consistency Check ==============================================================================
    print("\n--- Causal Consistency Check for WordNet Hierarchy ---")
    all_constraints_met_wn = True
    violations_count = 0
    checked_constraints = 0
    if minkowski_coordinates_wn:
        for child_name, parent_name in hierarchy_wn:
            if parent_name is None:
                continue
            checked_constraints +=1
            if child_name in minkowski_coordinates_wn and parent_name in minkowski_coordinates_wn:
                child_c = minkowski_coordinates_wn[child_name]
                parent_c = minkowski_coordinates_wn[parent_name]
                is_consistent = check_causal_link_eval(child_c, parent_c, epsilon_wn, spatial_dimensions_wn)
                if not is_consistent:
                    print(f"VIOLATION: {parent_name} -> {child_name}")
                    all_constraints_met_wn = False
                    violations_count +=1
            else:
                print(f"Warning: {child_name} or {parent_name} not in output for WordNet check.")
                all_constraints_met_wn = False
                violations_count +=1

        if violations_count > 0:
             print(f"{violations_count}/{checked_constraints} causal constraints VIOLATED for WordNet hierarchy.")
        elif checked_constraints > 0:
             print(f"All {checked_constraints} checked causal constraints met for WordNet hierarchy!")
        else:
             print("No constraints to check for WordNet hierarchy (hierarchy empty or nodes missing).")

    # 7. === Visualization =========================================================================================================
    if minkowski_coordinates_wn and (spatial_dimensions_wn == 2 or spatial_dimensions_wn == 1) and len(minkowski_coordinates_wn) > 1:
        print("\nVisualizing WordNet Minkowski Embedding...")
        fig_wn = plt.figure(figsize=(12, 10))

        # Determine plot type based on spatial_dims
        if spatial_dimensions_wn == 2:
            ax_wn = fig_wn.add_subplot(111, projection='3d')
            ax_wn.set_xlabel("Spatial X")
            ax_wn.set_ylabel("Spatial Y")
            ax_wn.set_zlabel("Time (t)")
            coords_to_plot = [(coords[1], coords[2], coords[0]) for coords in minkowski_coordinates_wn.values()] # x,y,t
            link_coords_indices = (1, 2, 0)
        elif spatial_dimensions_wn == 1:
            ax_wn = fig_wn.add_subplot(111)
            ax_wn.set_xlabel("Spatial X")
            ax_wn.set_ylabel("Time (t)")
            coords_to_plot = [(coords[1], coords[0]) for coords in minkowski_coordinates_wn.values()] # x,t
            link_coords_indices = (1, 0)
        else: # Should not happen with current spatial_dimensions_wn setting
            print("Unsupported spatial dimensions for this plotting snippet.")
            coords_to_plot = []

        if coords_to_plot:
            # Scatter plot points
            unique_names = list(minkowski_coordinates_wn.keys())
            for i, name in enumerate(unique_names):
                coords = minkowski_coordinates_wn[name]
                if spatial_dimensions_wn == 2:
                    ax_wn.scatter(coords[link_coords_indices[0]], coords[link_coords_indices[1]], coords[link_coords_indices[2]], s=30)
                    ax_wn.text(coords[link_coords_indices[0]], coords[link_coords_indices[1]], coords[link_coords_indices[2]], f" {name}", fontsize=7)
                elif spatial_dimensions_wn == 1:
                    ax_wn.scatter(coords[link_coords_indices[0]], coords[link_coords_indices[1]], s=30)
                    ax_wn.text(coords[link_coords_indices[0]], coords[link_coords_indices[1]], f" {name}", fontsize=7)

            # Plot hierarchy links
            for child_name, parent_name in hierarchy_wn:
                if parent_name is None or child_name not in minkowski_coordinates_wn or parent_name not in minkowski_coordinates_wn:
                    continue
                p1_coords = minkowski_coordinates_wn[parent_name]
                p2_coords = minkowski_coordinates_wn[child_name]

                plot_args = []
                for idx in link_coords_indices: # x, y, t or x, t
                    plot_args.extend([[p1_coords[idx], p2_coords[idx]]])
                ax_wn.plot(*plot_args, c='darkgrey', alpha=0.5, linewidth=0.8)

            plt.title(f"Minkowski Embedding of WordNet Mammal Subtree (spatial_dims={spatial_dimensions_wn})")
            plt.tight_layout()
            plt.show()

elif not mammal_data_wn:
    print("Demo cannot run as WordNet data was not loaded.")
elif not concept_bert_embeddings_wn:
    print("Demo cannot run as BERT embeddings were not generated for WordNet concepts.")
elif not hierarchy_wn:
    print("Demo cannot run as no hierarchy pairs were constructed for WordNet data.")

"""minkowski"""

def compute_minkowski_embedding(concept_bert_embeddings: dict[str, list[float]], hierarchy_pairs: list[tuple[str, str | None]], spatial_dims: int, epsilon_causal_margin: float) -> dict[str, list[float]]:
    """
    Computes Minkowski spacetime coordinates for concepts based on BERT embeddings
    and a hierarchy, ensuring causal consistency.
    """
    concept_names = list(concept_bert_embeddings.keys())
    if not concept_names:
        return {}

    bert_matrix = np.array([concept_bert_embeddings[name] for name in concept_names])
    num_concepts = bert_matrix.shape[0]

    if bert_matrix.size == 0: # No data in bert_matrix
        print("Warning: BERT matrix is empty. Returning empty coords.")
        return {name: [0.0] * (1 + spatial_dims) for name in concept_names} if concept_names else {}

    bert_dim = bert_matrix.shape[1]
    coords = {}

    # 1. Initial Spatial Embedding (using PCA) ========
    if spatial_dims > 0:
        # PCA requires n_samples >= n_features if n_features > n_components
        # and n_samples > n_components
        n_components_pca = min(spatial_dims, bert_dim, num_concepts -1 if num_concepts > 1 else 1) # Ensure n_components < n_samples
        spatial_coords_matrix = np.zeros((num_concepts, spatial_dims))
        if num_concepts > 1 and n_components_pca > 0 and bert_dim > 0 :
             try:
                 pca = PCA(n_components=n_components_pca)
                 spatial_coords_pca = pca.fit_transform(bert_matrix)
                 spatial_coords_matrix[:, :n_components_pca] = spatial_coords_pca
             except ValueError as e:
                 print(f"Warning: PCA failed: {e}. Using zeros for spatial coordinates.")
        elif num_concepts == 1 and n_components_pca > 0 and bert_dim > 0:
            # PCA not meaningful for a single point, place at origin or use part of its BERT
            spatial_coords_matrix[:, :min(spatial_dims, bert_dim)] = bert_matrix[0, :min(spatial_dims, bert_dim)]
        for i, name in enumerate(concept_names):
            coords[name] = [0.0] + spatial_coords_matrix[i].tolist()
    else:
        for name in concept_names:
            coords[name] = [0.0]
    # 2. Build Parent-Child Graph and Identify Roots =======
    parent_to_children = defaultdict(list)
    child_to_parents = defaultdict(list)
    explicit_root_children = set()

    for child_name, parent_name in hierarchy_pairs:
        if parent_name is not None:
            if parent_name in concept_names and child_name in concept_names:
                 parent_to_children[parent_name].append(child_name)
                 child_to_parents[child_name].append(parent_name)
        else:
            if child_name in concept_names:
                explicit_root_children.add(child_name)

    if not explicit_root_children:
         all_children_in_set = {c for c, p in hierarchy_pairs if p is not None and p in concept_names and c in concept_names}
         root_nodes = [name for name in concept_names if name not in all_children_in_set]
    else:
         root_nodes = list(explicit_root_children)

    if not root_nodes and concept_names:
        print("Warning: No explicit or implicit roots found. Using first concept as a pseudo-root for time normalization.")
        root_nodes = [concept_names[0]]

    # 3. Iterative Time Adjustment (Relaxation) ========
    max_iterations = 1000
    learning_rate_adjustment = 0.1 # Added for potentially smoother adjustments
    # This is the tiny_buffer to satisfy strict inequality discussed
    strict_inequality_buffer = epsilon_causal_margin * 0.01 + 1e-9

    for iteration in range(max_iterations):
        changed_in_iteration = False
        for child_name, parent_name in hierarchy_pairs:
             if parent_name is None:
                 continue
             if parent_name not in coords or child_name not in coords:
                 continue

             p_coords = coords[parent_name]
             c_coords = coords[child_name]

             if spatial_dims > 0:
                 spatial_dist_sq = np.sum((np.array(c_coords[1:]) - np.array(p_coords[1:]))**2)
             else:
                 spatial_dist_sq = 0

             # Ensure argument to sqrt is non-negative
             sqrt_arg = spatial_dist_sq + epsilon_causal_margin
             if sqrt_arg < 0: sqrt_arg = 0 # Should not happen with positive epsilon

             required_dt = np.sqrt(sqrt_arg)
             # Apply the strict inequality buffer here:
             required_t_c = p_coords[0] + required_dt + strict_inequality_buffer

             if c_coords[0] < required_t_c:
                 # Apply with a learning rate to prevent wild oscillations if hierarchy is complex
                 diff = required_t_c - c_coords[0]
                 coords[child_name][0] += diff * learning_rate_adjustment
                 changed_in_iteration = True
        if not changed_in_iteration and iteration > 0:
            print(f"Converged at iteration {iteration}")
            break

    # 4. Shift times so the minimum root time is 0  =======
    if root_nodes and all(name in coords for name in root_nodes):
        min_t = min(coords[name][0] for name in root_nodes)
    elif coords:
        min_t = min(c[0] for c in coords.values())
    else:
        min_t = 0.0

    for name in coords:
        coords[name][0] -= min_t
        # Ensure no negative times due to floating point issues after shift
        if coords[name][0] < 0 and abs(coords[name][0]) < 1e-9: # if very close to 0
            coords[name][0] = 0.0

    return coords
# --- Helper Functions for Evaluation ---
def spatial_distance_sq_eval(coord1_spatial: list[float], coord2_spatial: list[float]) -> float:
    return np.sum((np.array(coord1_spatial) - np.array(coord2_spatial))**2)

def check_causal_link_eval(child_coord_minkowski: list[float],
                           parent_coord_minkowski: list[float],
                           epsilon_causal_margin: float,
                           spatial_dims: int) -> bool:
    """
      Checks if child time is strictly greater than parent time + spatial distance.
    """
    if len(child_coord_minkowski) != 1 + spatial_dims or len(parent_coord_minkowski) != 1 + spatial_dims:
        return False # Invalid input format

    dt = child_coord_minkowski[0] - parent_coord_minkowski[0]
    # For strict inequality in causal link, dt MUST be > 0.
    if dt <= 1e-9: # dt must be positive by more than just float noise
        return False
    d_spatial_sq_val = spatial_distance_sq_eval(child_coord_minkowski[1:], parent_coord_minkowski[1:])
    # Check for strict inequality: dt^2 > d_spatial_sq + epsilon
    return dt**2 > d_spatial_sq_val + epsilon_causal_margin

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA # For LLM function
from collections import defaultdict # For LLM function
import math # For LLM function

# --- 1. Define a Generic Metric Function -- -
# In General Relativity, the metric tensor g_munu defines the geometry of spacetime.
# For 3D space + 1D time (4D spacetime), g_munu is a 4x4 symmetric matrix.
# The proper interval squared (ds^2) is given by ds^2 = g_munu dx^mu dx^nu.
# Here, we'll define a function that returns the metric tensor at a given spacetime coordinate.

def flat_minkowski_metric(coords):
    """
    Returns the flat Minkowski metric tensor at any given coordinates (t, x, y, z).
    Signature: g_00 = -1, g_11 = 1, g_22 = 1, g_33 = 1, others 0.
    """
    g = np.diag([-1.0, 1.0, 1.0, 1.0])
    return g

def toy_curved_metric(coords):
    """
    A simple toy metric that introduces some 'curvature'.
    For illustration, let's make the 'time' component dependent on spatial position,
    and perhaps 'compress' space near the origin.
    This is NOT a physically derived metric from GR, but for conceptual demonstration.
    """
    t, x, y, z = coords
    g = np.diag([-1.0, 1.0, 1.0, 1.0]) # Start with Minkowski

    # Introduce a 'gravitational potential' like effect on time component
    # (stronger "gravity" near the origin, slowing down time)
    r_sq = x**2 + y**2 + z**2
    # Avoid division by zero or very small numbers near r=0, add a small constant
    g[0, 0] = -(1.0 - 0.1 / (r_sq + 0.1)) # g_tt becomes more negative (time stretches)

    # Introduce spatial 'compression' or 'expansion'
    # For example, make spatial distances scale with distance from origin
    spatial_scaling = 1.0 + 0.05 * r_sq # Space 'expands' further out
    g[1, 1] *= spatial_scaling
    g[2, 2] *= spatial_scaling
    g[3, 3] *= spatial_scaling

    return g

# --- 2. Generalize the Event Class to use a Metric ---
class Event:
    def __init__(self, token_id, metric_func, initial_pos=None):
        self.token_id = token_id
        self.metric_func = metric_func # Function that returns g_munu(coords)

        if initial_pos is None:
            # Initialize spatial coordinates uniformly in (-1, 1) and time to 0
            # (t, x, y, z)
            self.coords = np.array([0.0, np.random.uniform(-1, 1), np.random.uniform(-1, 1), np.random.uniform(-1, 1)])
        else:
            self.coords = np.array(initial_pos, dtype=float)

    @property
    def t(self):
        return self.coords[0]

    @t.setter
    def t(self, value):
        self.coords[0] = value

    @property
    def spatial(self):
        return self.coords[1:]

    def proper_interval_sq(self, other_event):
        """
        Calculates the squared proper interval (ds^2) between this event and another.
        In curved spacetime, this is path-dependent. For a simple point-to-point
        calculation, we'll use the metric at the 'midpoint' or average, which is a
        simplification but illustrates the concept.
        For true geodesics, you'd need to integrate along the path.
        """
        # Vector connecting the two events in coordinate space (dx^mu)
        delta_coords = self.coords - other_event.coords

        # Get the metric tensor at a representative point (e.g., midpoint)
        # This is a simplification; for accuracy, one would integrate along a geodesic.
        midpoint_coords = (self.coords + other_event.coords) / 2.0
        g_munu = self.metric_func(midpoint_coords)

        # Calculate ds^2 = g_munu dx^mu dx^nu
        # Using Einstein summation convention (np.einsum)
        ds_sq = np.einsum('i,ij,j', delta_coords, g_munu, delta_coords)
        return ds_sq

# --- 3. Adaptation of the Embedding Algorithm (Conceptual) ---

def embed_hierarchy_curved(tokens, hierarchy_pairs, metric_func, epsilon1=1e-5, epsilon2=0, max_iterations=1000):
    """
    Adapts the embedding algorithm for a curved spacetime.
    NOTE: The time adjustment rule (delta = T_min - delta_t) is a simplification.
    In true curved spacetime, adjusting coordinates to achieve 'almost null'
    geodesics requires solving differential equations (geodesic equations)
    and is significantly more complex. This function illustrates how the
    proper_interval_sq would be used.
    """
    event_map = {token_id: Event(token_id, metric_func) for token_id in tokens}

    print(f"Starting embedding in curved spacetime with metric: {metric_func.__name__}")

    for iteration in range(max_iterations):
        violations_found = False
        current_violations = []

        for child_id, parent_id in hierarchy_pairs:
            child_event = event_map[child_id]
            parent_event = event_map[parent_id]

            ds_sq = child_event.proper_interval_sq(parent_event)
            target_ds_sq = -epsilon1**2

            if child_event.t <= parent_event.t:
                violations_found = True
                delta = (parent_event.t - child_event.t) + 0.1
                child_event.t += delta * (1 - epsilon2)
                parent_event.t -= delta * epsilon2
                continue

            if ds_sq > 0 or ds_sq < target_ds_sq:
                violations_found = True
                if ds_sq > 0:
                    delta_t_needed_sq = -ds_sq + target_ds_sq
                    if delta_t_needed_sq > 0:
                        delta_t_adjust = np.sqrt(delta_t_needed_sq)
                        child_event.t += delta_t_adjust * (1 - epsilon2)
                        parent_event.t -= delta_t_adjust * epsilon2
                elif ds_sq < target_ds_sq:
                    delta_t_needed_sq = ds_sq - target_ds_sq
                    if delta_t_needed_sq > 0:
                        delta_t_adjust = np.sqrt(delta_t_needed_sq)
                        child_event.t -= delta_t_adjust * (1 - epsilon2)
                        parent_event.t += delta_t_adjust * epsilon2

        if not violations_found:
            print(f"Convergence achieved after {iteration} iterations.")
            break

        if iteration == max_iterations - 1:
            print(f"Max iterations ({max_iterations}) reached. Not fully converged.")

    return event_map

# --- Example Usage ---

# 1. Define some dummy hierarchical data
tokens = ['animal', 'mammal', 'dog', 'cat', 'beagle', 'persian']
hierarchy_pairs = [
    ('mammal', 'animal'),
    ('dog', 'mammal'),
    ('cat', 'mammal'),
    ('beagle', 'dog'),
    ('persian', 'cat')
]

# 2. Run the embedding with the flat Minkowski metric
print("--- Embedding with Flat Minkowski Metric ---")
flat_events = embed_hierarchy_curved(tokens, hierarchy_pairs, flat_minkowski_metric, max_iterations=200)

# 3. Run the embedding with the toy curved metric
print("\n--- Embedding with Toy Curved Metric ---")
curved_events = embed_hierarchy_curved(tokens, hierarchy_pairs, toy_curved_metric, max_iterations=200)

# --- 4. Visualization (Conceptual) ---
def plot_embedding(event_map, title="Spacetime Embedding"):
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    points = np.array([event.coords for event in event_map.values()])
    labels = [event.token_id for event in event_map.values()]

    scatter = ax.scatter(points[:, 1], points[:, 2], points[:, 0], c=points[:, 3], cmap='viridis', s=100)
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Time (t)')
    fig.colorbar(scatter, label='Z-coordinate')

    for i, txt in enumerate(labels):
        ax.text(points[i, 1], points[i, 2], points[i, 0], f' {txt}', size=8, zorder=1)

    for child_id, parent_id in hierarchy_pairs:
        child_coords = event_map[child_id].coords
        parent_coords = event_map[parent_id].coords
        ax.plot(
            [parent_coords[1], child_coords[1]],
            [parent_coords[2], child_coords[2]],
            [parent_coords[0], child_coords[0]],
            color='red', linestyle='--', alpha=0.5
        )

    ax.set_title(title)
    plt.show()

plot_embedding(flat_events, "Flat Minkowski Embedding of Hierarchy (Z as color)")
plot_embedding(curved_events, "Toy Curved Spacetime Embedding of Hierarchy (Z as color)")

print("\n--- Understanding the Current 'Curved' Embedding and Path to True GR ---")
print("The `toy_curved_metric` and its use in `embed_hierarchy_curved` provide a conceptual step towards curved spacetime.")
print("However, it's important to understand the simplifications made and what a rigorous General Relativistic (GR) approach entails:")
print("\n1. Metric Tensor (g_munu):")
print("   - Current: `toy_curved_metric` is a heuristic, not derived from physical principles. It makes g_00 (time component) and spatial components (g_ii) vary with position.")
print("   - GR Concept: In GR, g_munu is derived from Einstein's Field Equations (G_munu = 8piG T_munu), which relate spacetime geometry to the distribution of mass-energy. Specific solutions like Schwarzschild (for a static, spherically symmetric mass) or Kerr (for a rotating mass) provide precise, complex forms for g_munu.")
print("   - Conceptual Landscape: For a 'conceptual spacetime', the 'mass-energy' could be analogous to information density, concept centrality, or some other measure of conceptual 'gravity'.")

print("\n2. Proper Interval (ds^2 = g_munu dx^mu dx^nu):")
print("   - Current: `proper_interval_sq` calculates ds^2 between two events using dx^mu (coordinate differences) and g_munu evaluated at the *midpoint*. This is an approximation.")
print("   - GR Concept: The true proper interval (or proper time for timelike paths) between two events is found by integrating the metric along the *geodesic path* connecting them: tau = integral sqrt(-ds^2). For small separations, the midpoint approximation can be reasonable, but for larger separations, it deviates.")

print("\n3. Geodesics (Paths of 'Free' Motion/Information Flow):")
print("   - Current: The `embed_hierarchy_curved` function adjusts time coordinates heuristically to satisfy ds^2 approx -epsilon1^2 and child.t > parent.t. It does *not* find or enforce geodesic paths.")
print("   - GR Concept: Particles and light follow geodesics, which are the 'straightest possible lines' in curved spacetime. They are solutions to the geodesic equation:")
print("     d^2x^mu / d(tau)^2 + Gamma^mu_alpha_beta * (dx^alpha/d(tau)) * (dx^beta/d(tau)) = 0")
print("     where tau is the proper time (or an affine parameter for null geodesics) and Gamma^mu_alpha_beta are the Christoffel symbols (connection coefficients).")
print("   - Christoffel Symbols: These are derived from the first derivatives of the metric tensor: Gamma^mu_alpha_beta = (1/2) * g^mu_sigma * (dg_sigma_alpha/dx^beta + dg_sigma_beta/dx^alpha - dg_alpha_beta/dx^sigma).")
print("   - Conceptual Landscape: Hierarchical links (parent-child) should ideally correspond to 'almost null' or short timelike geodesics, representing the most direct path for conceptual inheritance or information flow.")

print("\n4. Coordinate Adjustment Logic:")
print("   - Current: The time adjustment is a simple shift based on the deviation of ds^2 from a target. Spatial coordinates are not adjusted in this iterative step (though initialized by PCA in the LLM example for flat space, and randomly here).")
print("   - GR Concept for Embedding: A more rigorous approach would involve an optimization problem. The goal would be to find event coordinates (t, x, y, z) for all concepts suchthat:")
print("     a) For each parent-child pair, there exists a geodesic connecting them whose proper time squared is approximately -epsilon1^2 (almost null and timelike).")
print("     b) Causal order (child.t > parent.t along the geodesic path) is maintained.")
print("     This is a complex two-point boundary value problem for geodesics, embedded within an optimization loop.")

print("\n5. Tools for a GR-based Approach:")
print("   - Symbolic Calculation: Libraries like `sympy` are invaluable for deriving Christoffel symbols from a given metric tensor analytically.")
print("   - Numerical ODE Solvers: `scipy.integrate.solve_ivp` or `odeint` would be used to numerically solve the geodesic equations once the Christoffel symbols are known.")
print("   - GR Libraries: `einsteinpy` provides tools for defining metrics, calculating Christoffel symbols, and solving geodesic equations, which can significantly simplify these tasks.")
print("   - Optimization: `scipy.optimize` or more advanced libraries would be needed for the global optimization problem of placing events.")

print("\n6. Visualization in Curved Spacetime:")
print("   - Current: 3D scatter plot (x, y, t) with z as color. This is a coordinate representation.")
print("   - GR Concept: Visualizing true 4D curved spacetime is inherently challenging. Techniques include:")
print("     - Embedding diagrams: Slicing the 4D spacetime (e.g., at constant time or equatorial plane) and embedding the resulting 2D or 3D curved manifold into a higher-dimensional flat space (e.g., Flamm's paraboloid for Schwarzschild).")
print("     - Ray tracing: Simulating how light rays bend to visualize distortions.")
print("     - Visualizing the deformation of light cones or the paths of geodesics.")

print("\nIn summary, this notebook provides an excellent starting point for thinking about conceptual embeddings in non-flat spacetimes. The `toy_curved_metric` shows that the *idea* of a varying metric can be incorporated. Transitioning to a full GR model means replacing heuristic adjustments with the rigorous mathematics of geodesics and potentially Einstein's equations if the metric itself is to be learned or derived from 'conceptual matter'.")

#@title WordNet Embeddings with BERT!
import nltk
from nltk.corpus import wordnet as wn
import torch
from transformers import BertTokenizer, BertModel

try:
    wn.ensure_loaded()
except LookupError:
    nltk.download('wordnet')
    nltk.download('omw-1.4') # For broader compatibility
    wn.ensure_loaded()


def get_bert_embedding(text, model, tokenizer):
  inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
  with torch.no_grad():
    outputs = model(**inputs)
  return outputs.last_hidden_state[:, 0, :].squeeze().numpy()

# --- WordNet Mammal Subtree Function ---
def get_mammal_concepts(depth_limit=3, start_synset_name='mammal.n.01'):
    concepts = {}
    try:
        mammal_root_synset = wn.synset(start_synset_name)
    except Exception as e:
        print(f"Error accessing WordNet synset '{start_synset_name}': {e}")
        print("Make sure NLTK WordNet data is downloaded and the synset name is correct.")
        return {}

    queue = [(mammal_root_synset, 0, None)]
    visited_synsets = set()
    processed_names = set() # To handle cases where different synsets might yield same primary lemma

    while queue:
        current_synset, depth, parent_name = queue.pop(0)

        if current_synset in visited_synsets or depth > depth_limit:
            continue
        visited_synsets.add(current_synset)

        # Try to get a clean name, prefer single words if possible
        name = None
        lemmas = current_synset.lemmas()
        if lemmas:
            name = lemmas[0].name().replace('_', ' ')
            # Prefer shorter names or names without hyphens if multiple lemmas give same concept
            for lem in lemmas[1:]:
                temp_name = lem.name().replace('_', ' ')
                if len(temp_name) < len(name) and '-' not in temp_name:
                    name = temp_name

        if name is None or name in processed_names: # Skip if no good name or name already processed from another synset
            continue
        processed_names.add(name)

        concepts[name] = {'synset': current_synset, 'depth': depth, 'parents': set(), 'children': set()}

        if parent_name and parent_name in concepts: # Ensure parent was successfully added
            concepts[name]['parents'].add(parent_name)
            concepts[parent_name]['children'].add(name)

        for hyponym in current_synset.hyponyms():
            queue.append((hyponym, depth + 1, name))
    return concepts

print("=== Minkowski Embedding Demo with WordNet Mammals and BERT ===")

# === 1. Load Mammal Concepts from WordNet =====================================================
print("\nLoading mammal concepts from WordNet...")
# 'placental.n.01' or 'carnivore.n.01' can give interesting subtrees
mammal_data_wn = get_mammal_concepts(depth_limit=NUM_DIMENSIONS, start_synset_name='mammal.n.01')

if not mammal_data_wn:
    print("Failed to load WordNet concepts. Exiting demo.")
    # exit() # Use this if running as a script
else:
    print(f"Loaded {len(mammal_data_wn)} mammal-related concepts from WordNet.")
    # print("Sample concepts:", list(mammal_data_wn.keys())[:10])
# ================================================================================================

# 2. === Generate Actual BERT Embeddings =========================================================
concept_bert_embeddings_wn = {}
if mammal_data_wn:
    print("\nGenerating BERT embeddings for WordNet concepts...")
    for name in mammal_data_wn.keys():
        try:
            concept_bert_embeddings_wn[name] = get_bert_embedding(name, bert_model, tokenizer)
        except Exception as e:
            print(f"Error getting BERT embedding for '{name}': {e}")
    print(f"Generated {len(concept_bert_embeddings_wn)} BERT embeddings.")
    if len(concept_bert_embeddings_wn) < len(mammal_data_wn):
        print("Warning: Not all WordNet concepts received a BERT embedding.")
        # Filter mammal_data_wn to only include concepts for which we have embeddings
        mammal_data_wn = {name: data for name, data in mammal_data_wn.items() if name in concept_bert_embeddings_wn}
        print(f"Proceeding with {len(mammal_data_wn)} concepts that have embeddings.")


# 3. === Define Hierarchy from WordNet Data ========================================================
hierarchy_wn = []
root_wn_name = None
min_depth_wn = float('inf')

if mammal_data_wn:
    for name, data in mammal_data_wn.items():
        if not data['parents']: # Potential root
            if data['depth'] <= min_depth_wn: # Find the highest root
                min_depth_wn = data['depth']
                root_wn_name = name
        for parent_name in data['parents']:
            if parent_name in mammal_data_wn: # Ensure parent is also in our selected concept set
                hierarchy_wn.append((name, parent_name))

    if root_wn_name:
        hierarchy_wn.append((root_wn_name, None))
        print(f"Identified WordNet hierarchy root: {root_wn_name}")
    elif mammal_data_wn: # Fallback if no explicit root was easily found (e.g. if start_synset had parents outside depth_limit)
        # Find any node with no parents WITHIN the current mammal_data_wn set
        all_children_in_set = {child for child,parent in hierarchy_wn}
        potential_roots = [name for name in mammal_data_wn.keys() if name not in all_children_in_set]
        if potential_roots:
            root_wn_name = min(potential_roots, key=lambda r: mammal_data_wn[r]['depth']) # Pick highest one
            hierarchy_wn.append((root_wn_name, None))
            print(f"Fallback WordNet hierarchy root: {root_wn_name}")
        else:
            print("Warning: Could not definitively identify a single root for the WordNet hierarchy subset.")

    # Ensure hierarchy_wn contains only concepts present in concept_bert_embeddings_wn
    valid_concepts_for_hierarchy = set(concept_bert_embeddings_wn.keys())
    hierarchy_wn_filtered = []
    for child, parent in hierarchy_wn:
        if child in valid_concepts_for_hierarchy and (parent is None or parent in valid_concepts_for_hierarchy):
            hierarchy_wn_filtered.append((child,parent))
    hierarchy_wn = sorted(list(set(hierarchy_wn_filtered))) # Remove duplicates and sort

    print(f"Constructed hierarchy with {len(hierarchy_wn)} pairs for embedding.")


# === 4. Set Parameters ====================================================================================================
spatial_dimensions_wn = 2 # (t, x, y)
epsilon_wn = 1e-5 # make this smaller for more challenging strict inequality.

# 5. === Run the LLM's embedding function ==================================================================================
if concept_bert_embeddings_wn and hierarchy_wn:
    print("\nComputing Minkowski Embedding for WordNet data...")
    minkowski_coordinates_wn = compute_minkowski_embedding(
        concept_bert_embeddings_wn,
        hierarchy_wn,
        spatial_dimensions_wn,
        epsilon_wn
    )

    print("\n--- Computed Minkowski Coordinates for WordNet concepts (sample) ---")
    if minkowski_coordinates_wn:
        for i, (name, coords) in enumerate(minkowski_coordinates_wn.items()):
            if i < 10: # Print first 10
                 print(f"{name}: [{coords[0]:.3f}, " + ", ".join([f"{c:.3f}" for c in coords[1:]]) + "]")
            elif i == 10:
                 print("...")
    else:
        print("No WordNet coordinates were computed.")

    # 6. === Basic Causal Consistency Check ==============================================================================
    print("\n--- Causal Consistency Check for WordNet Hierarchy ---")
    all_constraints_met_wn = True
    violations_count = 0
    checked_constraints = 0
    if minkowski_coordinates_wn:
        for child_name, parent_name in hierarchy_wn:
            if parent_name is None:
                continue
            checked_constraints +=1
            if child_name in minkowski_coordinates_wn and parent_name in minkowski_coordinates_wn:
                child_c = minkowski_coordinates_wn[child_name]
                parent_c = minkowski_coordinates_wn[parent_name]
                is_consistent = check_causal_link_eval(child_c, parent_c, epsilon_wn, spatial_dimensions_wn)
                if not is_consistent:
                    print(f"VIOLATION: {parent_name} -> {child_name}")
                    all_constraints_met_wn = False
                    violations_count +=1
            else:
                print(f"Warning: {child_name} or {parent_name} not in output for WordNet check.")
                all_constraints_met_wn = False
                violations_count +=1

        if violations_count > 0:
             print(f"{violations_count}/{checked_constraints} causal constraints VIOLATED for WordNet hierarchy.")
        elif checked_constraints > 0:
             print(f"All {checked_constraints} checked causal constraints met for WordNet hierarchy!")
        else:
             print("No constraints to check for WordNet hierarchy (hierarchy empty or nodes missing).")

    # 7. === Visualization =========================================================================================================
    if minkowski_coordinates_wn and (spatial_dimensions_wn == 2 or spatial_dimensions_wn == 1) and len(minkowski_coordinates_wn) > 1:
        print("\nVisualizing WordNet Minkowski Embedding...")
        fig_wn = plt.figure(figsize=(12, 10))

        # Determine plot type based on spatial_dims
        if spatial_dimensions_wn == 2:
            ax_wn = fig_wn.add_subplot(111, projection='3d')
            ax_wn.set_xlabel("Spatial X")
            ax_wn.set_ylabel("Spatial Y")
            ax_wn.set_zlabel("Time (t)")
            coords_to_plot = [(coords[1], coords[2], coords[0]) for coords in minkowski_coordinates_wn.values()] # x,y,t
            link_coords_indices = (1, 2, 0)
        elif spatial_dimensions_wn == 1:
            ax_wn = fig_wn.add_subplot(111)
            ax_wn.set_xlabel("Spatial X")
            ax_wn.set_ylabel("Time (t)")
            coords_to_plot = [(coords[1], coords[0]) for coords in minkowski_coordinates_wn.values()] # x,t
            link_coords_indices = (1, 0)
        else: # Should not happen with current spatial_dimensions_wn setting
            print("Unsupported spatial dimensions for this plotting snippet.")
            coords_to_plot = []

        if coords_to_plot:
            # Scatter plot points
            unique_names = list(minkowski_coordinates_wn.keys())
            for i, name in enumerate(unique_names):
                coords = minkowski_coordinates_wn[name]
                if spatial_dimensions_wn == 2:
                    ax_wn.scatter(coords[link_coords_indices[0]], coords[link_coords_indices[1]], coords[link_coords_indices[2]], s=30)
                    ax_wn.text(coords[link_coords_indices[0]], coords[link_coords_indices[1]], coords[link_coords_indices[2]], f" {name}", fontsize=7)
                elif spatial_dimensions_wn == 1:
                    ax_wn.scatter(coords[link_coords_indices[0]], coords[link_coords_indices[1]], s=30)
                    ax_wn.text(coords[link_coords_indices[0]], coords[link_coords_indices[1]], f" {name}", fontsize=7)

            # Plot hierarchy links
            for child_name, parent_name in hierarchy_wn:
                if parent_name is None or child_name not in minkowski_coordinates_wn or parent_name not in minkowski_coordinates_wn:
                    continue
                p1_coords = minkowski_coordinates_wn[parent_name]
                p2_coords = minkowski_coordinates_wn[child_name]

                plot_args = []
                for idx in link_coords_indices: # x, y, t or x, t
                    plot_args.extend([[p1_coords[idx], p2_coords[idx]]])
                ax_wn.plot(*plot_args, c='darkgrey', alpha=0.5, linewidth=0.8)

            plt.title(f"Minkowski Embedding of WordNet Mammal Subtree (spatial_dims={spatial_dimensions_wn})")
            plt.tight_layout()
            plt.show()

elif not mammal_data_wn:
    print("Demo cannot run as WordNet data was not loaded.")
elif not concept_bert_embeddings_wn:
    print("Demo cannot run as BERT embeddings were not generated for WordNet concepts.")
elif not hierarchy_wn:
    print("Demo cannot run as no hierarchy pairs were constructed for WordNet data.")

import numpy as np
from scipy.integrate import solve_ivp
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Assume previous definitions (Event class, embed_hierarchy_curved, plot_embedding,
# flat_minkowski_metric, toy_curved_metric) are in a preceding cell.

print("--- Phase 2 (Simplified): Geodesics in Schwarzschild Spacetime (Manual ODEs) ---")

# --- 1. Define Schwarzschild Metric Components (Spherical Coordinates) ---
# Coordinates: (t, r, theta, phi)
# M is the mass of the central object. Using units where G=c=1.

def schwarzschild_metric_components_spherical(r, th, M_val):
    """
    Returns the components of the Schwarzschild metric tensor g_munu
    at a given r, theta for mass M.
    g_tt = -(1 - 2M/r)
    g_rr = 1 / (1 - 2M/r)
    g_thth = r^2
    g_phph = r^2 * sin(th)^2
    """
    if r <= 2 * M_val: # Inside or at event horizon - singularity
        # Handle singularity or return values that indicate it
        # For simplicity in ODE, we might return extreme values or rely on solver to stop
        return -np.inf, np.inf, np.inf, np.inf # Or some other indicator

    factor = (1 - 2 * M_val / r)
    g_tt = -factor
    g_rr = 1 / factor
    g_thth = r**2
    g_phph = r**2 * np.sin(th)**2
    return g_tt, g_rr, g_thth, g_phph

# --- 2. Geodesic Equations for Schwarzschild (Manual ODE System) ---
# These are derived from the geodesic equation using the known non-zero Christoffel symbols
# for Schwarzschild metric.
# y = [t, r, theta, phi, dt/dlambda, dr/dlambda, dtheta/dlambda, dphi/dlambda]
# lambda is the affine parameter.

def schwarzschild_geodesic_odes(lambda_param, y, M_val):
    t, r, th, ph, vt, vr, vth, vph = y # position and "velocities"

    # Avoid division by zero if r is too small (near singularity)
    if r < 1e-6 or r <= 2*M_val: # Added check for event horizon
        return np.zeros_like(y)


    # Non-zero Christoffel symbols (Gamma^mu_alpha_beta) for Schwarzschild:
    # Gamma^t_rt = Gamma^t_tr = M / (r * (r - 2M))
    # Gamma^r_tt = M * (r - 2M) / r^3
    # Gamma^r_rr = -M / (r * (r - 2M))
    # Gamma^r_thth = -(r - 2M)
    # Gamma^r_phph = -(r - 2M) * np.sin(th)**2
    # Gamma^th_rth = Gamma^th_thr = 1/r
    # Gamma^th_phph = -np.sin(th) * np.cos(th)
    # Gamma^ph_rph = Gamma^ph_phr = 1/r
    # Gamma^ph_thph = Gamma^ph_phth = np.cos(th) / np.sin(th) (cot(th))

    # Derivatives (d^2 x^mu / d lambda^2 = - Gamma^mu_alpha_beta * v^alpha * v^beta)
    # dt/dlambda is y[4] = vt
    # dr/dlambda is y[5] = vr
    # dtheta/dlambda is y[6] = vth
    # dphi/dlambda is y[7] = vph

    dv_t_dlambda = (-2 * M_val / (r * (r - 2 * M_val))) * vr * vt # -2 * Gamma^t_rt * vr * vt

    dv_r_dlambda = (-(M_val * (r - 2 * M_val) / r**3) * vt**2 +
                    (M_val / (r * (r - 2 * M_val))) * vr**2 +
                    ((r - 2 * M_val)) * vth**2 +  # Note: This term is positive because Gamma^r_thth is negative
                    ((r - 2 * M_val) * np.sin(th)**2) * vph**2) # Same for phi term

    dv_th_dlambda = (-2 / r) * vr * vth + (np.sin(th) * np.cos(th)) * vph**2 # -2 * Gamma^th_rth * vr * vth - Gamma^th_phph * vph^2

    dv_ph_dlambda = (-2 / r) * vr * vph - (2 * np.cos(th) / (np.sin(th) + 1e-9)) * vth * vph # -2 * Gamma^ph_rph * vr * vph - 2 * Gamma^ph_thph * vth * vph
                                                                                    # Added 1e-9 to sin(th) to avoid division by zero if th=0 or pi

    return [vt, vr, vth, vph, dv_t_dlambda, dv_r_dlambda, dv_th_dlambda, dv_ph_dlambda]


# --- 3. Solve Geodesics (Initial Value Problem Example) ---
M_val = 1.0  # Mass for Schwarzschild spacetime

# Initial conditions for a particle:
# Start at r=10, equatorial plane (theta=pi/2), phi=0.
# Timelike geodesic: g_munu v^mu v^nu = -1 (for massive particle, lambda = proper time tau)
# Null geodesic: g_munu v^mu v^nu = 0 (for light, lambda is an affine parameter)

r_init = 10.0
th_init = np.pi/2
ph_init = 0.0
t_init = 0.0

# For a particle dropped from "rest" (dr/dt=0, dtheta/dt=0, dphi/dt=0 initially in coordinate time)
# We need to find dt/dlambda (vt_init).
# For timelike geodesic (massive particle "dropped"):
# -(1 - 2M/r) * (dt/dlambda)^2 = -1  (since vr, vth, vph are 0 relative to lambda initially IF d(coord)/dt = 0)
# This means (dt/dlambda)^2 = 1 / (1 - 2M/r)
vt_init_timelike = 1.0 / np.sqrt(1 - 2 * M_val / r_init)
vr_init_timelike = 0.0 # Initially no radial velocity *with respect to affine parameter*
vth_init_timelike = 0.0
vph_init_timelike = 0.00 # Slight angular momentum to see orbit if not falling straight in

y0_timelike = [t_init, r_init, th_init, ph_init,
               vt_init_timelike, vr_init_timelike, vth_init_timelike, vph_init_timelike]

# For a null geodesic (light ray) shot radially outward:
# -(1 - 2M/r) * (dt/dlambda)^2 + (1/(1-2M/r)) * (dr/dlambda)^2 = 0
# Let dr/dlambda = 1 (arbitrary scaling for affine parameter)
# (dt/dlambda)^2 = (dr/dlambda)^2 / (1 - 2M/r)^2
# dt/dlambda = (dr/dlambda) / (1 - 2M/r)
dr_dlambda_null = 1.0 # moving outwards
vt_init_null = dr_dlambda_null / (1 - 2 * M_val / r_init)
vph_init_null_tangential = 0.1 / r_init # give it some tangential velocity for an orbit-like path
# Recheck null condition for tangential: r^2 sin^2(th) (vph_null)^2 = (1-2M/r)(vt_null)^2 - (1/(1-2M/r))(vr_null)^2
# If purely tangential vr=0: r^2 sin^2(th) (vph_null)^2 = (1-2M/r)(vt_null)^2
# vt_null = r * sin(th) * vph_null / sqrt(1-2M/r)

# Let's simplify: shoot light tangentially with a small outward radial component
vr_init_null_outward = 0.1 # Small outward radial
vph_init_null_tangential = 1.0 / r_init # Initial angular velocity for null geodesic
# From null condition: -(1-2M/r)vt^2 + (1/(1-2M/r))vr^2 + r^2 vph^2 = 0 (assuming th=pi/2, vth=0)
vt_init_null_orbit = np.sqrt( ( (1/(1-2*M_val/r_init))*vr_init_null_outward**2 + r_init**2 * vph_init_null_tangential**2 ) / (1-2*M_val/r_init) )


y0_null = [t_init, r_init, th_init, ph_init,
           vt_init_null_orbit, vr_init_null_outward, 0.0, vph_init_null_tangential]


print(f"\nInitial conditions for timelike geodesic (falling): {y0_timelike}")
print(f"Initial conditions for null geodesic (orbiting outwards): {y0_null}")

# Time span for integration (affine parameter lambda)
lambda_span = (0, 100)
lambda_eval_points = np.linspace(lambda_span[0], lambda_span[1], 500)

print("\nCalculating timelike geodesic trajectory (falling)...")
sol_timelike = solve_ivp(schwarzschild_geodesic_odes, lambda_span, y0_timelike,
                         dense_output=True, args=(M_val,), method='RK45', rtol=1e-7, atol=1e-9)
traj_timelike_lambda = lambda_eval_points
traj_timelike_solution = sol_timelike.sol(traj_timelike_lambda)
# solution is [t, r, th, ph, vt, vr, vth, vph]
t_tl, r_tl, th_tl, ph_tl = traj_timelike_solution[0], traj_timelike_solution[1], traj_timelike_solution[2], traj_timelike_solution[3]
print("Timelike trajectory calculated.")

print("\nCalculating null geodesic trajectory (orbiting outwards)...")
sol_null = solve_ivp(schwarzschild_geodesic_odes, lambda_span, y0_null,
                     dense_output=True, args=(M_val,), method='RK45', rtol=1e-7, atol=1e-9)
traj_null_lambda = lambda_eval_points
traj_null_solution = sol_null.sol(traj_null_lambda)
t_nl, r_nl, th_nl, ph_nl = traj_null_solution[0], traj_null_solution[1], traj_null_solution[2], traj_null_solution[3]
print("Null trajectory calculated.")

# --- 4. Visualization of Geodesics ---
print("\n--- Visualizing Example Geodesics (Schwarzschild) ---")
fig_geo = plt.figure(figsize=(10, 8))
ax_geo = fig_geo.add_subplot(111, projection='3d')

# Convert spherical to Cartesian for plotting
x_tl = r_tl * np.sin(th_tl) * np.cos(ph_tl)
y_tl = r_tl * np.sin(th_tl) * np.sin(ph_tl)
z_tl = r_tl * np.cos(th_tl)
ax_geo.plot(x_tl, y_tl, z_tl, label='Timelike Geodesic (falling)', color='blue')
ax_geo.scatter(x_tl[0], y_tl[0], z_tl[0], color='blue', marker='o', s=50, label='Start (Timelike)')
ax_geo.scatter(x_tl[-1], y_tl[-1], z_tl[-1], color='blue', marker='x', s=50, label='End (Timelike)')

x_nl = r_nl * np.sin(th_nl) * np.cos(ph_nl)
y_nl = r_nl * np.sin(th_nl) * np.sin(ph_nl)
z_nl = r_nl * np.cos(th_nl)
ax_geo.plot(x_nl, y_nl, z_nl, label='Null Geodesic (orbiting outwards)', color='orange', linestyle='--')
ax_geo.scatter(x_nl[0], y_nl[0], z_nl[0], color='orange', marker='o', s=50, label='Start (Null)')
ax_geo.scatter(x_nl[-1], y_nl[-1], z_nl[-1], color='orange', marker='x', s=50, label='End (Null)')


# Plot the "central mass" and event horizon (Schwarzschild radius = 2M)
ax_geo.scatter([0], [0], [0], color='black', s=200, label=f'Central Mass (M={M_val})', depthshade=False)
schwarzschild_radius = 2 * M_val
u = np.linspace(0, 2 * np.pi, 100)
v = np.linspace(0, np.pi, 100)
x_h = schwarzschild_radius * np.outer(np.cos(u), np.sin(v))
y_h = schwarzschild_radius * np.outer(np.sin(u), np.sin(v))
z_h = schwarzschild_radius * np.outer(np.ones(np.size(u)), np.cos(v))
ax_geo.plot_surface(x_h, y_h, z_h, color='gray', alpha=0.3, rstride=5, cstride=5, label=f'Event Horizon (r_s={schwarzschild_radius})')

ax_geo.set_xlabel('X')
ax_geo.set_ylabel('Y')
ax_geo.set_zlabel('Z')
ax_geo.set_title(f'Example Geodesics in Schwarzschild Spacetime (M={M_val}) - Spatial Projection')

# Set limits to better visualize orbits, prevent trajectories from going too far.
max_r_plot = max(r_init * 2, np.max(r_nl[r_nl < r_init * 5]) if len(r_nl[r_nl < r_init * 5]) > 0 else r_init*2) # Avoid extreme zoom out if one particle escapes far
ax_geo.set_xlim([-max_r_plot, max_r_plot])
ax_geo.set_ylim([-max_r_plot, max_r_plot])
ax_geo.set_zlim([-max_r_plot, max_r_plot])
ax_geo.legend()
plt.show()


# --- 5. Conceptual Embedding with a Toy Cartesian "Curved" Metric (Re-run from previous notebook) ---
# For this part, we'll reuse the `embed_hierarchy_curved` and `plot_embedding`
# with your `toy_curved_metric` or the `schwarzschild_inspired_cartesian_metric`.
# This part remains a conceptual illustration because solving the two-point boundary value
# problem for geodesics within the embedding loop is a major step up in complexity.

print("\n--- Conceptual Embedding with Toy Curved Metric (from previous logic) ---")
# Using the simpler tokens list from your first cell for this part
tokens_embed = ['animal', 'mammal', 'dog', 'cat', 'beagle', 'persian']
hierarchy_pairs_embed = [
    ('mammal', 'animal'),
    ('dog', 'mammal'),
    ('cat', 'mammal'),
    ('beagle', 'dog'),
    ('persian', 'cat')
]

toy_curved_events = embed_hierarchy_curved(
    tokens_embed,
    hierarchy_pairs_embed,
    toy_curved_metric, # Using the simpler toy metric
    epsilon1=1e-5,
    max_iterations=200
)
plot_embedding(toy_curved_events, "Toy Curved Spacetime Embedding (Conceptual)")


print("\n--- Key Differences and Next Steps for True GR-based Embedding ---")
print("1. Geodesic as the Path: The interval between two events for the 'almost null' condition should be calculated along the *geodesic* connecting them, not a straight line in coordinate space with a midpoint metric.")
print("2. Two-Point Boundary Value Problem: The core of a GR-based embedding algorithm would be to iteratively find coordinates for events (P, C) and the *initial velocity* at P such that the resulting geodesic from P hits C, and the proper time along this geodesic squared is ~ -epsilon^2.")
print("3. Optimization: This becomes a very complex optimization problem, likely requiring techniques like the shooting method or variational principles combined with numerical ODE solvers.")
print("4. Metric Choice/Learning: For a conceptual landscape, the Schwarzschild metric is an analogy. A true 'conceptual metric' might be learned from data or derived from a theory of 'conceptual gravity'.")
print("5. Computational Cost: Repeatedly solving geodesic BVPs for all pairs in a hierarchy is highly computationally intensive.")

"""Using Global Diffeomorphism.

phase 2 ok
"""

import numpy as np
from scipy.integrate import solve_ivp
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
# Assuming PCA and defaultdict are imported if you reuse the LLM's flat embedding function later

# --- Retain Schwarzschild Geodesic Solver (for context/illustration) ---
# (Code from the previous cell for schwarzschild_metric_components_spherical,
# schwarzschild_geodesic_odes, and the plotting of example geodesics
# can be collapsed or kept here for reference. For brevity, I'll omit re-pasting it
# but assume it's available if you want to run that part again.)
print("--- Phase 2 (Revised): Curved Conceptual Landscape via Diffeomorphism ---")

# --- 1. Define a Diffeomorphism (phi) and its Inverse (phi_inverse) ---
# This is a TOY diffeomorphism to illustrate the concept.
# M: Conceptual Manifold Coordinates (abstract, e.g., c_t, c_x, c_y, c_z)
# E: Flat Minkowski Embedding/Computational Space Coordinates (e_t, e_x, e_y, e_z)

def phi_toy_curvature(m_coords, k_spatial=0.1, k_time_radial=0.05, M_conceptual=0.5):
    """
    Maps from conceptual manifold M (m_coords) to flat Minkowski space E (e_coords).
    Introduces spatial warping and time dilation-like effects.
    m_coords = [m_t, m_x, m_y, m_z]
    """
    m_t, m_x, m_y, m_z = m_coords
    e_coords = np.zeros_like(m_coords)

    # Spatial warping: concepts "further out" in M get mapped to even further out in E
    # This creates an "expanding" conceptual space feel.
    # Or, if k_spatial is negative, a "compressing" feel.
    m_r_sq = m_x**2 + m_y**2 + m_z**2
    spatial_warp_factor = (1 + k_spatial * m_r_sq)

    e_coords[1] = m_x * spatial_warp_factor
    e_coords[2] = m_y * spatial_warp_factor
    e_coords[3] = m_z * spatial_warp_factor

    # Time "dilation/contraction" based on proximity to a "central conceptual mass"
    # and also overall temporal scaling.
    # This is highly heuristic.
    # Avoid r=0 singularity for the time factor
    time_radial_factor = (1 - M_conceptual / (np.sqrt(m_r_sq) + 1e-3)) # Similar to g_tt in Schwarzschild

    e_coords[0] = m_t * time_radial_factor # * (1 + k_time_radial * m_t) # Optional self-time warping

    return e_coords

def phi_inverse_toy_curvature(e_coords, k_spatial=0.1, k_time_radial=0.05, M_conceptual=0.5):
    """
    Maps from flat Minkowski space E (e_coords) back to conceptual manifold M (m_coords).
    This can be hard to find analytically for complex phi. For this toy phi:
    We need to solve for m_x, m_y, m_z from e_x, e_y, e_z first.
    Let e_r_sq = e_x^2 + e_y^2 + e_z^2. Then e_r = m_r * (1 + k_spatial * m_r^2).
    This is a cubic equation in m_r, which is hard to invert simply.

    Simplification for inverse:
    If k_spatial is small, m_r approx e_r. Use this to approximate factors.
    This makes it an *approximate* inverse for visualization.
    For true diffeomorphism, phi must be invertible.
    """
    e_t, e_x, e_y, e_z = e_coords
    m_coords = np.zeros_like(e_coords)

    # Approximate inverse for spatial coordinates
    # If e_r = m_r * (1 + k_spatial * m_r^2), then if k_spatial * m_r^2 << 1, m_r approx e_r.
    # This is a strong simplification for the inverse.
    # A better way for a general phi might involve numerical root finding.
    e_r_sq_approx = e_x**2 + e_y**2 + e_z**2
    m_r_approx = np.sqrt(e_r_sq_approx) # Very rough if spatial_warp_factor is not close to 1

    # Solve m_r * (1 + k_spatial * m_r^2) - e_r = 0 numerically for m_r
    # For simplicity here, we'll use a very rough approximation:
    if k_spatial == 0:
        spatial_unwarp_factor_approx = 1.0
    else:
        # Iterative approach to find m_r (e.g., Newton's method or bisection if bounds known)
        # Or a simpler approximation: if e_r = m_r(1+k*m_r^2) -> m_r ~ e_r / (1+k*e_r^2)
        # This is still not great. Let's assume k_spatial is small enough that m_r ~ e_r
        spatial_unwarp_factor_approx = (1 + k_spatial * e_r_sq_approx) # This is incorrect for inverse
        # If e_s = m_s * (1+k*m_r^2), then m_s = e_s / (1+k*m_r^2). We need m_r.
        # Let's assume m_x = e_x / factor, m_y = e_y / factor, m_z = e_z / factor
        # where factor = (1 + k_spatial * (m_x^2+m_y^2+m_z^2)).
        # This requires solving a non-linear system.
        # For a *toy* example, let's just scale back proportionally, very roughly.
        # This inverse is problematic and for a real system needs care.
        # We'll use a very simple inverse: m_s = e_s / (1 + k_spatial * e_r^2_approx) - not robust
        if spatial_unwarp_factor_approx == 0 : spatial_unwarp_factor_approx = 1 # avoid division by zero
        m_coords[1] = e_x / spatial_unwarp_factor_approx
        m_coords[2] = e_y / spatial_unwarp_factor_approx
        m_coords[3] = e_z / spatial_unwarp_factor_approx

    m_r_sq_final_approx = m_coords[1]**2 + m_coords[2]**2 + m_coords[3]**2
    time_radial_factor_approx = (1 - M_conceptual / (np.sqrt(m_r_sq_final_approx) + 1e-3))

    if time_radial_factor_approx == 0: time_radial_factor_approx = 1e-9 # avoid division by zero
    m_coords[0] = e_t / time_radial_factor_approx

    return m_coords

# --- 2. New ConceptualEvent Class ---
class ConceptualEvent:
    def __init__(self, token_id, phi_func, phi_inv_func, initial_m_coords=None):
        self.token_id = token_id
        self.phi = phi_func
        self.phi_inverse = phi_inv_func

        if initial_m_coords is None:
            # Initialize conceptual manifold coordinates (m_coords)
            # e.g., spatially uniformly in (-1, 1) and m_t to 0
            self.m_coords = np.array([0.0,
                                      np.random.uniform(-1, 1),
                                      np.random.uniform(-1, 1),
                                      np.random.uniform(-1, 1)])
        else:
            self.m_coords = np.array(initial_m_coords, dtype=float)

        # Compute corresponding embedding space coordinates (e_coords)
        self.e_coords = self.phi(self.m_coords)

    # Properties for e_coords (computational space)
    @property
    def e_t(self):
        return self.e_coords[0]

    @e_t.setter
    def e_t(self, value):
        self.e_coords[0] = value
        self.m_coords = self.phi_inverse(self.e_coords) # Update m_coords if e_coords change

    @property
    def e_spatial(self):
        return self.e_coords[1:]

    # Properties for m_coords (conceptual manifold space)
    @property
    def m_t(self):
        return self.m_coords[0]

    @property
    def m_spatial(self):
        return self.m_coords[1:]


    def minkowski_interval_sq_in_E(self, other_event):
        """
        Calculates the squared Minkowski interval (ds^2) in the flat embedding space E
        between this event's e_coords and another's e_coords.
        ds^2 = - (delta e_t)^2 + (delta e_x)^2 + (delta e_y)^2 + (delta e_z)^2
        """
        delta_e_coords = self.e_coords - other_event.e_coords
        delta_e_t = delta_e_coords[0]
        delta_e_spatial_sq = np.sum(delta_e_coords[1:]**2)

        ds_sq_E = -delta_e_t**2 + delta_e_spatial_sq
        return ds_sq_E

# --- 3. Modified Embedding Algorithm ---
def embed_hierarchy_diffeomorphism(tokens, hierarchy_pairs,
                                   phi_func, phi_inv_func,
                                   epsilon1=1e-5, epsilon2=0, max_iterations=1000,
                                   k_spatial_phi=0.1, k_time_radial_phi=0.05, M_conceptual_phi=0.5): # phi params
    """
    Embeds hierarchy by operating on e_coords in flat Minkowski space E,
    then maps back to conceptual manifold M.
    """

    # Pass phi parameters to the event constructor
    event_map = {token_id: ConceptualEvent(token_id,
                                           lambda m: phi_func(m, k_spatial_phi, k_time_radial_phi, M_conceptual_phi),
                                           lambda e: phi_inv_func(e, k_spatial_phi, k_time_radial_phi, M_conceptual_phi)
                                          ) for token_id in tokens}

    print(f"Starting embedding with diffeomorphism: {phi_func.__name__}")

    for iteration in range(max_iterations):
        violations_found = False
        for child_id, parent_id in hierarchy_pairs:
            child_event = event_map[child_id]
            parent_event = event_map[parent_id]

            # Calculations are done in the flat embedding space E
            ds_sq_E = child_event.minkowski_interval_sq_in_E(parent_event)

            target_ds_sq_E = -epsilon1**2 # Target "almost null" in E

            # Causal order check in E
            if child_event.e_t <= parent_event.e_t:
                violations_found = True
                delta = (parent_event.e_t - child_event.e_t) + 0.1
                child_event.e_t += delta * (1 - epsilon2) # This updates e_coords and m_coords via setter
                parent_event.e_t -= delta * epsilon2
                continue

            # "Almost null" or timelike check in E
            if ds_sq_E > 0 or ds_sq_E < target_ds_sq_E: # Spacelike or "too" timelike in E
                violations_found = True
                # Heuristic time adjustment in E space
                # (Same logic as before, but applied to e_t and ds_sq_E)
                delta_e_t_current_sq = (child_event.e_t - parent_event.e_t)**2
                spatial_dist_sq_E = ds_sq_E + delta_e_t_current_sq # D_XY^2 in E

                # We want new_delta_e_t_sq = spatial_dist_sq_E - target_ds_sq_E
                # (target_ds_sq_E is negative, so -target_ds_sq_E is positive)
                if spatial_dist_sq_E - target_ds_sq_E < 0: # Should not happen if epsilon1 is small
                    # This means spatial distance is too small for the target timelike interval
                    # For now, we'll just try to make it marginally timelike by a small push
                    target_new_delta_e_t = np.sqrt(spatial_dist_sq_E + epsilon1**2 * 0.1) + 1e-4 # Ensure > D_XY
                else:
                    target_new_delta_e_t = np.sqrt(spatial_dist_sq_E - target_ds_sq_E)

                # The adjustment logic from the original paper:
                # T_min = sqrt(D_XY^2 + epsilon1^2)
                # delta = T_min - (current T_XY)
                # child_t += delta * (1-epsilon2); parent_t -= delta * epsilon2
                current_delta_e_t = child_event.e_t - parent_event.e_t
                delta_adjustment = target_new_delta_e_t - current_delta_e_t

                child_event.e_t += delta_adjustment * (1 - epsilon2)
                parent_event.e_t -= delta_adjustment * epsilon2

        if not violations_found:
            print(f"Convergence achieved in E-space after {iteration} iterations.")
            break
        if iteration == max_iterations - 1:
            print(f"Max iterations ({max_iterations}) reached for E-space. Not fully converged.")

    # Final m_coords are already updated by the e_t.setter
    # Create a map of m_coords for plotting/output
    m_coords_map = {name: event.m_coords for name, event in event_map.items()}
    return m_coords_map


# --- 4. Example Usage & Visualization ---
tokens = ['universe', 'galaxy', 'star_system', 'star', 'planet', 'sun', 'earth', 'milky_way', 'solar_system']
hierarchy_pairs = [
    ('galaxy', 'universe'),
    ('star_system', 'galaxy'),
    ('star', 'star_system'),
    ('planet', 'star_system'),
    ('sun', 'star'), # Sun is a star in our solar system
    ('earth', 'planet'), # Earth is a planet in our solar system
    ('milky_way', 'galaxy'),
    ('solar_system', 'star_system'),
    ('solar_system', 'milky_way'), # Our solar system is in Milky Way
    ('sun', 'solar_system'),
    ('earth', 'solar_system')
]

# Define parameters for our toy diffeomorphism
phi_params = {
    'k_spatial_phi': 0.05, # Gentle spatial expansion further from m-origin
    'k_time_radial_phi': 0.0, # No explicit self-time warping for now
    'M_conceptual_phi': 0.1  # Small "conceptual mass" effect
}

print("\n--- Embedding with Toy Diffeomorphism ---")
diffeo_m_coords_map = embed_hierarchy_diffeomorphism(
    tokens, hierarchy_pairs,
    phi_toy_curvature, phi_inverse_toy_curvature,
    epsilon1=1e-4, max_iterations=500, # Adjusted epsilon, more iterations
    **phi_params
)

# Visualization of m_coords (the "curved" conceptual landscape)
# We need a plotting function that takes the m_coords_map
def plot_m_embedding(m_coords_map_plot, title="Conceptual Manifold Embedding (m_coords)"):
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    points = np.array(list(m_coords_map_plot.values()))
    labels = list(m_coords_map_plot.keys())

    scatter = ax.scatter(points[:, 1], points[:, 2], points[:, 0], c=points[:, 3], cmap='plasma', s=100)
    ax.set_xlabel('Conceptual X (m_x)')
    ax.set_ylabel('Conceptual Y (m_y)')
    ax.set_zlabel('Conceptual Time (m_t)')
    fig.colorbar(scatter, label='Conceptual Z (m_z)')

    for i, txt in enumerate(labels):
        ax.text(points[i, 1], points[i, 2], points[i, 0], f' {txt}', size=8, zorder=1)

    for child_id, parent_id in hierarchy_pairs:
        if parent_id is None or child_id not in m_coords_map_plot or parent_id not in m_coords_map_plot:
            continue
        child_m_coords = m_coords_map_plot[child_id]
        parent_m_coords = m_coords_map_plot[parent_id]
        ax.plot(
            [parent_m_coords[1], child_m_coords[1]],
            [parent_m_coords[2], child_m_coords[2]],
            [parent_m_coords[0], child_m_coords[0]],
            color='green', linestyle=':', alpha=0.6
        )
    ax.set_title(title)
    plt.show()

plot_m_embedding(diffeo_m_coords_map, f"Conceptual Landscape (m_coords) via Diffeomorphism (phi_toy_curvature with k_s={phi_params['k_spatial_phi']}, M_c={phi_params['M_conceptual_phi']})")


print("\n--- Discussion on Diffeomorphism Approach ---")
print("1. Effective Curvature: The `phi_toy_curvature` function introduces non-linear transformations. When we embed in the flat 'E-space' (Minkowski) and then map back to 'M-space' using `phi_inverse`, the geometry in M-space is effectively curved. Straight lines (almost null geodesics) in E-space become curved paths in M-space.")
print("2. Choice of Phi: The nature of the curvature is entirely determined by `phi`. A more principled `phi` could be derived from assumptions about how conceptual density or importance warps conceptual 'distance' and 'time'.")
print("3. Inverse Phi: A robust `phi_inverse` is crucial. For complex `phi`, this might require numerical methods (e.g., `scipy.optimize.fsolve`) to find `m_coords` given `e_coords` by solving `phi(m_coords) - e_coords = 0`.")
print("4. Computational Advantage: All iterative adjustments and interval calculations happen in the flat Minkowski E-space, avoiding direct, complex GR calculations (Christoffel symbols, geodesic ODEs) within the optimization loop.")
print("5. Interpretation: The resulting `m_coords` represent concepts in a landscape where distances and causal relations are non-trivial. For instance, two concepts might be far apart in `m_spatial` coordinates but 'causally close' because `phi` maps them to be so in E-space.")
print("6. Next Steps (Advanced):")
print("   - Design more sophisticated `phi` functions based on conceptual hypotheses (e.g., making 'time' flow slower near 'dense' concepts, or warping space around influential ideas).")
print("   - Explore learning `phi` itself from data, as suggested by the Riemannian Flow Matching paper, to discover the intrinsic geometry of the conceptual hierarchy.")
print("   - For analysis on M, one could compute the pullback metric: g_M = (J_phi)^T g_E J_phi, where J_phi is the Jacobian of phi. This g_M would be the explicit metric tensor of your curved conceptual manifold.")

# Schwarzschild geodesic plotting part (can be run for context, assuming it's defined above)
# This is separate from the diffeomorphism embedding.
# (Plotting code for schwarzschild_geodesic_odes solution omitted for brevity here,
#  but it's the same as in the previous response if you want to run it)

import numpy as np
from scipy.integrate import solve_ivp
from scipy.optimize import fsolve # For numerical inverse
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# --- Assume all previous class/function definitions from earlier cells are available ---
# Event, ConceptualEvent, embed_hierarchy_curved, plot_embedding,
# flat_minkowski_metric, toy_curved_metric, schwarzschild_metric_components_spherical,
# schwarzschild_geodesic_odes, phi_toy_curvature (previous simple inverse).
# For clarity, I'll redefine phi_toy_curvature here if it had the simple inverse before.

print("--- Phase 2 (Revised) - Step 2: Robust Inverse and Pullback Metric ---")

# --- 1. Diffeomorphism (phi) and its Robust Inverse (phi_inverse_numerical) ---

def phi_toy_curvature(m_coords, k_spatial=0.1, k_time_radial=0.05, M_conceptual=0.5):
    """
    Maps from conceptual manifold M (m_coords) to flat Minkowski space E (e_coords).
    m_coords = [m_t, m_x, m_y, m_z]
    """
    m_t, m_x, m_y, m_z = m_coords
    e_coords = np.zeros_like(m_coords)

    m_r_sq = m_x**2 + m_y**2 + m_z**2
    m_r = np.sqrt(m_r_sq)
    spatial_warp_factor = (1 + k_spatial * m_r_sq)

    e_coords[1] = m_x * spatial_warp_factor
    e_coords[2] = m_y * spatial_warp_factor
    e_coords[3] = m_z * spatial_warp_factor

    time_radial_factor = (1 - M_conceptual / (m_r + 1e-6)) # Avoid division by zero
    if time_radial_factor <= 0: # ensure factor is positive for t scaling
        time_radial_factor = 1e-6

    e_coords[0] = m_t * time_radial_factor
    return e_coords

def phi_inverse_toy_curvature_numerical(e_coords_target, k_spatial=0.1, k_time_radial=0.05, M_conceptual=0.5):
    """
    Numerically inverts phi_toy_curvature using scipy.optimize.fsolve.
    Finds m_coords such that phi_toy_curvature(m_coords) - e_coords_target = 0.
    """
    def objective_function(m_coords_guess):
        # phi_toy_curvature expects k_spatial, k_time_radial, M_conceptual as args
        return phi_toy_curvature(m_coords_guess, k_spatial, k_time_radial, M_conceptual) - e_coords_target

    # Initial guess for m_coords (e.g., e_coords itself, or a simpler inverse if available)
    # A very rough initial guess might be e_coords, assuming deformations are not too extreme.
    initial_guess = e_coords_target.copy()

    # Handle the time component separately if its phi is simpler and invertible
    # e_t = m_t * time_radial_factor(m_x, m_y, m_z)
    # To make the fsolve more stable, we can try to solve for spatial first, then time.
    # Or, if k_spatial is 0, m_spatial = e_spatial.

    if k_spatial == 0: # If no spatial warping, m_spatial = e_spatial
        m_x_found, m_y_found, m_z_found = e_coords_target[1], e_coords_target[2], e_coords_target[3]
    else:
        # Solve for spatial components first by inverting the spatial part of phi
        def spatial_objective(m_spatial_guess):
            m_t_dummy = 0 # time component doesn't affect spatial phi in this toy model
            m_coords_for_phi = np.concatenate(([m_t_dummy], m_spatial_guess))
            e_spatial_phi = phi_toy_curvature(m_coords_for_phi, k_spatial, k_time_radial, M_conceptual)[1:]
            return e_spatial_phi - e_coords_target[1:]

        initial_spatial_guess = e_coords_target[1:]
        m_spatial_found, info_spatial, ier_spatial, msg_spatial = fsolve(spatial_objective, initial_spatial_guess, full_output=True)
        if ier_spatial != 1:
            print(f"Warning: fsolve for spatial part failed for e_coords {e_coords_target}: {msg_spatial}")
            # Fallback to a simpler approximation if fsolve fails
            e_r_sq = np.sum(e_coords_target[1:]**2)
            approx_factor_inv = (1 + k_spatial * e_r_sq) # This is actually spatial_warp_factor for m_r=e_r
            m_x_found = e_coords_target[1] / approx_factor_inv
            m_y_found = e_coords_target[2] / approx_factor_inv
            m_z_found = e_coords_target[3] / approx_factor_inv
        else:
            m_x_found, m_y_found, m_z_found = m_spatial_found


    # With m_spatial found, calculate time_radial_factor and invert for m_t
    m_r_found_sq = m_x_found**2 + m_y_found**2 + m_z_found**2
    m_r_found = np.sqrt(m_r_found_sq)
    time_radial_factor_at_m_found = (1 - M_conceptual / (m_r_found + 1e-6))
    if time_radial_factor_at_m_found <= 1e-9: # Avoid division by zero or very small numbers
        time_radial_factor_at_m_found = 1e-9

    m_t_found = e_coords_target[0] / time_radial_factor_at_m_found

    return np.array([m_t_found, m_x_found, m_y_found, m_z_found])


# --- Update ConceptualEvent to use the numerical inverse ---
class ConceptualEvent: # Redefine to ensure it uses the new inverse
    def __init__(self, token_id, phi_func, phi_inv_func, initial_m_coords=None,
                 phi_params_dict=None): # Added phi_params_dict
        self.token_id = token_id
        self._phi_params = phi_params_dict if phi_params_dict is not None else {}
        self.phi = lambda m: phi_func(m, **self._phi_params) # Pass params to phi
        self.phi_inverse = lambda e: phi_inv_func(e, **self._phi_params) # Pass params to phi_inverse


        if initial_m_coords is None:
            self.m_coords = np.array([0.0,
                                      np.random.uniform(-1, 1),
                                      np.random.uniform(-1, 1),
                                      np.random.uniform(-1, 1)])
        else:
            self.m_coords = np.array(initial_m_coords, dtype=float)

        self.e_coords = self.phi(self.m_coords)

    @property
    def e_t(self): return self.e_coords[0]
    @e_t.setter
    def e_t(self, value):
        self.e_coords[0] = value
        self.m_coords = self.phi_inverse(self.e_coords)
    @property
    def e_spatial(self): return self.e_coords[1:]
    @property
    def m_t(self): return self.m_coords[0]
    @property
    def m_spatial(self): return self.m_coords[1:]

    def minkowski_interval_sq_in_E(self, other_event):
        delta_e_coords = self.e_coords - other_event.e_coords
        delta_e_t = delta_e_coords[0]
        delta_e_spatial_sq = np.sum(delta_e_coords[1:]**2)
        return -delta_e_t**2 + delta_e_spatial_sq

# --- 2. Jacobian of phi_toy_curvature (Analytical) ---
def jacobian_phi_toy_curvature(m_coords, k_spatial=0.1, k_time_radial=0.05, M_conceptual=0.5):
    """
    Analytically computes the Jacobian J_ij = de_i / dm_j for phi_toy_curvature.
    Coords order: (t, x, y, z)
    """
    m_t, m_x, m_y, m_z = m_coords
    J = np.zeros((4, 4))

    m_r_sq = m_x**2 + m_y**2 + m_z**2
    m_r = np.sqrt(m_r_sq)

    # Derivatives for e_t
    m_r_plus_eps = m_r + 1e-6
    term_in_time_factor = M_conceptual / m_r_plus_eps
    time_radial_factor = (1 - term_in_time_factor)
    if time_radial_factor <= 0: time_radial_factor = 1e-6


    J[0, 0] = time_radial_factor # de_t / dm_t
    if m_r > 1e-9: # Avoid division by zero if m_r is very small
        de_t_dm_spatial_factor = m_t * M_conceptual * (m_r_plus_eps)**-2 * (1.0 / m_r)
        J[0, 1] = de_t_dm_spatial_factor * m_x # de_t / dm_x
        J[0, 2] = de_t_dm_spatial_factor * m_y # de_t / dm_y
        J[0, 3] = de_t_dm_spatial_factor * m_z # de_t / dm_z
    else:
        J[0, 1] = J[0, 2] = J[0, 3] = 0


    # Derivatives for e_x
    J[1, 0] = 0 # de_x / dm_t
    J[1, 1] = (1 + k_spatial * m_r_sq) + m_x * (k_spatial * 2 * m_x) # de_x / dm_x
    J[1, 2] = m_x * (k_spatial * 2 * m_y) # de_x / dm_y
    J[1, 3] = m_x * (k_spatial * 2 * m_z) # de_x / dm_z

    # Derivatives for e_y
    J[2, 0] = 0 # de_y / dm_t
    J[2, 1] = m_y * (k_spatial * 2 * m_x) # de_y / dm_x
    J[2, 2] = (1 + k_spatial * m_r_sq) + m_y * (k_spatial * 2 * m_y) # de_y / dm_y
    J[2, 3] = m_y * (k_spatial * 2 * m_z) # de_y / dm_z

    # Derivatives for e_z
    J[3, 0] = 0 # de_z / dm_t
    J[3, 1] = m_z * (k_spatial * 2 * m_x) # de_z / dm_x
    J[3, 2] = m_z * (k_spatial * 2 * m_y) # de_z / dm_y
    J[3, 3] = (1 + k_spatial * m_r_sq) + m_z * (k_spatial * 2 * m_z) # de_z / dm_z

    return J

# --- 3. Pullback Metric g_M ---
def pullback_metric_gM(m_coords, k_spatial=0.1, k_time_radial=0.05, M_conceptual=0.5):
    """
    Computes the pullback metric g_M = J_phi^T eta_E J_phi on the manifold M.
    eta_E is the flat Minkowski metric diag([-1, 1, 1, 1]).
    """
    J_phi = jacobian_phi_toy_curvature(m_coords, k_spatial, k_time_radial, M_conceptual)
    eta_E = np.diag([-1.0, 1.0, 1.0, 1.0])

    g_M = J_phi.T @ eta_E @ J_phi
    return g_M

# --- 4. Re-run Embedding with Numerical Inverse ---
# (Need to redefine embed_hierarchy_diffeomorphism if it wasn't already using the new ConceptualEvent)
def embed_hierarchy_diffeomorphism(tokens, hierarchy_pairs,
                                   phi_func, phi_inv_func, # Now expects the numerical inverse
                                   epsilon1=1e-5, epsilon2=0, max_iterations=2000,
                                   phi_params_dict_for_event=None):
    """
    Embeds hierarchy by operating on e_coords in flat Minkowski space E,
    then maps back to conceptual manifold M.
    Uses the updated ConceptualEvent that takes phi_params_dict.
    """
    event_map = {token_id: ConceptualEvent(token_id,
                                           phi_func,
                                           phi_inv_func,
                                           phi_params_dict=phi_params_dict_for_event
                                          ) for token_id in tokens}

    print(f"Starting embedding with diffeomorphism: {phi_func.__name__} (using numerical inverse)")

    for iteration in range(max_iterations):
        violations_found = False
        for child_id, parent_id in hierarchy_pairs:
            if parent_id is None: continue # Skip root links for interval check
            child_event = event_map[child_id]
            parent_event = event_map[parent_id]

            ds_sq_E = child_event.minkowski_interval_sq_in_E(parent_event)
            target_ds_sq_E = -epsilon1**2

            if child_event.e_t <= parent_event.e_t:
                violations_found = True
                delta = (parent_event.e_t - child_event.e_t) + 0.01 # smaller push for numerical inverse
                current_child_et = child_event.e_t
                current_parent_et = parent_event.e_t
                child_event.e_t = current_child_et + delta * (1 - epsilon2)
                parent_event.e_t = current_parent_et - delta * epsilon2
                continue

            if ds_sq_E > 0 or ds_sq_E < target_ds_sq_E:
                violations_found = True
                delta_e_t_current = child_event.e_t - parent_event.e_t
                delta_e_t_current_sq = delta_e_t_current**2
                spatial_dist_sq_E = ds_sq_E + delta_e_t_current_sq

                if spatial_dist_sq_E - target_ds_sq_E < 0:
                     target_new_delta_e_t = np.sqrt(spatial_dist_sq_E + epsilon1**2 * 0.01) + 1e-5
                else:
                    target_new_delta_e_t = np.sqrt(spatial_dist_sq_E - target_ds_sq_E)

                delta_adjustment = target_new_delta_e_t - delta_e_t_current
                current_child_et = child_event.e_t # Re-fetch in case other adjustments happened
                current_parent_et = parent_event.e_t

                child_event.e_t = current_child_et + delta_adjustment * (1 - epsilon2)
                parent_event.e_t = current_parent_et - delta_adjustment * epsilon2

        if not violations_found:
            print(f"Convergence achieved in E-space after {iteration} iterations.")
            break
        if iteration % 50 == 0 : print(f"Iteration {iteration}...") # Progress
        if iteration == max_iterations - 1:
            print(f"Max iterations ({max_iterations}) reached for E-space. Not fully converged.")

    m_coords_map = {name: event.m_coords for name, event in event_map.items()}
    return m_coords_map


# --- Example Usage ---
tokens_pb = ['universe', 'galaxy', 'star_system', 'star', 'planet', 'sun', 'earth', 'milky_way', 'solar_system']
hierarchy_pairs_pb = [
    ('galaxy', 'universe'), ('star_system', 'galaxy'), ('star', 'star_system'),
    ('planet', 'star_system'), ('sun', 'star'), ('earth', 'planet'),
    ('milky_way', 'galaxy'), ('solar_system', 'star_system'),
    ('solar_system', 'milky_way'), ('sun', 'solar_system'), ('earth', 'solar_system')
]

phi_params_pb = {
    'k_spatial': 0.05,
    'k_time_radial': 0.0,
    'M_conceptual': 0.1
}

print("\n--- Embedding with Toy Diffeomorphism (Numerical Inverse) ---")
diffeo_m_coords_map_pb = embed_hierarchy_diffeomorphism(
    tokens_pb, hierarchy_pairs_pb,
    phi_toy_curvature, phi_inverse_toy_curvature_numerical, # Pass the numerical inverse
    epsilon1=1e-4, max_iterations=2000, # Fewer iterations for demo speed
    phi_params_dict_for_event=phi_params_pb
)

plot_m_embedding(diffeo_m_coords_map_pb, f"Manifold (m_coords) via Numerical Inverse Phi (k_s={phi_params_pb['k_spatial']}, M_c={phi_params_pb['M_conceptual']})")


# --- 5. Demonstrate Pullback Metric Calculation ---
print("\n--- Calculating Pullback Metric g_M at some embedded points ---")
if diffeo_m_coords_map_pb:
    points_to_check = ['universe', 'earth', 'sun']
    for name in points_to_check:
        if name in diffeo_m_coords_map_pb:
            m_coords = diffeo_m_coords_map_pb[name]
            g_M_at_point = pullback_metric_gM(m_coords, **phi_params_pb)
            print(f"\nPullback metric g_M at '{name}' (m_coords: {np.round(m_coords,2)}):")
            with np.printoptions(precision=3, suppress=True): # For cleaner printing
                 print(g_M_at_point)
            print(f"Determinant of g_M at '{name}': {np.linalg.det(g_M_at_point):.3e}")
            # A determinant far from -1 (Minkowski) indicates significant volume distortion.
else:
    print("Embedding map is empty, cannot calculate pullback metrics.")

print("\n--- Interpretation of Pullback Metric g_M ---")
print("g_M(m_coords) tells you how intervals (ds^2) are measured *on the conceptual manifold M* at point m_coords.")
print("- g_M[0,0]: The squared rate at which conceptual proper time (on M) passes relative to coordinate time m_t, at that point.")
print("  If g_M[0,0] is more negative than -1, conceptual proper time flows 'slower' than m_t.")
print("- g_M[i,i] (i=1,2,3): Relate to spatial distances along coordinate axes on M.")
print("  If g_M[i,i] > 1, coordinate distance dm_i corresponds to a larger proper distance on M (space is 'stretched').")
print("  If g_M[i,i] < 1, coordinate distance dm_i corresponds to a smaller proper distance on M (space is 'compressed').")
print("- Off-diagonal terms g_M[i,j] (i!=j): Indicate how spacetime axes are tilted or sheared relative to each other on M.")
print("- det(g_M): The determinant of the metric is related to the volume element. In flat Minkowski, det(eta)=-1.")
print("  Deviations from -1 indicate how conceptual 'volumes' are distorted by phi.")
print("\nThis pullback metric IS the geometry of your conceptual landscape induced by phi.")
print("Further analysis could involve calculating curvature scalars (Ricci scalar) from g_M, which is more involved.")

"""try with wordnet no regularation."""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from scipy.optimize import fsolve # For numerical inverse at inference/analysis
from collections import defaultdict # If needed for hierarchy processing

# --- Assume previous helper function definitions if needed ---
# (e.g., if you re-use parts of embed_hierarchy_diffeomorphism for inspiration)

print("--- Phase 3: Learning the Diffeomorphism phi ---")

# --- 1. Neural Network Definition for phi ---
# Maps m_coords (conceptual manifold) to e_coords (flat Minkowski embedding space)
# Both are 4D: [t, x, y, z]

class PhiNetwork(nn.Module):
    def __init__(self, input_dim=4, output_dim=4, hidden_dims=[64, 128, 64]):
        super(PhiNetwork, self).__init__()
        layers = []
        prev_dim = input_dim
        for h_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, h_dim))
            layers.append(nn.Tanh()) # Using Tanh for bounded activations, can experiment
            prev_dim = h_dim
        layers.append(nn.Linear(prev_dim, output_dim))
        self.network = nn.Sequential(*layers)

    def forward(self, m_coords_batch):
        # m_coords_batch is a tensor of shape (batch_size, 4)
        return self.network(m_coords_batch)

# --- 2. ConceptualEvent Class using Learned Phi ---
# For training, we primarily work with e_coords generated by phi.
# The m_coords will be the *initial random positions* that phi maps from.
# The "true" m_coords after training would require inverting phi.

class ConceptualEventLearnedPhi:
    def __init__(self, token_id, initial_m_coords_np, phi_nn_model):
        self.token_id = token_id
        # m_coords are fixed initial points; phi learns to map them appropriately
        self.initial_m_coords = torch.tensor(initial_m_coords_np, dtype=torch.float32)
        self.phi_nn = phi_nn_model

        # e_coords are learnable through the phi_nn network's parameters
        # For loss calculation, we'll pass initial_m_coords through phi_nn

    def get_e_coords(self):
        # Ensure phi_nn is in eval mode if not training phi itself directly here,
        # but typically phi_nn will be part of the computation graph.
        return self.phi_nn(self.initial_m_coords.unsqueeze(0)).squeeze(0) # Add batch dim, then remove

    def minkowski_interval_sq_in_E(self, other_event_e_coords_tensor):
        """Calculates Minkowski interval using current e_coords (PyTorch tensors)."""
        delta_e_coords = self.get_e_coords() - other_event_e_coords_tensor
        delta_e_t = delta_e_coords[0]
        delta_e_spatial_sq = torch.sum(delta_e_coords[1:]**2)
        return -delta_e_t**2 + delta_e_spatial_sq

# --- 3. Training Loop ---

def train_phi_embedding(
    tokens, hierarchy_pairs,
    phi_nn, # The neural network model for phi
    num_epochs=100, learning_rate=1e-3,
    epsilon1_target_sq=1e-5, # Target for -ds_sq_E (so ds_sq_E should be -epsilon1_target_sq)
    causal_margin = 0.01 # Child_t > Parent_t + causal_margin
):
    """
    Trains the phi_nn network to embed the hierarchy.
    """
    # Initialize m_coords randomly for each token (these are the inputs to phi_nn)
    initial_m_coords_map = {
        token_id: np.array([0.0] + np.random.uniform(-1, 1, 3).tolist(), dtype=np.float32)
        for token_id in tokens
    }

    # Create event objects (they store initial_m_coords and the phi_nn model)
    event_objects = {
        token_id: ConceptualEventLearnedPhi(token_id, initial_m_coords_map[token_id], phi_nn)
        for token_id in tokens
    }

    optimizer = optim.Adam(phi_nn.parameters(), lr=learning_rate)

    # For simplicity, use all pairs in each epoch (no explicit batching here)
    # Convert hierarchy_pairs to use event objects
    training_pairs = []
    for child_id, parent_id in hierarchy_pairs:
        if parent_id is not None: # We only care about actual parent-child links for loss
             if child_id in event_objects and parent_id in event_objects:
                training_pairs.append((event_objects[child_id], event_objects[parent_id]))

    if not training_pairs:
        print("No valid training pairs found. Aborting training.")
        return initial_m_coords_map # Return initial m_coords

    print(f"Starting training of phi_nn for {num_epochs} epochs...")
    for epoch in range(num_epochs):
        phi_nn.train() # Set model to training mode
        total_loss = 0.0

        for child_event, parent_event in training_pairs:
            optimizer.zero_grad()

            # Get current e_coords by passing fixed initial_m_coords through phi_nn
            e_coords_child = child_event.get_e_coords()
            e_coords_parent = parent_event.get_e_coords()

            # --- Causal Loss Component ---
            delta_e_t = e_coords_child[0] - e_coords_parent[0]
            # Loss if child_t <= parent_t + margin
            loss_causal_order = torch.relu(-(delta_e_t - causal_margin))

            # --- "Almost Null" Interval Loss Component in E ---
            # ds_sq_E = - (delta_e_t)^2 + ||delta_e_spatial||^2
            delta_e_spatial_sq = torch.sum((e_coords_child[1:] - e_coords_parent[1:])**2)
            ds_sq_E = -delta_e_t**2 + delta_e_spatial_sq

            # We want ds_sq_E to be close to -epsilon1_target_sq
            # Loss = (ds_sq_E - (-epsilon1_target_sq))^2 = (ds_sq_E + epsilon1_target_sq)^2
            loss_interval = (ds_sq_E + epsilon1_target_sq)**2

            # Combine losses (can add weights if needed)
            loss = loss_causal_order + loss_interval

            loss.backward() # Compute gradients
            optimizer.step() # Update phi_nn parameters

            total_loss += loss.item()

        if epoch % (num_epochs // 10 or 1) == 0:
            print(f"Epoch {epoch+1}/{num_epochs}, Avg Loss: {total_loss / len(training_pairs):.6f}")

    print("Training finished.")
    phi_nn.eval() # Set model to evaluation mode

    # After training, the "conceptual manifold" positions (m_coords) are still the initial random ones.
    # The learned phi_nn transforms these into an E-space where the hierarchy is respected.
    # To get the "final" positions on a manifold M that has been "shaped" by phi_nn,
    # we would need phi_nn_inverse. For now, we can visualize the initial_m_coords
    # and understand that their mapping through phi_nn creates the causal structure.

    # Or, we can visualize the *output* e_coords of phi_nn (which are in flat Minkowski)
    final_e_coords_map = {
        name: event.get_e_coords().detach().numpy()
        for name, event in event_objects.items()
    }
    # And for the m_coords (which were the inputs to phi), we can use the initial ones
    # or attempt a numerical inverse if needed for a specific interpretation of M.

    # For this demo, let's return the initial m_coords (which phi maps FROM)
    # and the final e_coords (which phi maps TO, and where the hierarchy is embedded)
    return initial_m_coords_map, final_e_coords_map, phi_nn


# --- Example Usage ---

tokens_lphi = ['universe', 'galaxy', 'star_system', 'star', 'planet', 'sun', 'earth', 'milky_way', 'solar_system']
hierarchy_pairs_lphi = [
    ('galaxy', 'universe'), ('star_system', 'galaxy'), ('star', 'star_system'),
    ('planet', 'star_system'), ('sun', 'star'), ('earth', 'planet'),
    ('milky_way', 'galaxy'), ('solar_system', 'star_system'),
    ('solar_system', 'milky_way'), ('sun', 'solar_system'), ('earth', 'solar_system')
]

# Instantiate the phi network
phi_model = PhiNetwork(input_dim=4, output_dim=4, hidden_dims=[128, 256, 128])

print("\n--- Training Phi Network for Conceptual Embedding ---")
initial_m_coords_map_trained, final_e_coords_map_trained, trained_phi_model = train_phi_embedding(
    tokens_lphi, hierarchy_pairs_lphi,
    phi_model,
    num_epochs=500, # Might need more for good convergence
    learning_rate=0.001,
    epsilon1_target_sq=1e-4, # Target -ds^2
    causal_margin=0.05
)

# --- 4. Visualization ---
# We can plot the initial_m_coords (inputs to phi)
# And the final_e_coords (outputs of phi, where embedding happened)
# To visualize the "curved conceptual manifold M", we would ideally have phi_inverse
# and apply it to a grid of e_coords, or analyze the pullback metric.

def plot_coords_map(coords_map, title="Coordinates Plot", z_label_suffix=""):
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    points = np.array(list(coords_map.values()))
    labels = list(coords_map.keys())
    scatter = ax.scatter(points[:, 1], points[:, 2], points[:, 0], c=points[:, 3], cmap='plasma', s=100)
    ax.set_xlabel(f'X {z_label_suffix}')
    ax.set_ylabel(f'Y {z_label_suffix}')
    ax.set_zlabel(f'Time {z_label_suffix}')
    fig.colorbar(scatter, label=f'Z {z_label_suffix}')
    for i, txt in enumerate(labels):
        ax.text(points[i, 1], points[i, 2], points[i, 0], f' {txt}', size=8, zorder=1)
    # Plot hierarchy links based on the plotted coordinates
    for child_id, parent_id in hierarchy_pairs_lphi:
        if parent_id is None or child_id not in coords_map or parent_id not in coords_map:
            continue
        child_coords_plot = coords_map[child_id]
        parent_coords_plot = coords_map[parent_id]
        ax.plot(
            [parent_coords_plot[1], child_coords_plot[1]],
            [parent_coords_plot[2], child_coords_plot[2]],
            [parent_coords_plot[0], child_coords_plot[0]],
            color='gray', linestyle=':', alpha=0.6
        )
    ax.set_title(title)
    plt.show()

plot_coords_map(initial_m_coords_map_trained, "Initial Conceptual Manifold Coords (m_coords - Inputs to Phi)")
for i in initial_m_coords_map_trained:
    print(i, initial_m_coords_map_trained[i])
    print(f"determinant of g_M at {i}: {np.linalg.det(pullback_metric_gM(initial_m_coords_map_trained[i], **phi_params_pb)):.3e}")
plot_coords_map(final_e_coords_map_trained, "Final Embedding Space Coords (e_coords - Outputs of Phi)")


# --- 5. Discussion on Pullback Metric and Next Steps ---
print("\n--- Pullback Metric and True Manifold Visualization (Next Steps) ---")
print("1. Learned Phi: The `trained_phi_model` now represents our diffeomorphism phi_theta.")
print("2. M-coords vs E-coords: We plotted the initial `m_coords` (which were random inputs) and the final `e_coords` (where the Minkowski embedding was enforced). The 'true' conceptual manifold positions that correspond to the learned `e_coords` would be `phi_inverse(e_coords)`.")
print("3. Numerical Inverse of NN Phi: To get these 'true' M-coords, we'd need to numerically invert `trained_phi_model`. This can be done using `scipy.optimize.fsolve` for each `e_coords_point` by solving `trained_phi_model(m_guess) - e_coords_point = 0`.")
print("4. Pullback Metric g_M:")
print("   - Once we have `phi_inverse` (even numerically), we can get the Jacobian of `phi_nn` (J_phi) using `torch.autograd.functional.jacobian`.")
print("   - Then, g_M = J_phi^T * eta_E * J_phi, where eta_E is diag([-1,1,1,1]).")
print("   - This g_M will vary with the m_coords and represents the learned geometry.")
print("5. Visualization of M: Plotting the `phi_inverse(final_e_coords_map)` would give the positions on M. Visualizing the curvature could involve plotting Ricci scalar derived from g_M, or showing how test geodesics (calculated using g_M) behave on this learned manifold.")
print("6. Invertible Neural Networks: For a more robust approach to learning phi and its inverse simultaneously, architectures like Normalizing Flows or other INNs should be explored. This avoids the need for numerical inversion during training or inference.")

# Example of how to get Jacobian for one point (conceptual)
# if initial_m_coords_map_trained:
#     example_m_coord_tensor = torch.tensor(initial_m_coords_map_trained['star'], dtype=torch.float32)
#     def phi_for_jacobian(m_in):
#         return trained_phi_model(m_in) # trained_phi_model is already a function

#     # jac_at_star = torch.autograd.functional.jacobian(phi_for_jacobian, example_m_coord_tensor)
#     # print(f"\nJacobian of phi_nn at 'star' (m_coords: {example_m_coord_tensor.numpy()}):")
#     # print(jac_at_star.detach().numpy())
#     # eta_E = torch.diag(torch.tensor([-1.0, 1.0, 1.0, 1.0]))
#     # g_M_at_star = jac_at_star.T @ eta_E @ jac_at_star
#     # print(f"\nPullback metric g_M at 'star':")
#     # print(g_M_at_star.detach().numpy())

# --- Code to be added AFTER training phi_model ---

# Make sure phi_model is the trained network from the previous cell
phi_model.eval() # Set to evaluation mode

# Example: Calculate pullback metric at the m_coord of 'star'
if 'star' in initial_m_coords_map_trained:
    m_coord_star_np = initial_m_coords_map_trained['star']
    m_coord_star_tensor = torch.tensor(m_coord_star_np, dtype=torch.float32).requires_grad_(False) # No grad needed for input

    # Define a wrapper for phi_model if jacobian function expects a specific input signature
    def phi_wrapper_for_jacobian(m_input_tensor):
        return phi_model(m_input_tensor)

    # Compute Jacobian J_phi = de_i / dm_j
    # jacobian function expects the input to be a single tensor
    # If m_coord_star_tensor is already (4,), it's fine. If it were (1,4), you might need to squeeze.
    J_phi_at_star = torch.autograd.functional.jacobian(phi_wrapper_for_jacobian, m_coord_star_tensor)
    # J_phi_at_star will have shape (output_dim, input_dim), so (4, 4)

    print(f"\nJacobian of phi_nn at 'star' (initial m_coords: {m_coord_star_np}):")
    print(J_phi_at_star.detach().numpy())

    # Minkowski metric in E-space (eta_E)
    eta_E_np = np.diag([-1.0, 1.0, 1.0, 1.0])
    eta_E_tensor = torch.tensor(eta_E_np, dtype=torch.float32)

    # Pullback metric g_M = J_phi^T * eta_E * J_phi
    g_M_at_star = J_phi_at_star.T @ eta_E_tensor @ J_phi_at_star

    print(f"\nPullback metric g_M at 'star' (based on initial m_coords):")
    with np.printoptions(precision=3, suppress=True):
        print(g_M_at_star.detach().numpy())

    det_g_M_at_star = torch.linalg.det(g_M_at_star)
    print(f"Determinant of g_M at 'star': {det_g_M_at_star.item():.3e}")

    # To get the g_M at the *final effective m_coord on the manifold*:
    # 1. Get e_coord_star_final_tensor = torch.tensor(final_e_coords_map_trained['star'], dtype=torch.float32)
    # 2. Numerically invert: m_coord_star_final_np = numerical_inverse_of_phi(e_coord_star_final_tensor, phi_model)
    # 3. m_coord_star_final_tensor_for_jac = torch.tensor(m_coord_star_final_np, dtype=torch.float32)
    # 4. J_phi_at_star_final = torch.autograd.functional.jacobian(phi_wrapper_for_jacobian, m_coord_star_final_tensor_for_jac)
    # 5. g_M_at_star_final = J_phi_at_star_final.T @ eta_E_tensor @ J_phi_at_star_final
    # This requires implementing the numerical_inverse_of_phi.

    print("\nNote: The g_M above is calculated at the *initial* random m_coord for 'star'.")
    print("To get g_M at the 'true' learned position on the manifold, phi_model would need to be inverted first.")

else:
    print("Concept 'star' not found in initial_m_coords_map_trained for Jacobian demo.")

# --- Code to be added AFTER training phi_model ---
# Ensure phi_model is the trained network from the previous cell (e.g., trained_phi_model)
# Also ensure initial_m_coords_map_trained and final_e_coords_map_trained are available.

print("\n--- Step 1 & 2: Calculating Final Manifold Coordinates (m*) via Numerical Inverse ---")

# Define the numerical inverse function specifically for the PyTorch NN
def phi_inverse_numerical_nn(e_coord_target_np, phi_network_model, initial_m_guess_np, tolerance=1e-7, max_iter=2000):
    """
    Numerically finds the m_coords corresponding to a given e_coords_target using a trained phi_network_model.
    """
    phi_network_model.eval() # Ensure model is in evaluation mode

    def objective_function(m_guess_np):
        m_guess_tensor = torch.tensor(m_guess_np, dtype=torch.float32).unsqueeze(0)
        with torch.no_grad(): # No need to track gradients for inference/solving
            e_output_tensor = phi_network_model(m_guess_tensor)
        e_target_tensor = torch.tensor(e_coord_target_np, dtype=torch.float32).unsqueeze(0)
        return (e_output_tensor - e_target_tensor).squeeze(0).numpy() # fsolve expects numpy array

    try:
        m_final_coord_np, info, ier, msg = fsolve(
            objective_function, initial_m_guess_np,
            xtol=tolerance, maxfev=max_iter, full_output=True
        )
        if ier == 1:
            return m_final_coord_np
        else:
            print(f"Warning: fsolve did not converge for e_coord {e_coord_target_np}. Message: {msg}. Using initial guess.")
            return initial_m_guess_np
    except Exception as e:
        print(f"Error during fsolve for e_coord {e_coord_target_np}: {e}. Using initial guess.")
        return initial_m_guess_np

final_m_coords_on_manifold_map = {}
print("Calculating final m_coords on manifold (this may take some time)...")
count = 0
total_tokens = len(final_e_coords_map_trained)
for token_id, e_final_coord_np in final_e_coords_map_trained.items():
    # Use the corresponding initial m_coord as a starting guess for its inverse
    # This assumes initial_m_coords_map_trained has the same keys
    initial_m_guess_np = initial_m_coords_map_trained.get(token_id, e_final_coord_np.copy()) # Fallback to e_coord if no initial m

    m_star_coord_np = phi_inverse_numerical_nn(e_final_coord_np, trained_phi_model, initial_m_guess_np)
    final_m_coords_on_manifold_map[token_id] = m_star_coord_np
    count += 1
    if count % (total_tokens // 10 or 1) == 0:
        print(f"Processed {count}/{total_tokens} tokens for inverse phi.")

print("Finished calculating final m_coords on manifold.")


# --- Step 3: Re-calculate and Analyze g_M at these "Final" Manifold Coordinates ---
import torch.autograd.functional as F # For Jacobian

eta_E_torch = torch.tensor(np.diag([-1.0, 1.0, 1.0, 1.0]), dtype=torch.float32)

print("\n--- Analyzing Pullback Metric g_M at Final Learned Manifold Coordinates ---")
if final_m_coords_on_manifold_map:
    points_to_analyze = tokens_lphi[:5] # Analyze for the first few tokens for brevity
    for token_id in points_to_analyze:
        if token_id in final_m_coords_on_manifold_map:
            m_final_coord_np = final_m_coords_on_manifold_map[token_id]
            m_final_coord_torch = torch.tensor(m_final_coord_np, dtype=torch.float32, requires_grad=False)

            # Ensure phi_model is callable with a single tensor input for Jacobian
            def phi_for_jacobian_final(m_input_tensor):
                return trained_phi_model(m_input_tensor.unsqueeze(0)).squeeze(0)

            try:
                J_phi_at_m_star = F.jacobian(phi_for_jacobian_final, m_final_coord_torch)
                g_M_at_m_star = J_phi_at_m_star.T @ eta_E_torch @ J_phi_at_m_star

                g_M_at_m_star_np = g_M_at_m_star.detach().numpy()
                det_g_M_at_m_star = np.linalg.det(g_M_at_m_star_np)

                print(f"\n--- g_M at '{token_id}' (final m_coords: {m_final_coord_np.round(3)}) ---")
                with np.printoptions(precision=3, suppress=True):
                    print(g_M_at_m_star_np)
                print(f"Determinant of g_M: {det_g_M_at_m_star:.3e}")
            except Exception as e_jac:
                print(f"Error calculating Jacobian or g_M for '{token_id}': {e_jac}")
        else:
            print(f"Token '{token_id}' not found in final_m_coords_on_manifold_map.")
else:
    print("final_m_coords_on_manifold_map is empty. Cannot analyze pullback metrics.")

print("Finished analyzing g_M at final learned manifold coordinates.")

# --- Step 4: Visualize final_m_coords_on_manifold_map ---
# (Ensure plot_m_embedding and hierarchy_pairs_lphi are defined from previous cells)
if final_m_coords_on_manifold_map:
    plot_m_embedding(final_m_coords_on_manifold_map, "Learned Conceptual Landscape (Final m_coords)")
else:
    print("Cannot plot final_m_coords_on_manifold_map as it is empty.")

print("\n--- Further Interpretation Notes ---")
print("Compare the g_M values and determinants at these 'final' m_coords to those at the initial random m_coords.")
print("Ideally, the determinants at the final m_coords should be closer to -1 (or a consistent non-zero value),")
print("indicating that phi has learned a less degenerate mapping in the regions relevant to your hierarchy.")
print("The components of g_M now describe the local geometry (time flow rate, spatial stretching/compression, axis tilting)")
print("of your *learned* conceptual manifold at the positions where your concepts are actually embedded.")

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from collections import defaultdict # If needed

# --- Assume previous helper function definitions if needed ---
print("--- Phase 3 (Revised): Learning Diffeomorphism phi with an INN ---")

# --- 1. Invertible Neural Network (INN) Definition for phi ---
# Using a stack of Additive Coupling Layers (RealNVP-style)

class AdditiveCouplingLayer(nn.Module):
    def __init__(self, input_dim, hidden_dim=128, mask_type='even'):
        super().__init__()
        assert input_dim % 2 == 0, "Input dimension must be even for splitting."
        self.input_dim_half = input_dim // 2

        # Determine which part is masked (unchanged) and which is transformed
        if mask_type == 'even': # x0 is unchanged, x1 is transformed
            self.mask_indices = torch.arange(0, self.input_dim_half)
            self.transform_indices = torch.arange(self.input_dim_half, input_dim)
        elif mask_type == 'odd': # x1 is unchanged, x0 is transformed
            self.transform_indices = torch.arange(0, self.input_dim_half)
            self.mask_indices = torch.arange(self.input_dim_half, input_dim)
        else:
            raise ValueError("mask_type must be 'even' or 'odd'")

        # Neural network for the translation (t in RealNVP, or s_net in your AdditiveCoupling)
        # It takes the masked part as input and outputs the translation for the transformed part.
        self.translation_net = nn.Sequential(
            nn.Linear(self.input_dim_half, hidden_dim),
            nn.Tanh(), # Using Tanh, can be ReLU or other
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, self.input_dim_half) # Outputs translation of same dim as transformed part
        )

    def forward(self, x, reverse=False):
        # x is expected to be (batch_size, input_dim)
        masked_part = x[:, self.mask_indices]
        transform_part = x[:, self.transform_indices]

        translation = self.translation_net(masked_part)

        if not reverse: # Forward pass: y_transform = x_transform + t(x_masked)
            y_transform = transform_part + translation
        else: # Inverse pass: x_transform = y_transform - t(x_masked)
            y_transform = transform_part - translation

        # Reconstruct the output tensor
        y = torch.zeros_like(x)
        y[:, self.mask_indices] = masked_part
        y[:, self.transform_indices] = y_transform
        return y

    def log_det_jacobian(self, x):
        # For additive coupling, the Jacobian is triangular with 1s on the diagonal.
        # So, log(det(J)) = log(1) = 0.
        # If we had scaling: y1 = s(x0)*x1 + t(x0), then log_det_J = sum(log|s(x0)|)
        return torch.zeros(x.shape[0], device=x.device)


class INNPhi(nn.Module):
    def __init__(self, input_dim=4, hidden_dim=128, num_coupling_layers=4):
        super().__init__()
        assert input_dim % 2 == 0, "INNPhi requires even input_dim for current AdditiveCouplingLayer design"
        self.input_dim = input_dim

        layers = []
        for i in range(num_coupling_layers):
            mask_type = 'even' if i % 2 == 0 else 'odd'
            layers.append(AdditiveCouplingLayer(input_dim, hidden_dim, mask_type))
        self.coupling_layers = nn.ModuleList(layers)

    def forward(self, m_coords_batch, reverse=False):
        # m_coords_batch is (batch_size, input_dim)
        z = m_coords_batch
        if not reverse:
            for layer in self.coupling_layers:
                z = layer(z, reverse=False)
        else: # Inverse pass
            for layer in reversed(self.coupling_layers):
                z = layer(z, reverse=True)
        return z

    def log_det_jacobian(self, m_coords_batch):
        # For a stack of coupling layers, log_det_J_phi = sum(log_det_J_layer_i)
        # For purely additive coupling, this will be 0.
        log_det_J = torch.zeros(m_coords_batch.shape[0], device=m_coords_batch.device)
        # If scaling was used in AdditiveCouplingLayer, you'd sum their log_det_jacobians here.
        # Example if scaling were present:
        # x_intermediate = m_coords_batch
        # for layer in self.coupling_layers:
        #     log_det_J += layer.log_det_jacobian(x_intermediate)
        #     x_intermediate = layer(x_intermediate, reverse=False) # Pass through layer
        return log_det_J


# --- 2. ConceptualEvent Class using INN Phi ---
class ConceptualEventINNPhi:
    def __init__(self, token_id, initial_m_coords_np, inn_phi_model):
        self.token_id = token_id
        self.initial_m_coords_tensor = torch.tensor(initial_m_coords_np, dtype=torch.float32)
        self.inn_phi = inn_phi_model

    def get_e_coords_tensor(self):
        # m_coords are fixed inputs, e_coords are outputs of inn_phi
        return self.inn_phi(self.initial_m_coords_tensor.unsqueeze(0), reverse=False).squeeze(0)

    def get_m_coords_from_e_tensor(self, e_coords_tensor):
        # Use the direct inverse of inn_phi
        return self.inn_phi(e_coords_tensor.unsqueeze(0), reverse=True).squeeze(0)

    @property
    def m_coords(self): # The initial m_coords (input to phi)
        return self.initial_m_coords_tensor.detach().numpy()

    @property
    def e_coords(self): # The current e_coords (output of phi)
        return self.get_e_coords_tensor().detach().numpy()

    def minkowski_interval_sq_in_E(self, other_event_e_coords_tensor):
        delta_e_coords = self.get_e_coords_tensor() - other_event_e_coords_tensor
        delta_e_t = delta_e_coords[0]
        delta_e_spatial_sq = torch.sum(delta_e_coords[1:]**2)
        return -delta_e_t**2 + delta_e_spatial_sq


# --- 3. Training Loop for INN Phi ---
def train_inn_phi_embedding(
    tokens, hierarchy_pairs,
    inn_phi_model, # The INN model for phi
    num_epochs=100, learning_rate=1e-3,
    epsilon1_target_sq=1e-5,
    causal_margin = 0.01,
    log_det_jacobian_weight = 0.0 # Weight for Jacobian determinant regularization (0 for additive)
):
    initial_m_coords_map = {
        token_id: np.array([0.0] + np.random.uniform(-1, 1, inn_phi_model.input_dim - 1).tolist(), dtype=np.float32)
        for token_id in tokens
    }
    event_objects = {
        token_id: ConceptualEventINNPhi(token_id, initial_m_coords_map[token_id], inn_phi_model)
        for token_id in tokens
    }
    optimizer = optim.Adam(inn_phi_model.parameters(), lr=learning_rate)

    training_pairs = []
    for child_id, parent_id in hierarchy_pairs:
        if parent_id is not None and child_id in event_objects and parent_id in event_objects:
            training_pairs.append((event_objects[child_id], event_objects[parent_id]))

    if not training_pairs:
        print("No valid training pairs. Aborting.")
        return initial_m_coords_map, {name: obj.e_coords for name, obj in event_objects.items()}, inn_phi_model

    print(f"Starting training of INN_phi for {num_epochs} epochs...")
    for epoch in range(num_epochs):
        inn_phi_model.train()
        total_loss = 0.0

        for child_event, parent_event in training_pairs:
            optimizer.zero_grad()

            e_coords_child = child_event.get_e_coords_tensor()
            e_coords_parent = parent_event.get_e_coords_tensor()

            delta_e_t = e_coords_child[0] - e_coords_parent[0]
            loss_causal_order = torch.relu(-(delta_e_t - causal_margin))

            delta_e_spatial_sq = torch.sum((e_coords_child[1:] - e_coords_parent[1:])**2)
            ds_sq_E = -delta_e_t**2 + delta_e_spatial_sq
            loss_interval = (ds_sq_E + epsilon1_target_sq)**2

            # Jacobian determinant regularization (optional, and 0 for purely additive)
            # We need to pass the m_coords that produced these e_coords to log_det_jacobian
            # For simplicity, let's compute for child and parent m_coords.
            # This adds computational cost.
            loss_log_det_J = 0.0
            if log_det_jacobian_weight > 0:
                 # This would be more meaningful if AdditiveCouplingLayer also had scaling
                 log_det_J_child = inn_phi_model.log_det_jacobian(child_event.initial_m_coords_tensor.unsqueeze(0))
                 log_det_J_parent = inn_phi_model.log_det_jacobian(parent_event.initial_m_coords_tensor.unsqueeze(0))
                 # We want log|det(J)| to not be too negative (i.e., det(J) not too small)
                 # So, penalize -log|det(J)| or (log|det(J)| - target_log_det)^2
                 # For additive, log_det_J is 0. If we had scaling, we might want it near 0.
                 loss_log_det_J = (log_det_J_child**2 + log_det_J_parent**2).mean()


            loss = loss_causal_order + loss_interval + log_det_jacobian_weight * loss_log_det_J

            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        if epoch % (num_epochs // 10 or 1) == 0:
            print(f"Epoch {epoch+1}/{num_epochs}, Avg Loss: {total_loss / len(training_pairs):.6f}")

    print("Training finished.")
    inn_phi_model.eval()

    final_e_coords_map = {name: obj.e_coords for name, obj in event_objects.items()}
    # The "final" m_coords on the manifold are those that map to final_e_coords
    # Since m_coords were fixed inputs, for analysis of g_M, we use these fixed initial_m_coords.
    # If we wanted "learned" m_coords, we'd invert phi_model(final_e_coords).
    return initial_m_coords_map, final_e_coords_map, inn_phi_model

# --- Example Usage ---
tokens_inn = ['universe', 'galaxy', 'star_system', 'star', 'planet', 'sun', 'earth', 'milky_way', 'solar_system']
hierarchy_pairs_inn = [
    ('galaxy', 'universe'), ('star_system', 'galaxy'), ('star', 'star_system'),
    ('planet', 'star_system'), ('sun', 'star'), ('earth', 'planet'),
    ('milky_way', 'galaxy'), ('solar_system', 'star_system'),
    ('solar_system', 'milky_way'), ('sun', 'solar_system'), ('earth', 'solar_system')
]

# Instantiate the INN phi model
# Input dimension must be even. Our coords are (t,x,y,z) -> dim 4.
inn_phi_network = INNPhi(input_dim=4, hidden_dim=64, num_coupling_layers=6) # More layers for complexity

print("\n--- Training INN Phi Network ---")
initial_m_coords_inn, final_e_coords_inn, trained_inn_phi = train_inn_phi_embedding(
    tokens_inn, hierarchy_pairs_inn,
    inn_phi_network,
    num_epochs=300, # Adjust as needed
    learning_rate=0.001,
    epsilon1_target_sq=1e-4,
    causal_margin=0.05,
    log_det_jacobian_weight=0.0 # Keep at 0 for purely additive coupling
)

# --- 4. Visualization & Pullback Metric Analysis ---
# Plot the e_coords (where embedding happened)
plot_coords_map(final_e_coords_inn, "Embedding Space Coords (e_coords) via INN Phi")


# Plot the initial m_coords (which were inputs to INN phi)
# These are NOT the "final" manifold coordinates unless phi is identity.
plot_coords_map(initial_m_coords_inn, "Initial Conceptual Manifold Coords (m_coords - Inputs to INN Phi)")


# Calculate and analyze pullback metric g_M using the *initial_m_coords*
# as these are the points on M that phi maps *from*.
# For the "true" learned manifold points, we would invert phi(final_e_coords).

eta_E_torch_inn = torch.tensor(np.diag([-1.0, 1.0, 1.0, 1.0]), dtype=torch.float32)
print("\n--- Analyzing Pullback Metric g_M at Initial m_coords (using INN Phi) ---")
if initial_m_coords_inn:
    points_to_analyze_inn = tokens_inn[:3]
    for token_id in points_to_analyze_inn:
        if token_id in initial_m_coords_inn:
            m_coord_np = initial_m_coords_inn[token_id]
            m_coord_torch = torch.tensor(m_coord_np, dtype=torch.float32)

            def phi_for_jacobian_inn(m_input_tensor):
                return trained_inn_phi(m_input_tensor.unsqueeze(0), reverse=False).squeeze(0)

            try:
                J_phi_at_m = torch.autograd.functional.jacobian(phi_for_jacobian_inn, m_coord_torch)
                g_M_at_m = J_phi_at_m.T @ eta_E_torch_inn @ J_phi_at_m

                g_M_at_m_np = g_M_at_m.detach().numpy()
                det_g_M_at_m = np.linalg.det(g_M_at_m_np)

                print(f"\n--- g_M at '{token_id}' (initial m_coords: {m_coord_np.round(3)}) ---")
                with np.printoptions(precision=3, suppress=True):
                    print(g_M_at_m_np)
                print(f"Determinant of g_M: {det_g_M_at_m:.3e}")
                print(f"Log Det Jacobian from INN (should be ~0 for additive): {trained_inn_phi.log_det_jacobian(m_coord_torch.unsqueeze(0)).item():.3e}")
            except Exception as e_jac_inn:
                print(f"Error calculating Jacobian or g_M for '{token_id}' with INN: {e_jac_inn}")
else:
    print("initial_m_coords_inn is empty. Cannot analyze pullback metrics.")

print("\n--- Interpretation for INN Phi ---")
print("With an INN (especially purely additive coupling), the log_det_jacobian of phi is often 0 (or constant if scaling is identity).")
print("This means det(J_phi) = 1 (or constant).")
print("Therefore, det(g_M) = det(J_phi^T * eta_E * J_phi) = det(J_phi^T) * det(eta_E) * det(J_phi) = det(J_phi)^2 * det(eta_E).")
print("If det(J_phi) = 1, then det(g_M) = det(eta_E) = -1. This means a purely additive INN phi is volume-preserving.")
print("The off-diagonal terms in g_M will indicate how phi shears or mixes the conceptual coordinates.")
print("The diagonal terms will show scaling. If g_M is not eta_E, then phi has introduced non-trivial geometry even if volume is preserved.")
print("To get the 'true' manifold structure, we should analyze g_M at m* = phi_inverse(e_final). For INNs, phi_inverse is exact.")

# To plot the "true" learned manifold positions:
final_m_coords_from_inn_inverse = {}
if final_e_coords_inn:
    print("\nCalculating final m_coords on manifold using INN inverse...")
    for token_id, e_final_np in final_e_coords_inn.items():
        e_final_tensor = torch.tensor(e_final_np, dtype=torch.float32)
        with torch.no_grad():
            m_star_tensor = trained_inn_phi(e_final_tensor.unsqueeze(0), reverse=True).squeeze(0)
        final_m_coords_from_inn_inverse[token_id] = m_star_tensor.numpy()
    plot_coords_map(final_m_coords_from_inn_inverse, "Learned Conceptual Manifold (m* = phi_inv(e_final)) via INN")

    print("\n--- Analyzing Pullback Metric g_M at Final INVERTED m_coords (using INN Phi) ---")
    for token_id in points_to_analyze_inn: # Analyze for the same first few tokens
        if token_id in final_m_coords_from_inn_inverse:
            m_star_coord_np = final_m_coords_from_inn_inverse[token_id]
            m_star_coord_torch = torch.tensor(m_star_coord_np, dtype=torch.float32)

            def phi_for_jacobian_inn_final(m_input_tensor): # Ensure fresh definition for clarity
                return trained_inn_phi(m_input_tensor.unsqueeze(0), reverse=False).squeeze(0)

            try:
                J_phi_at_m_star_final = torch.autograd.functional.jacobian(phi_for_jacobian_inn_final, m_star_coord_torch)
                g_M_at_m_star_final = J_phi_at_m_star_final.T @ eta_E_torch_inn @ J_phi_at_m_star_final

                g_M_at_m_star_final_np = g_M_at_m_star_final.detach().numpy()
                det_g_M_at_m_star_final = np.linalg.det(g_M_at_m_star_final_np)

                print(f"\n--- g_M at '{token_id}' (final m* coords: {m_star_coord_np.round(3)}) ---")
                with np.printoptions(precision=3, suppress=True):
                    print(g_M_at_m_star_final_np)
                print(f"Determinant of g_M: {det_g_M_at_m_star_final:.3e}")
            except Exception as e_jac_inn_final:
                print(f"Error calculating Jacobian or g_M for '{token_id}' (final m*): {e_jac_inn_final}")
else:
    print("final_e_coords_inn is empty. Cannot calculate final m_coords or their g_M.")

import numpy as np
import torch
import torch.autograd.functional as F # For Jacobian

# --- Ensure trained_inn_phi and final_m_coords_on_manifold_map are available ---
# For example, if they were named differently in your training output:
# trained_inn_phi = trained_inn_phi # Your trained INN model
# final_m_coords_map_for_analysis = final_m_coords_from_inn_inverse # Your final m* coords

# If not, ensure they are loaded or replace with your variable names.
# For demonstration, I'll assume they are named as in the previous cell output discussion.
if 'trained_inn_phi' not in locals() or 'final_m_coords_on_manifold_map' not in locals():
    print("Error: `trained_inn_phi` or `final_m_coords_on_manifold_map` not found.")
    print("Please ensure these variables are defined from the INN training and inversion steps.")
    # You might want to stop execution here or load them if this is a standalone analysis script.
    # For now, we'll create dummy ones if they don't exist to allow the code to run.
    class DummyPhi(torch.nn.Module):
        def __init__(self): super().__init__(); self.fc = torch.nn.Linear(4,4)
        def forward(self, x, reverse=False): return self.fc(x) if not reverse else x # very basic
    if 'trained_inn_phi' not in locals():
        trained_inn_phi = DummyPhi()
        print("Using a dummy trained_inn_phi for demonstration.")
    if 'final_m_coords_on_manifold_map' not in locals():
        final_m_coords_on_manifold_map = {'universe': np.array([0.,0.1,0.1,0.1]), 'galaxy': np.array([0.1,0.2,0.2,0.2])}
        print("Using dummy final_m_coords_on_manifold_map for demonstration.")
        tokens_inn = list(final_m_coords_on_manifold_map.keys()) # ensure tokens_inn matches

trained_inn_phi.eval() # Ensure model is in evaluation mode

# Minkowski metric in E-space (eta_E)
eta_E_torch = torch.tensor(np.diag([-1.0, 1.0, 1.0, 1.0]), dtype=torch.float32)

# --- 1. Function to Calculate Pullback Metric and Features ---
def analyze_local_geometry(m_coord_np, phi_network_model, eta_E):
    """
    Calculates the pullback metric g_M and extracts geometric features
    at a given m_coord on the conceptual manifold.
    """
    m_coord_torch = torch.tensor(m_coord_np, dtype=torch.float32)

    # Wrapper for Jacobian calculation
    def phi_for_jacobian(m_input_tensor):
        return phi_network_model(m_input_tensor.unsqueeze(0), reverse=False).squeeze(0)

    J_phi = F.jacobian(phi_for_jacobian, m_coord_torch)
    g_M_tensor = J_phi.T @ eta_E @ J_phi

    g_M_np = g_M_tensor.detach().numpy()
    det_g_M = np.linalg.det(g_M_np)

    # Extract features
    g_tt = g_M_np[0, 0]

    # Time Distortion Factor: sqrt(|g_tt|)
    # If g_tt is positive (m_t behaving spatially), this interpretation needs care.
    # We'll report g_tt directly and sqrt(|g_tt|) for magnitude.
    time_distortion_factor_val = np.sqrt(np.abs(g_tt))

    # Average Spatial Scaling: average of g_xx, g_yy, g_zz
    avg_spatial_scaling = np.mean(np.diag(g_M_np[1:, 1:]))

    # Spatial Volume Element Factor (approx for diagonal dominant)
    # More accurately from det of 3x3 spatial submatrix
    spatial_submatrix = g_M_np[1:, 1:]
    det_g_spatial = np.linalg.det(spatial_submatrix)
    spatial_volume_factor = np.sqrt(np.abs(det_g_spatial)) if det_g_spatial > 0 else np.nan # Proper if spatial part is Riemannian

    # Time-Space Mixing: sum of absolute values of g_0k (k=1,2,3)
    time_space_mixing = np.sum(np.abs(g_M_np[0, 1:])) + np.sum(np.abs(g_M_np[1:, 0])) # Symmetric
    time_space_mixing /= 2 # Avoid double counting

    return {
        "g_M": g_M_np,
        "det_g_M": det_g_M,
        "g_tt": g_tt,
        "time_distortion_sqrt_abs_g_tt": time_distortion_factor_val,
        "avg_spatial_diagonal_scaling": avg_spatial_scaling,
        "spatial_volume_sqrt_det_g_spatial": spatial_volume_factor,
        "time_space_mixing_abs_sum_g0k": time_space_mixing
    }

# --- 2. Analyze and Print Geometric Features for Key Concepts ---
print("\n--- Geometric Feature Analysis at Final Learned Manifold Coordinates (m*) ---")

geometric_features_map = {}

if final_m_coords_on_manifold_map:
    # Analyze for all tokens or a subset
    tokens_to_analyze = tokens_inn # Or choose specific ones like ['universe', 'earth', 'sun']

    for token_id in tokens_to_analyze:
        if token_id in final_m_coords_on_manifold_map:
            m_star_coord_np = final_m_coords_on_manifold_map[token_id]

            try:
                features = analyze_local_geometry(m_star_coord_np, trained_inn_phi, eta_E_torch)
                geometric_features_map[token_id] = features

                print(f"\n--- Analysis for '{token_id}' (m*: {m_star_coord_np.round(3)}) ---")
                with np.printoptions(precision=3, suppress=True):
                    print(f"  Pullback Metric g_M:\n{features['g_M']}")
                print(f"  Determinant of g_M: {features['det_g_M']:.3e}")
                print(f"  g_tt (Time Component): {features['g_tt']:.3f}")
                print(f"  sqrt(|g_tt|) (Time Distortion Factor): {features['time_distortion_sqrt_abs_g_tt']:.3f}")
                print(f"  Avg Spatial Diagonal Scaling (g_ii): {features['avg_spatial_diagonal_scaling']:.3f}")
                print(f"  sqrt(det(g_spatial)) (Spatial Volume Factor): {features['spatial_volume_sqrt_det_g_spatial']:.3f}")
                print(f"  Time-Space Mixing (sum |g_0k|): {features['time_space_mixing_abs_sum_g0k']:.3f}")

            except RuntimeError as e_jac: # Catch potential issues with Jacobian calculation (e.g., non-differentiable points if phi is complex)
                 print(f"Could not analyze geometry for '{token_id}' at m*={m_star_coord_np.round(3)}. Error: {e_jac}")
            except Exception as e_other:
                 print(f"An unexpected error occurred for '{token_id}': {e_other}")
        else:
            print(f"Token '{token_id}' not found in final_m_coords_on_manifold_map.")
else:
    print("final_m_coords_on_manifold_map is empty. Cannot perform geometric analysis.")

print("\nFinished geometric feature analysis.")

# --- 3. Storing and Correlating Features (Conceptual Next Steps) ---
# The `geometric_features_map` now contains detailed local geometry for each concept.
# You can convert this to a Pandas DataFrame for easier analysis:
#
# import pandas as pd
# if geometric_features_map:
#     df_geo_features = pd.DataFrame.from_dict(geometric_features_map, orient='index')
#     print("\nGeometric Features DataFrame (first 5 rows):")
#     print(df_geo_features.head())
#
#     # Now, you would add columns to this DataFrame for your conceptual properties:
#     # - Hierarchical depth
#     # - Number of children/parents (degree)
#     # - Centrality measures from the graph
#     # - Any other semantic or categorical labels for your concepts
#
#     # Then perform correlations or group comparisons:
#     # e.g., df_geo_features.corr()
#     # e.g., df_geo_features.groupby('concept_category')['g_tt'].mean()
# else:
#     print("No geometric features to convert to DataFrame.")

print("\n--- Interpretation Guidance based on Potential Outputs ---")
print("Look for patterns in the `geometric_features_map`:")
print(" - If g_tt is consistently positive and large (as in your previous INN output example):")
print("   This indicates your learned `phi` is strongly mapping the input `m_t` to influence spatial components in `E` OR")
print("   the spatial `m_s` inputs strongly influence `e_t`. The 'time' dimension on M behaves spatially.")
print("   This could mean your hierarchy is primarily spatial in its learned representation, or the loss function")
print("   forced `phi` into this configuration to satisfy constraints. You may need to regularize g_tt towards -1.")
print(" - If g_tt is negative but |g_tt| != 1:")
print("   |g_tt| < 1: Proper time on M flows 'faster' than coordinate m_t.")
print("   |g_tt| > 1: Proper time on M flows 'slower' than coordinate m_t (conceptual time dilation).")
print(" - If avg_spatial_diagonal_scaling (or sqrt(det(g_spatial))) != 1:")
print("   > 1: Conceptual space is locally 'stretched' on M.")
print("   < 1: Conceptual space is locally 'compressed' on M.")
print(" - Non-zero time_space_mixing_abs_sum_g0k:")
print("   Indicates that the conceptual time axis and spatial axes are not orthogonal on M.")
print("   'Movement' in conceptual space affects the 'flow' of conceptual time, and vice-versa.")
print(" - Determinant of g_M close to -1 (for INN with additive coupling):")
print("   Confirms volume preservation. If it deviates significantly with other INN types, it indicates volume distortion.")
print("\nThese quantitative features are the starting point for linking the *learned geometry* to the *semantics* of your hierarchy.")

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.autograd.functional as F # For Jacobian
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from collections import defaultdict

# --- Assume AdditiveCouplingLayer, INNPhi, ConceptualEventINNPhi are defined from previous cells ---
# --- Also, plot_coords_map and other helpers if used for final plotting ---

print("--- Phase 3 (Refined): Learning Diffeomorphism phi with Metric Regularization ---")

# Minkowski metric in E-space (eta_E), will be used in loss
eta_E_torch_for_loss = torch.tensor(np.diag([-1.0, 1.0, 1.0, 1.0]), dtype=torch.float32)
# Ensure it's on the same device as the model if using GPU
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# eta_E_torch_for_loss = eta_E_torch_for_loss.to(device)


def train_inn_phi_embedding_with_metric_reg(
    tokens, hierarchy_pairs,
    inn_phi_model, # The INN model for phi
    num_epochs=100, learning_rate=1e-3,
    epsilon1_target_sq=1e-5,
    causal_margin = 0.01,
    g_tt_target_value = -1.0,
    g_tt_reg_weight = 0.1, # Start with a small weight
    g_spatial_diag_target_value = 1.0,
    g_spatial_diag_reg_weight = 0.1, # Start with a small weight
    g_off_diag_target_value = 0.0,
    g_off_diag_reg_weight = 0.01 # Start with a smaller weight for off-diagonals
):
    # Move model to device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inn_phi_model.to(device)
    global eta_E_torch_for_loss # Use global or pass as arg
    eta_E_torch_for_loss = eta_E_torch_for_loss.to(device)


    initial_m_coords_map = {
        token_id: np.array([0.0] + np.random.uniform(-1, 1, inn_phi_model.input_dim - 1).tolist(), dtype=np.float32)
        for token_id in tokens
    }

    # Store initial_m_coords as tensors on the correct device directly
    initial_m_coords_tensors_map = {
        name: torch.tensor(coords, dtype=torch.float32, device=device)
        for name, coords in initial_m_coords_map.items()
    }

    optimizer = optim.Adam(inn_phi_model.parameters(), lr=learning_rate)
    mse_loss_fn = nn.MSELoss() # For metric component regularization

    training_pairs_ids = []
    for child_id, parent_id in hierarchy_pairs:
        if parent_id is not None and child_id in initial_m_coords_tensors_map and parent_id in initial_m_coords_tensors_map:
            training_pairs_ids.append((child_id, parent_id))

    if not training_pairs_ids:
        print("No valid training pairs. Aborting.")
        # For returning, get e_coords for the initial m_coords
        final_e_coords_map_on_abort = {}
        with torch.no_grad():
            for name, m_tensor in initial_m_coords_tensors_map.items():
                 final_e_coords_map_on_abort[name] = inn_phi_model(m_tensor.unsqueeze(0)).squeeze(0).cpu().numpy()
        return initial_m_coords_map, final_e_coords_map_on_abort, inn_phi_model

    print(f"Starting training of INN_phi with metric regularization for {num_epochs} epochs...")
    for epoch in range(num_epochs):
        inn_phi_model.train()
        total_loss = 0.0
        total_causal_loss = 0.0
        total_interval_loss = 0.0
        total_g_tt_loss = 0.0
        total_g_spatial_loss = 0.0
        total_g_off_diag_loss = 0.0

        for child_id, parent_id in training_pairs_ids:
            optimizer.zero_grad()

            m_coords_child_tensor = initial_m_coords_tensors_map[child_id]
            m_coords_parent_tensor = initial_m_coords_tensors_map[parent_id]

            e_coords_child = inn_phi_model(m_coords_child_tensor.unsqueeze(0)).squeeze(0)
            e_coords_parent = inn_phi_model(m_coords_parent_tensor.unsqueeze(0)).squeeze(0)

            # --- Causal and Interval Loss (in E-space) ---
            delta_e_t = e_coords_child[0] - e_coords_parent[0]
            loss_causal_order = torch.relu(-(delta_e_t - causal_margin))

            delta_e_spatial_sq = torch.sum((e_coords_child[1:] - e_coords_parent[1:])**2)
            ds_sq_E = -delta_e_t**2 + delta_e_spatial_sq
            loss_interval = (ds_sq_E + epsilon1_target_sq)**2

            # --- Metric Regularization Loss (on M-space, via pullback) ---
            loss_metric_reg = torch.tensor(0.0, device=device)

            # Jacobian and g_M for child
            J_phi_child = F.jacobian(lambda x: inn_phi_model(x.unsqueeze(0)).squeeze(0), m_coords_child_tensor)
            g_M_child = J_phi_child.T @ eta_E_torch_for_loss @ J_phi_child

            # Jacobian and g_M for parent
            J_phi_parent = F.jacobian(lambda x: inn_phi_model(x.unsqueeze(0)).squeeze(0), m_coords_parent_tensor)
            g_M_parent = J_phi_parent.T @ eta_E_torch_for_loss @ J_phi_parent

            # g_tt regularization
            if g_tt_reg_weight > 0:
                loss_g_tt_c = mse_loss_fn(g_M_child[0,0], torch.tensor(g_tt_target_value, device=device))
                loss_g_tt_p = mse_loss_fn(g_M_parent[0,0], torch.tensor(g_tt_target_value, device=device))
                loss_metric_reg += g_tt_reg_weight * (loss_g_tt_c + loss_g_tt_p)
                total_g_tt_loss += (loss_g_tt_c + loss_g_tt_p).item()


            # Spatial diagonal regularization
            if g_spatial_diag_reg_weight > 0:
                target_spatial_diagonals = torch.tensor([g_spatial_diag_target_value]* (inn_phi_model.input_dim -1) , device=device)
                loss_g_sp_diag_c = mse_loss_fn(torch.diag(g_M_child[1:,1:]), target_spatial_diagonals)
                loss_g_sp_diag_p = mse_loss_fn(torch.diag(g_M_parent[1:,1:]), target_spatial_diagonals)
                loss_metric_reg += g_spatial_diag_reg_weight * (loss_g_sp_diag_c + loss_g_sp_diag_p)
                total_g_spatial_loss += (loss_g_sp_diag_c + loss_g_sp_diag_p).item()

            # Off-diagonal regularization (encourage orthogonality)
            if g_off_diag_reg_weight > 0:
                # For g_M_child
                off_diag_child_sum_sq = torch.sum(g_M_child[0,1:]**2) + torch.sum(g_M_child[1:,0]**2) + \
                                        torch.sum(torch.triu(g_M_child[1:,1:], diagonal=1)**2) + \
                                        torch.sum(torch.tril(g_M_child[1:,1:], diagonal=-1)**2)
                # For g_M_parent
                off_diag_parent_sum_sq = torch.sum(g_M_parent[0,1:]**2) + torch.sum(g_M_parent[1:,0]**2) + \
                                         torch.sum(torch.triu(g_M_parent[1:,1:], diagonal=1)**2) + \
                                         torch.sum(torch.tril(g_M_parent[1:,1:], diagonal=-1)**2)
                loss_g_off_diag = mse_loss_fn(off_diag_child_sum_sq + off_diag_parent_sum_sq, torch.tensor(0.0, device=device)) # Target is 0
                loss_metric_reg += g_off_diag_reg_weight * loss_g_off_diag
                total_g_off_diag_loss += loss_g_off_diag.item()

            loss = loss_causal_order + loss_interval + loss_metric_reg

            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            total_causal_loss += loss_causal_order.item()
            total_interval_loss += loss_interval.item()

        avg_loss = total_loss / len(training_pairs_ids)
        avg_causal = total_causal_loss / len(training_pairs_ids)
        avg_interval = total_interval_loss / len(training_pairs_ids)
        avg_g_tt = total_g_tt_loss / (len(training_pairs_ids) * 2) if g_tt_reg_weight > 0 else 0
        avg_g_spatial = total_g_spatial_loss / (len(training_pairs_ids) * 2) if g_spatial_diag_reg_weight > 0 else 0
        avg_g_off_diag = total_g_off_diag_loss / len(training_pairs_ids) if g_off_diag_reg_weight > 0 else 0


        if epoch % (num_epochs // 10 or 1) == 0:
            print(f"Epoch {epoch+1}/{num_epochs}, Avg Total Loss: {avg_loss:.4f} "
                  f"(Causal: {avg_causal:.4f}, Interval: {avg_interval:.4f}, "
                  f"g_tt_reg: {avg_g_tt:.4f}, g_sp_reg: {avg_g_spatial:.4f}, g_off_diag_reg: {avg_g_off_diag:.4f})")

    print("Training finished.")
    inn_phi_model.eval()

    final_e_coords_map = {}
    with torch.no_grad():
        for name, m_tensor in initial_m_coords_tensors_map.items():
             final_e_coords_map[name] = inn_phi_model(m_tensor.unsqueeze(0)).squeeze(0).cpu().numpy()

    return initial_m_coords_map, final_e_coords_map, inn_phi_model

# --- Example Usage with Metric Regularization ---
tokens_inn_reg = ['universe', 'galaxy', 'star_system', 'star', 'planet', 'sun', 'earth', 'milky_way', 'solar_system']
hierarchy_pairs_inn_reg = [
    ('galaxy', 'universe'), ('star_system', 'galaxy'), ('star', 'star_system'),
    ('planet', 'star_system'), ('sun', 'star'), ('earth', 'planet'),
    ('milky_way', 'galaxy'), ('solar_system', 'star_system'),
    ('solar_system', 'milky_way'), ('sun', 'solar_system'), ('earth', 'solar_system')
]

# Instantiate the INN phi model
inn_phi_network_reg = INNPhi(input_dim=4, hidden_dim=64, num_coupling_layers=6)

print("\n--- Training INN Phi Network with Metric Regularization ---")
initial_m_coords_reg, final_e_coords_reg, trained_inn_phi_reg = train_inn_phi_embedding_with_metric_reg(
    tokens_inn_reg, hierarchy_pairs_inn_reg,
    inn_phi_network_reg,
    num_epochs=1000, # Increase epochs for regularization to take effect
    learning_rate=0.0005, # Potentially smaller LR with more complex loss
    epsilon1_target_sq=1e-4,
    causal_margin=0.05,
    g_tt_target_value = -1.0,
    g_tt_reg_weight = 0.5, # Increase weight if g_tt is still far from target
    g_spatial_diag_target_value = 1.0,
    g_spatial_diag_reg_weight = 0.2,
    g_off_diag_target_value = 0.0,
    g_off_diag_reg_weight = 0.1
)

# --- Analysis of Pullback Metric at Final Inverted m_coords (as before) ---
final_m_coords_from_inn_inverse_reg = {}
if final_e_coords_reg:
    print("\nCalculating final m_coords on manifold using INN inverse (after reg training)...")
    for token_id, e_final_np in final_e_coords_reg.items():
        e_final_tensor = torch.tensor(e_final_np, dtype=torch.float32).to(next(trained_inn_phi_reg.parameters()).device) # Ensure on same device
        with torch.no_grad():
            m_star_tensor = trained_inn_phi_reg(e_final_tensor.unsqueeze(0), reverse=True).squeeze(0)
        final_m_coords_from_inn_inverse_reg[token_id] = m_star_tensor.cpu().numpy()

    # Plot the m* coordinates
    if 'plot_m_embedding' in locals() and 'hierarchy_pairs_inn_reg' in locals():
        plot_m_embedding(final_m_coords_from_inn_inverse_reg, "Learned Conceptual Manifold (m* = phi_inv(e_final)) with Metric Reg")
    else:
        print("plot_m_embedding or hierarchy_pairs_inn_reg not defined, skipping m* plot.")


    print("\n--- Analyzing Pullback Metric g_M at Final INVERTED m_coords (after reg training) ---")
    points_to_analyze_reg = tokens_inn_reg[:5]
    for token_id in points_to_analyze_reg:
        if token_id in final_m_coords_from_inn_inverse_reg:
            m_star_coord_np = final_m_coords_from_inn_inverse_reg[token_id]
            m_star_coord_torch = torch.tensor(m_star_coord_np, dtype=torch.float32).to(next(trained_inn_phi_reg.parameters()).device)

            def phi_for_jacobian_inn_final_reg(m_input_tensor):
                return trained_inn_phi_reg(m_input_tensor.unsqueeze(0), reverse=False).squeeze(0)
            try:
                J_phi_at_m_star_final = F.jacobian(phi_for_jacobian_inn_final_reg, m_star_coord_torch)
                # Ensure eta_E_torch_for_loss is on the same device as J_phi
                g_M_at_m_star_final = J_phi_at_m_star_final.T @ eta_E_torch_for_loss.to(J_phi_at_m_star_final.device) @ J_phi_at_m_star_final

                g_M_at_m_star_final_np = g_M_at_m_star_final.detach().cpu().numpy()
                det_g_M_at_m_star_final = np.linalg.det(g_M_at_m_star_final_np)

                print(f"\n--- g_M at '{token_id}' (final m* coords: {m_star_coord_np.round(3)}) ---")
                with np.printoptions(precision=3, suppress=True):
                    print(g_M_at_m_star_final_np)
                print(f"Determinant of g_M: {det_g_M_at_m_star_final:.3e}")
            except Exception as e_jac_inn_final:
                 print(f"Error calculating Jacobian or g_M for '{token_id}' (final m* with reg): {e_jac_inn_final}")

else:
    print("final_e_coords_reg is empty. Cannot calculate final m_coords or their g_M.")

print("\n--- Expected Outcome of Metric Regularization ---")
print("With these regularization terms, we expect:")
print(" - g_tt components of the pullback metric (at final m* coordinates) to be closer to -1.0.")
print(" - Spatial diagonal components (g_xx, g_yy, g_zz) to be closer to 1.0.")
print(" - Off-diagonal components to be closer to 0.0.")
print(" - The determinant of g_M should remain close to -1.0 due to the volume-preserving nature of additive INNs.")
print("The training loss components for these regularizations should decrease over epochs if effective.")

"""Fixing Loss

"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.autograd.functional as F # For Jacobian
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from collections import defaultdict

# --- Assume AdditiveCouplingLayer, INNPhi, ConceptualEventINNPhi are defined from previous cells ---
# --- Also, plot_coords_map and other helpers if used for final plotting ---

print("--- Phase 3 (Revised with Loss Adjustments): Learning Diffeomorphism phi ---")

eta_E_torch_for_loss = torch.tensor(np.diag([-1.0, 1.0, 1.0, 1.0]), dtype=torch.float32)

def train_inn_phi_embedding_with_metric_reg_v2( # Renamed for clarity
    tokens, hierarchy_pairs,
    inn_phi_model,
    num_epochs=500, # Keep epochs, but we'll watch for early stopping based on loss
    learning_rate=1e-4, # REDUCED LEARNING RATE
    epsilon1_target_sq=1e-5,
    causal_margin = 0.05,
    g_tt_target_value = -1.0,
    g_tt_reg_weight = 0.01,   # SIGNIFICANTLY REDUCED g_tt weight
    g_spatial_diag_target_value = 1.0,
    g_spatial_diag_reg_weight = 0.01, # SIGNIFICANTLY REDUCED spatial weight
    g_off_diag_target_value = 0.0,
    g_off_diag_reg_weight = 0.001, # SIGNIFICANTLY REDUCED off-diag weight
    gradient_clip_value = 1.0 # Added gradient clipping
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inn_phi_model.to(device)
    global eta_E_torch_for_loss
    eta_E_torch_for_loss = eta_E_torch_for_loss.to(device)

    initial_m_coords_map = {
        token_id: np.array([0.0] + np.random.uniform(-0.5, 0.5, inn_phi_model.input_dim - 1).tolist(), dtype=np.float32) # Smaller initial range
        for token_id in tokens
    }
    initial_m_coords_tensors_map = {
        name: torch.tensor(coords, dtype=torch.float32, device=device)
        for name, coords in initial_m_coords_map.items()
    }

    optimizer = optim.Adam(inn_phi_model.parameters(), lr=learning_rate)
    mse_loss_fn = nn.MSELoss()

    training_pairs_ids = []
    for child_id, parent_id in hierarchy_pairs:
        if parent_id is not None and child_id in initial_m_coords_tensors_map and parent_id in initial_m_coords_tensors_map:
            training_pairs_ids.append((child_id, parent_id))

    if not training_pairs_ids:
        print("No valid training pairs. Aborting.")
        final_e_coords_map_on_abort = {}
        with torch.no_grad():
            for name, m_tensor in initial_m_coords_tensors_map.items():
                 final_e_coords_map_on_abort[name] = inn_phi_model(m_tensor.unsqueeze(0)).squeeze(0).cpu().numpy()
        return initial_m_coords_map, final_e_coords_map_on_abort, inn_phi_model

    print(f"Starting training of INN_phi with MODIFIED metric regularization for {num_epochs} epochs...")
    print(f"LR={learning_rate}, g_tt_w={g_tt_reg_weight}, g_sp_w={g_spatial_diag_reg_weight}, g_off_w={g_off_diag_reg_weight}, clip={gradient_clip_value}")

    best_loss = float('inf')
    epochs_no_improve = 0
    patience = 200 # Stop if no improvement for 50 epochs

    for epoch in range(num_epochs):
        inn_phi_model.train()
        epoch_losses = {
            'total': [], 'causal': [], 'interval': [],
            'g_tt': [], 'g_spatial': [], 'g_off_diag': []
        }

        for child_id, parent_id in training_pairs_ids:
            optimizer.zero_grad()

            m_coords_child_tensor = initial_m_coords_tensors_map[child_id]
            m_coords_parent_tensor = initial_m_coords_tensors_map[parent_id]

            e_coords_child = inn_phi_model(m_coords_child_tensor.unsqueeze(0)).squeeze(0)
            e_coords_parent = inn_phi_model(m_coords_parent_tensor.unsqueeze(0)).squeeze(0)

            delta_e_t = e_coords_child[0] - e_coords_parent[0]
            loss_causal_order = torch.relu(-(delta_e_t - causal_margin))

            delta_e_spatial_sq = torch.sum((e_coords_child[1:] - e_coords_parent[1:])**2)
            ds_sq_E = -delta_e_t**2 + delta_e_spatial_sq
            loss_interval = (ds_sq_E + epsilon1_target_sq)**2

            loss_metric_reg = torch.tensor(0.0, device=device)
            current_g_tt_loss = torch.tensor(0.0, device=device)
            current_g_spatial_loss = torch.tensor(0.0, device=device)
            current_g_off_diag_loss = torch.tensor(0.0, device=device)

            # Calculate Jacobians and g_M for regularization
            # To reduce computation, consider doing this only for a subset or less frequently
            # For now, doing it for both parent and child in the pair
            points_for_reg = [m_coords_child_tensor, m_coords_parent_tensor]
            g_Ms_for_reg = []

            for m_point_tensor in points_for_reg:
                # Detach m_point_tensor if it comes from a computation graph where grads are not needed for J input
                # but phi_model params DO need grads. For initial_m_coords, this is fine.
                J_phi = F.jacobian(lambda x: inn_phi_model(x.unsqueeze(0)).squeeze(0), m_point_tensor.detach().clone().requires_grad_(True))
                g_M = J_phi.T @ eta_E_torch_for_loss @ J_phi
                g_Ms_for_reg.append(g_M)

            g_M_child, g_M_parent = g_Ms_for_reg[0], g_Ms_for_reg[1]

            if g_tt_reg_weight > 0:
                # --- New g_tt regularization ---
                # This loss pushes g_tt towards g_tt_target_value (-1.0)
                # It penalizes if g_tt is greater than target_value (i.e., less negative or positive)
                # and also penalizes if it's too far below the target (too negative, less likely but possible)

                # Penalty 1: Push g_tt towards target (MSE part)
                loss_g_tt_mse_c = mse_loss_fn(g_M_child[0,0], torch.tensor(g_tt_target_value, device=device))
                loss_g_tt_mse_p = mse_loss_fn(g_M_parent[0,0], torch.tensor(g_tt_target_value, device=device))

                # Penalty 2: Stronger push for g_tt to be negative (one-sided ReLU penalty for positive g_tt)
                # If g_tt is positive (e.g., 5.0), this term becomes (5.0 - (-1.0)) = 6.0.
                # If g_tt is slightly negative (e.g., -0.5), this becomes (-0.5 - (-1.0)) = 0.5.
                # If g_tt is at target (e.g., -1.0), this becomes (-1.0 - (-1.0)) = 0.0.
                # This uses the relu(-(x-target)) form, so it penalizes values greater than target.

                # We can make it a simple relu(g_tt) to punish positive values explicitly
                # combined with a general push towards the target.

                # For g_tt: We want g_tt to be -1.0.
                # Option A: MSE (which you had, but can be too aggressive if g_tt is very positive)
                # Option B: ReLU-based push for negativity + MSE for target

                # Let's use a combination: a stronger penalty for positive values,
                # and MSE for general deviation from the target once it's negative.

                # Part 1: Ensure it's negative or at least not too far positive
                # Push g_tt to be <= g_tt_target_value (e.g., <= -1.0).
                # Relu(x) where x is (current_gtt - target_gtt) is what we want.
                # This penalizes if current_gtt is *greater* than target_gtt.
                push_negative_c = torch.relu(g_M_child[0,0] - torch.tensor(g_tt_target_value, device=device))
                push_negative_p = torch.relu(g_M_parent[0,0] - torch.tensor(g_tt_target_value, device=device))

                # The total g_tt loss: weighted sum of ensuring it's negative AND it's close to -1.0
                # The `push_negative` term is what replaces the `* 100.0` part from prior conceptual code.
                # You'll need to adjust `g_tt_reg_weight` now.
                current_g_tt_loss = g_tt_reg_weight * (
                    loss_g_tt_mse_c + loss_g_tt_mse_p + # Push towards target value
                    (push_negative_c + push_negative_p) * 10.0 # Extra penalty for being too high (e.g., positive)
                )

                loss_metric_reg += current_g_tt_loss

            if g_spatial_diag_reg_weight > 0:
                target_spatial_diagonals = torch.full((inn_phi_model.input_dim - 1,), g_spatial_diag_target_value, device=device)
                loss_g_sp_diag_c = mse_loss_fn(torch.diag(g_M_child[1:,1:]), target_spatial_diagonals)
                loss_g_sp_diag_p = mse_loss_fn(torch.diag(g_M_parent[1:,1:]), target_spatial_diagonals)
                current_g_spatial_loss = g_spatial_diag_reg_weight * (loss_g_sp_diag_c + loss_g_sp_diag_p)
                loss_metric_reg += current_g_spatial_loss

            if g_off_diag_reg_weight > 0:
                g_M_child_offdiag = g_M_child.clone()
                g_M_child_offdiag.diagonal().fill_(0) # Zero out diagonal to sum only off-diagonals
                loss_g_off_diag_c = mse_loss_fn(g_M_child_offdiag, torch.zeros_like(g_M_child_offdiag))

                g_M_parent_offdiag = g_M_parent.clone()
                g_M_parent_offdiag.diagonal().fill_(0)
                loss_g_off_diag_p = mse_loss_fn(g_M_parent_offdiag, torch.zeros_like(g_M_parent_offdiag))

                current_g_off_diag_loss = g_off_diag_reg_weight * (loss_g_off_diag_c + loss_g_off_diag_p)
                loss_metric_reg += current_g_off_diag_loss


            loss = loss_causal_order + loss_interval + loss_metric_reg

            loss.backward()
            if gradient_clip_value > 0:
                torch.nn.utils.clip_grad_norm_(inn_phi_model.parameters(), gradient_clip_value)
            optimizer.step()

            epoch_losses['total'].append(loss.item())
            epoch_losses['causal'].append(loss_causal_order.item())
            epoch_losses['interval'].append(loss_interval.item())
            epoch_losses['g_tt'].append(current_g_tt_loss.item() if isinstance(current_g_tt_loss, torch.Tensor) else current_g_tt_loss)
            epoch_losses['g_spatial'].append(current_g_spatial_loss.item() if isinstance(current_g_spatial_loss, torch.Tensor) else current_g_spatial_loss)
            epoch_losses['g_off_diag'].append(current_g_off_diag_loss.item() if isinstance(current_g_off_diag_loss, torch.Tensor) else current_g_off_diag_loss)

        avg_total_loss = np.mean(epoch_losses['total'])
        avg_causal = np.mean(epoch_losses['causal'])
        avg_interval = np.mean(epoch_losses['interval'])
        avg_g_tt = np.mean(epoch_losses['g_tt'])
        avg_g_spatial = np.mean(epoch_losses['g_spatial'])
        avg_g_off_diag = np.mean(epoch_losses['g_off_diag'])

        if epoch % (num_epochs // 20 or 1) == 0 or epoch == num_epochs -1 :
            print(f"Epoch {epoch+1}/{num_epochs}, Avg Total Loss: {avg_total_loss:.4f} "
                  f"(C: {avg_causal:.4f}, I: {avg_interval:.4f}, "
                  f"g_tt: {avg_g_tt:.4f}, g_sp_diag: {avg_g_spatial:.4f}, g_sp_pos: {avg_g_spatial:.4f}, g_off: {avg_g_off_diag:.4f})")

        # Early stopping
        if avg_total_loss < best_loss:
            best_loss = avg_total_loss
            epochs_no_improve = 0
            # torch.save(inn_phi_model.state_dict(), "best_inn_phi_model.pth") # Optional: save best model
        else:
            epochs_no_improve += 1

        if epochs_no_improve >= patience and epoch > num_epochs // 4 : # Only activate early stopping after some initial training
            print(f"Early stopping at epoch {epoch+1} due to no improvement in total loss for {patience} epochs.")
            break


    print("Training finished.")
    # inn_phi_model.load_state_dict(torch.load("best_inn_phi_model.pth")) # Optional: load best model
    inn_phi_model.eval()

    final_e_coords_map = {}
    with torch.no_grad():
        for name, m_tensor in initial_m_coords_tensors_map.items():
             final_e_coords_map[name] = inn_phi_model(m_tensor.unsqueeze(0)).squeeze(0).cpu().numpy()

    return initial_m_coords_map, final_e_coords_map, inn_phi_model

# --- Example Usage with Metric Regularization (Adjusted Weights) ---
tokens_inn_reg_v2 = ['universe', 'galaxy', 'star_system', 'star', 'planet', 'sun', 'earth', 'milky_way', 'solar_system']
hierarchy_pairs_inn_reg_v2 = [
    ('galaxy', 'universe'), ('star_system', 'galaxy'), ('star', 'star_system'),
    ('planet', 'star_system'), ('sun', 'star'), ('earth', 'planet'),
    ('milky_way', 'galaxy'), ('solar_system', 'star_system'),
    ('solar_system', 'milky_way'), ('sun', 'solar_system'), ('earth', 'solar_system')
]

# Instantiate a new INN phi model for this training run
inn_phi_network_reg_v2 = INNPhi(input_dim=4, hidden_dim=128, num_coupling_layers=8) # Potentially deeper network

print("\n--- Training INN Phi Network with Adjusted Metric Regularization ---")
initial_m_coords_reg_v2, final_e_coords_reg_v2, trained_inn_phi_reg_v2 = train_inn_phi_embedding_with_metric_reg_v2(
    tokens_inn_reg_v2, hierarchy_pairs_inn_reg_v2,
    inn_phi_network_reg_v2,
    num_epochs=1000, # More epochs
    learning_rate=5e-5, # Further reduced LR
    epsilon1_target_sq=1e-5, # Smaller target for interval tightness
    causal_margin=0.01,      # Smaller causal margin
    g_tt_target_value = -1.0,
    g_tt_reg_weight = 1e-7,   # Moderate weight for g_tt 1e-6?
    g_spatial_diag_target_value = 1.0,
    g_spatial_diag_reg_weight = 0.001, # Moderate weight for g_spatial
    g_off_diag_target_value = 0.0,
    g_off_diag_reg_weight = 0.0001, # Moderate weight for off-diagonals
    gradient_clip_value = 0.5    # Gradient clipping
)

# --- Analysis of Pullback Metric at Final Inverted m_coords (as before) ---
# (This part remains the same, ensure `final_e_coords_reg_v2` and `trained_inn_phi_reg_v2` are used)
final_m_coords_from_inn_inverse_reg_v2 = {}
if final_e_coords_reg_v2:
    print("\nCalculating final m_coords on manifold using INN inverse (after reg_v2 training)...")
    # (Re-use the phi_inverse_numerical_nn from the previous block if it's not specific to a phi_model instance,
    #  or redefine ConceptualEventINNPhi to have an internal inverse method)
    # For INN, the inverse is direct:
    for token_id, e_final_np in final_e_coords_reg_v2.items():
        e_final_tensor = torch.tensor(e_final_np, dtype=torch.float32).to(next(trained_inn_phi_reg_v2.parameters()).device)
        with torch.no_grad():
            m_star_tensor = trained_inn_phi_reg_v2(e_final_tensor.unsqueeze(0), reverse=True).squeeze(0)
        final_m_coords_from_inn_inverse_reg_v2[token_id] = m_star_tensor.cpu().numpy()

    if 'plot_m_embedding' in locals() and 'hierarchy_pairs_inn_reg_v2' in locals():
         plot_m_embedding(final_m_coords_from_inn_inverse_reg_v2, "Learned Conceptual Manifold (m* = phi_inv(e_final)) with Adjusted Metric Reg")
    else:
        print("plot_m_embedding or hierarchy_pairs_inn_reg_v2 not defined, skipping m* plot.")


    print("\n--- Analyzing Pullback Metric g_M at Final INVERTED m_coords (after adjusted reg training) ---")
    points_to_analyze_reg_v2 = tokens_inn_reg_v2[:5]
    for token_id in points_to_analyze_reg_v2:
        if token_id in final_m_coords_from_inn_inverse_reg_v2:
            m_star_coord_np = final_m_coords_from_inn_inverse_reg_v2[token_id]
            m_star_coord_torch = torch.tensor(m_star_coord_np, dtype=torch.float32).to(next(trained_inn_phi_reg_v2.parameters()).device)

            def phi_for_jacobian_inn_final_reg_v2(m_input_tensor):
                return trained_inn_phi_reg_v2(m_input_tensor.unsqueeze(0), reverse=False).squeeze(0)
            try:
                J_phi_at_m_star_final = F.jacobian(phi_for_jacobian_inn_final_reg_v2, m_star_coord_torch)
                g_M_at_m_star_final = J_phi_at_m_star_final.T @ eta_E_torch_for_loss.to(J_phi_at_m_star_final.device) @ J_phi_at_m_star_final

                g_M_at_m_star_final_np = g_M_at_m_star_final.detach().cpu().numpy()
                det_g_M_at_m_star_final = np.linalg.det(g_M_at_m_star_final_np)

                print(f"\n--- g_M at '{token_id}' (final m* coords: {m_star_coord_np.round(3)}) ---")
                with np.printoptions(precision=3, suppress=True):
                    print(g_M_at_m_star_final_np)
                print(f"Determinant of g_M: {det_g_M_at_m_star_final:.3e}")
            except Exception as e_jac_inn_final:
                 print(f"Error calculating Jacobian or g_M for '{token_id}' (final m* with adjusted reg): {e_jac_inn_final}")
else:
    print("final_e_coords_reg_v2 is empty. Cannot calculate final m_coords or their g_M.")

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.autograd.functional as F # For Jacobian
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from collections import defaultdict

# --- Assume AdditiveCouplingLayer, INNPhi, ConceptualEventINNPhi are defined from previous cells ---
# --- Also, plot_coords_map and other helpers if used for final plotting ---

print("--- Phase 3 (Revised with Loss Adjustments v3): Learning Diffeomorphism phi ---")

# Minkowski metric in E-space (eta_E), will be used in loss
eta_E_torch_for_loss = torch.tensor(np.diag([-1.0, 1.0, 1.0, 1.0]), dtype=torch.float32)

def train_inn_phi_embedding_with_metric_reg_v3(
    tokens, hierarchy_pairs,
    inn_phi_model,
    num_epochs=500,
    learning_rate=1e-4,
    epsilon1_target_sq=1e-5,
    causal_margin = 0.05,
    g_tt_target_value = -1.0,
    g_tt_reg_weight = 0.1,   # Adjusted: Moderate starting point
    g_spatial_diag_target_value = 1.0,
    g_spatial_diag_reg_weight = 0.1, # Adjusted: Moderate starting point
    g_spatial_positivity_weight = 0.5, # NEW: Weight for ensuring spatial diagonals are positive
    g_off_diag_target_value = 0.0,
    g_off_diag_reg_weight = 0.05, # Adjusted: Moderate starting point
    gradient_clip_value = 1.0
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inn_phi_model.to(device)
    global eta_E_torch_for_loss
    eta_E_torch_for_loss = eta_E_torch_for_loss.to(device)

    initial_m_coords_map = {
        token_id: np.array([0.0] + np.random.uniform(-0.5, 0.5, inn_phi_model.input_dim - 1).tolist(), dtype=np.float32)
        for token_id in tokens
    }
    initial_m_coords_tensors_map = {
        name: torch.tensor(coords, dtype=torch.float32, device=device)
        for name, coords in initial_m_coords_map.items()
    }

    optimizer = optim.Adam(inn_phi_model.parameters(), lr=learning_rate)
    mse_loss_fn = nn.MSELoss()

    training_pairs_ids = []
    for child_id, parent_id in hierarchy_pairs:
        if parent_id is not None and child_id in initial_m_coords_tensors_map and parent_id in initial_m_coords_tensors_map:
            training_pairs_ids.append((child_id, parent_id))

    if not training_pairs_ids:
        print("No valid training pairs. Aborting.")
        final_e_coords_map_on_abort = {}
        with torch.no_grad():
            for name, m_tensor in initial_m_coords_tensors_map.items():
                 final_e_coords_map_on_abort[name] = inn_phi_model(m_tensor.unsqueeze(0)).squeeze(0).cpu().numpy()
        return initial_m_coords_map, final_e_coords_map_on_abort, inn_phi_model

    print(f"Starting training of INN_phi with MODIFIED metric regularization (v3) for {num_epochs} epochs...")
    print(f"LR={learning_rate}, eps1_sq={epsilon1_target_sq}, causal_m={causal_margin}")
    print(f"g_tt_w={g_tt_reg_weight}, g_sp_diag_w={g_spatial_diag_reg_weight}, g_sp_pos_w={g_spatial_positivity_weight}, g_off_diag_w={g_off_diag_reg_weight}, clip={gradient_clip_value}")

    best_loss = float('inf')
    epochs_no_improve = 0
    patience = 200 # Adjusted patience

    for epoch in range(num_epochs):
        inn_phi_model.train()
        epoch_losses = {
            'total': [], 'causal': [], 'interval': [],
            'g_tt': [], 'g_spatial_diag': [], 'g_spatial_pos':[], 'g_off_diag': []
        }

        # --- Detach and Clone m_point_tensor for Jacobian Calculation ---
        # This is crucial to prevent interference with the main computation graph
        # if m_coords themselves were ever to become learnable or modified.
        # For fixed initial_m_coords, detach().clone() ensures a fresh tensor for jacobian.
        # We need requires_grad=True for the input to F.jacobian.

        # Function to compute g_M for a given m_point_tensor
        def get_g_M(m_tensor_for_jac):
            # Ensure the input to jacobian requires grad
            m_tensor_for_jac_grad = m_tensor_for_jac.detach().clone().requires_grad_(True)
            J_phi = F.jacobian(lambda x: inn_phi_model(x.unsqueeze(0)).squeeze(0), m_tensor_for_jac_grad)
            return J_phi.T @ eta_E_torch_for_loss @ J_phi

        for child_id, parent_id in training_pairs_ids:
            optimizer.zero_grad()

            m_coords_child_tensor_input = initial_m_coords_tensors_map[child_id]
            m_coords_parent_tensor_input = initial_m_coords_tensors_map[parent_id]

            # Forward pass to get e_coords
            e_coords_child = inn_phi_model(m_coords_child_tensor_input.unsqueeze(0)).squeeze(0)
            e_coords_parent = inn_phi_model(m_coords_parent_tensor_input.unsqueeze(0)).squeeze(0)

            # --- Causal and Interval Loss (in E-space) ---
            delta_e_t = e_coords_child[0] - e_coords_parent[0]
            loss_causal_order = torch.relu(-(delta_e_t - causal_margin))

            delta_e_spatial_sq = torch.sum((e_coords_child[1:] - e_coords_parent[1:])**2)
            ds_sq_E = -delta_e_t**2 + delta_e_spatial_sq
            loss_interval = (ds_sq_E + epsilon1_target_sq)**2

            # --- Metric Regularization Loss ---
            loss_metric_reg = torch.tensor(0.0, device=device)
            # Initialize per-component losses for this step
            current_g_tt_loss_val = 0.0
            current_g_spatial_diag_loss_val = 0.0
            current_g_spatial_pos_loss_val = 0.0
            current_g_off_diag_loss_val = 0.0

            g_M_child = get_g_M(m_coords_child_tensor_input)
            g_M_parent = get_g_M(m_coords_parent_tensor_input)

            # g_tt regularization: Push towards target and penalize if positive
            if g_tt_reg_weight > 0:
                g_tt_c, g_tt_p = g_M_child[0,0], g_M_parent[0,0]
                loss_g_tt_mse = mse_loss_fn(g_tt_c, torch.tensor(g_tt_target_value, device=device)) + \
                                mse_loss_fn(g_tt_p, torch.tensor(g_tt_target_value, device=device))
                # Stronger penalty for g_tt > 0. We want it to be target_value or more negative.
                # Penalize if g_tt > target_value (e.g. if target is -1, penalize values > -1)
                loss_g_tt_pos_penalty = torch.relu(g_tt_c - g_tt_target_value) + \
                                        torch.relu(g_tt_p - g_tt_target_value)

                step_g_tt_loss = g_tt_reg_weight * (loss_g_tt_mse + loss_g_tt_pos_penalty * 10.0) # x10 emphasis
                loss_metric_reg += step_g_tt_loss
                current_g_tt_loss_val = step_g_tt_loss.item()

            # Spatial diagonal regularization
            spatial_diagonals_child = torch.diag(g_M_child[1:,1:])
            spatial_diagonals_parent = torch.diag(g_M_parent[1:,1:])

            if g_spatial_diag_reg_weight > 0:
                target_spatial_diagonals = torch.full((inn_phi_model.input_dim - 1,), g_spatial_diag_target_value, device=device)
                loss_g_sp_diag = mse_loss_fn(spatial_diagonals_child, target_spatial_diagonals) + \
                                 mse_loss_fn(spatial_diagonals_parent, target_spatial_diagonals)
                step_g_sp_diag_loss = g_spatial_diag_reg_weight * loss_g_sp_diag
                loss_metric_reg += step_g_sp_diag_loss
                current_g_spatial_diag_loss_val = step_g_sp_diag_loss.item()

            if g_spatial_positivity_weight > 0:
                # Penalize negative spatial diagonal components
                loss_g_sp_pos_c = torch.sum(torch.relu(-spatial_diagonals_child))
                loss_g_sp_pos_p = torch.sum(torch.relu(-spatial_diagonals_parent))
                step_g_sp_pos_loss = g_spatial_positivity_weight * (loss_g_sp_pos_c + loss_g_sp_pos_p)
                loss_metric_reg += step_g_sp_pos_loss
                current_g_spatial_pos_loss_val = step_g_sp_pos_loss.item()

            if g_off_diag_reg_weight > 0:
                g_M_child_offdiag = g_M_child.clone(); g_M_child_offdiag.diagonal().fill_(0)
                g_M_parent_offdiag = g_M_parent.clone(); g_M_parent_offdiag.diagonal().fill_(0)
                loss_g_off_diag = mse_loss_fn(g_M_child_offdiag, torch.zeros_like(g_M_child_offdiag)) + \
                                  mse_loss_fn(g_M_parent_offdiag, torch.zeros_like(g_M_parent_offdiag))
                step_g_off_diag_loss = g_off_diag_reg_weight * loss_g_off_diag
                loss_metric_reg += step_g_off_diag_loss
                current_g_off_diag_loss_val = step_g_off_diag_loss.item()

            loss = loss_causal_order + loss_interval + loss_metric_reg

            loss.backward()
            if gradient_clip_value > 0:
                torch.nn.utils.clip_grad_norm_(inn_phi_model.parameters(), gradient_clip_value)
            optimizer.step()

            epoch_losses['total'].append(loss.item())
            epoch_losses['causal'].append(loss_causal_order.item())
            epoch_losses['interval'].append(loss_interval.item())
            epoch_losses['g_tt'].append(current_g_tt_loss_val)
            epoch_losses['g_spatial_diag'].append(current_g_spatial_diag_loss_val)
            epoch_losses['g_spatial_pos'].append(current_g_spatial_pos_loss_val)
            epoch_losses['g_off_diag'].append(current_g_off_diag_loss_val)

        avg_total_loss = np.mean(epoch_losses['total'])
        avg_causal = np.mean(epoch_losses['causal'])
        avg_interval = np.mean(epoch_losses['interval'])
        avg_g_tt = np.mean(epoch_losses['g_tt'])
        avg_g_spatial_diag = np.mean(epoch_losses['g_spatial_diag'])
        avg_g_spatial_pos = np.mean(epoch_losses['g_spatial_pos'])
        avg_g_off_diag = np.mean(epoch_losses['g_off_diag'])

        if epoch % (num_epochs // 20 or 1) == 0 or epoch == num_epochs -1 :
            print(f"Epoch {epoch+1}/{num_epochs}, Avg Total Loss: {avg_total_loss:.4f} "
                  f"(C: {avg_causal:.4f}, I: {avg_interval:.4f}, "
                  f"g_tt: {avg_g_tt:.4f}, g_sp_diag: {avg_g_spatial_diag:.4f}, g_sp_pos: {avg_g_spatial_pos:.4f}, g_off: {avg_g_off_diag:.4f})")

        if avg_total_loss < best_loss:
            best_loss = avg_total_loss
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        if epochs_no_improve >= patience and epoch > num_epochs // 3 :
            print(f"Early stopping at epoch {epoch+1}.")
            break

    print("Training finished.")
    inn_phi_model.eval()

    final_e_coords_map = {}
    with torch.no_grad():
        for name, m_tensor in initial_m_coords_tensors_map.items():
             final_e_coords_map[name] = inn_phi_model(m_tensor.unsqueeze(0)).squeeze(0).cpu().numpy()

    return initial_m_coords_map, final_e_coords_map, inn_phi_model

# --- Example Usage with New Regularization Scheme ---
tokens_inn_reg_v3 = ['universe', 'galaxy', 'star_system', 'star', 'planet', 'sun', 'earth', 'milky_way', 'solar_system']
hierarchy_pairs_inn_reg_v3 = [
    ('galaxy', 'universe'), ('star_system', 'galaxy'), ('star', 'star_system'),
    ('planet', 'star_system'), ('sun', 'star'), ('earth', 'planet'),
    ('milky_way', 'galaxy'), ('solar_system', 'star_system'),
    ('solar_system', 'milky_way'), ('sun', 'solar_system'), ('earth', 'solar_system')
]

inn_phi_network_reg_v3 = INNPhi(input_dim=4, hidden_dim=128, num_coupling_layers=8)

print("\n--- Training INN Phi Network with v3 Metric Regularization ---")
# --- ADJUST THESE WEIGHTS CAREFULLY ---
# Start with primary losses having more implicit weight (smaller reg weights)
# Then gradually increase reg weights if metric signature is not good.
initial_m_coords_reg_v3, final_e_coords_reg_v3, trained_inn_phi_reg_v3 = train_inn_phi_embedding_with_metric_reg_v3(
    tokens_inn_reg_v3, hierarchy_pairs_inn_reg_v3,
    inn_phi_network_reg_v3,
    num_epochs=1000,        # Can increase if needed
    learning_rate=1e-4,     # Start smaller
    epsilon1_target_sq=1e-5,
    causal_margin=0.01,
    g_tt_target_value = -1.0,
    g_tt_reg_weight = 0.5,                 # e.g., 0.01 to 1.0
    g_spatial_diag_target_value = 1.0,
    g_spatial_diag_reg_weight = 0.01,       # e.g., 0.01 to 1.0
    g_spatial_positivity_weight = 1.0,     # NEW: Make this reasonably strong to enforce g_ii > 0
    g_off_diag_target_value = 0.0,
    g_off_diag_reg_weight = 0.001,          # e.g., 0.001 to 0.1
    gradient_clip_value = 1.0
)

# --- Analysis of Pullback Metric at Final Inverted m_coords ---
# (This part for analysis can be copied from the previous block,
#  just ensure to use `trained_inn_phi_reg_v3` and `final_e_coords_reg_v3`)
# (The function `analyze_local_geometry` would also need to be available or redefined)
# For brevity, I'll skip re-pasting the full analysis code here.
# The key is to run it with the outputs of this new training function.
# --- Analysis of Pullback Metric at Final Inverted m_coords (as before) ---
# (This part remains the same, ensure `final_e_coords_reg_v2` and `trained_inn_phi_reg_v2` are used)
final_m_coords_from_inn_inverse_reg_v2 = {}
if final_e_coords_reg_v2:
    print("\nCalculating final m_coords on manifold using INN inverse (after reg_v2 training)...")
    # (Re-use the phi_inverse_numerical_nn from the previous block if it's not specific to a phi_model instance,
    #  or redefine ConceptualEventINNPhi to have an internal inverse method)
    # For INN, the inverse is direct:
    for token_id, e_final_np in final_e_coords_reg_v2.items():
        e_final_tensor = torch.tensor(e_final_np, dtype=torch.float32).to(next(trained_inn_phi_reg_v2.parameters()).device)
        with torch.no_grad():
            m_star_tensor = trained_inn_phi_reg_v2(e_final_tensor.unsqueeze(0), reverse=True).squeeze(0)
        final_m_coords_from_inn_inverse_reg_v2[token_id] = m_star_tensor.cpu().numpy()

    if 'plot_m_embedding' in locals() and 'hierarchy_pairs_inn_reg_v2' in locals():
         plot_m_embedding(final_m_coords_from_inn_inverse_reg_v2, "Learned Conceptual Manifold (m* = phi_inv(e_final)) with Adjusted Metric Reg")
    else:
        print("plot_m_embedding or hierarchy_pairs_inn_reg_v2 not defined, skipping m* plot.")


    print("\n--- Analyzing Pullback Metric g_M at Final INVERTED m_coords (after adjusted reg training) ---")
    points_to_analyze_reg_v2 = tokens_inn_reg_v2[:5]
    for token_id in points_to_analyze_reg_v2:
        if token_id in final_m_coords_from_inn_inverse_reg_v2:
            m_star_coord_np = final_m_coords_from_inn_inverse_reg_v2[token_id]
            m_star_coord_torch = torch.tensor(m_star_coord_np, dtype=torch.float32).to(next(trained_inn_phi_reg_v2.parameters()).device)

            def phi_for_jacobian_inn_final_reg_v2(m_input_tensor):
                return trained_inn_phi_reg_v2(m_input_tensor.unsqueeze(0), reverse=False).squeeze(0)
            try:
                J_phi_at_m_star_final = F.jacobian(phi_for_jacobian_inn_final_reg_v2, m_star_coord_torch)
                g_M_at_m_star_final = J_phi_at_m_star_final.T @ eta_E_torch_for_loss.to(J_phi_at_m_star_final.device) @ J_phi_at_m_star_final

                g_M_at_m_star_final_np = g_M_at_m_star_final.detach().cpu().numpy()
                det_g_M_at_m_star_final = np.linalg.det(g_M_at_m_star_final_np)

                print(f"\n--- g_M at '{token_id}' (final m* coords: {m_star_coord_np.round(3)}) ---")
                with np.printoptions(precision=3, suppress=True):
                    print(g_M_at_m_star_final_np)
                print(f"Determinant of g_M: {det_g_M_at_m_star_final:.3e}")
            except Exception as e_jac_inn_final:
                 print(f"Error calculating Jacobian or g_M for '{token_id}' (final m* with adjusted reg): {e_jac_inn_final}")
else:
    print("final_e_coords_reg_v2 is empty. Cannot calculate final m_coords or their g_M.")

# Example of how to get final m* and analyze for one token:
if final_e_coords_reg_v3 and 'universe' in final_e_coords_reg_v3:
    print("\n--- Post-Training Analysis (v3) for 'universe' ---")
    e_final_univ_np = final_e_coords_reg_v3['universe']
    e_final_univ_tensor = torch.tensor(e_final_univ_np, dtype=torch.float32).to(next(trained_inn_phi_reg_v3.parameters()).device)
    with torch.no_grad():
        m_star_univ_tensor = trained_inn_phi_reg_v3(e_final_univ_tensor.unsqueeze(0), reverse=True).squeeze(0)
    m_star_univ_np = m_star_univ_tensor.cpu().numpy()

    if 'analyze_local_geometry' in locals(): # Check if the analysis function is defined
        univ_geo_features = analyze_local_geometry(m_star_univ_np, trained_inn_phi_reg_v3, eta_E_torch_for_loss.to(next(trained_inn_phi_reg_v3.parameters()).device))
        print(f"g_M at 'universe' (final m*): \n{univ_geo_features['g_M'].round(3)}")
        print(f"det(g_M): {univ_geo_features['det_g_M']:.3e}")
        print(f"g_tt: {univ_geo_features['g_tt']:.3f}")
        print(f"Avg Spatial Diag: {univ_geo_features['avg_spatial_diagonal_scaling']:.3f}")
        print(f"Time-Space Mixing: {univ_geo_features['time_space_mixing_abs_sum_g0k']:.3f}")

    # Plot the m* coordinates if plot_m_embedding is defined
    final_m_coords_from_inn_inverse_reg_v3 = {}
    for token_id, e_final_np_plot in final_e_coords_reg_v3.items():
        e_final_tensor_plot = torch.tensor(e_final_np_plot, dtype=torch.float32).to(next(trained_inn_phi_reg_v3.parameters()).device)
        with torch.no_grad():
            m_star_tensor_plot = trained_inn_phi_reg_v3(e_final_tensor_plot.unsqueeze(0), reverse=True).squeeze(0)
        final_m_coords_from_inn_inverse_reg_v3[token_id] = m_star_tensor_plot.cpu().numpy()

    if 'plot_m_embedding' in locals() and 'hierarchy_pairs_inn_reg_v3' in locals():
        plot_m_embedding(final_m_coords_from_inn_inverse_reg_v3, "Learned Conceptual Manifold (m* = phi_inv(e_final)) with v3 Metric Reg")


print("\n--- After v3 training, check if g_tt is consistently negative and spatial g_ii are positive. ---")
print("--- Further tuning of regularization weights and LR might be needed. ---")

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.autograd.functional as F # For Jacobian
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from collections import defaultdict

# --- Assume AdditiveCouplingLayer, INNPhi, ConceptualEventINNPhi, analyze_local_geometry, plot_m_embedding are defined ---

print("--- Phase 3 (Revised with clearer Loss & Debugging v4): Learning Diffeomorphism phi ---")

eta_E_torch_for_loss = torch.tensor(np.diag([-1.0, 1.0, 1.0, 1.0]), dtype=torch.float32)

def train_inn_phi_embedding_with_metric_reg_v4(
    tokens, hierarchy_pairs,
    inn_phi_model,
    num_epochs=1000, # Increased epochs
    learning_rate=1e-5, # START VERY LOW, then increase if stable
    epsilon1_target_sq=1e-6, # Smaller target for tighter interval
    causal_margin = 0.001,   # Smaller margin, less aggressive push initially

    # Metric Signature Targets
    g_tt_target = -1.0,
    g_spatial_diag_target = 1.0,
    g_off_diag_target = 0.0,

    # Regularization Weights - CRITICAL TUNING PARAMETERS
    # These control the trade-off. Start with primary loss being dominant.
    # Weights for embedding quality
    causal_loss_weight = 1.0,
    interval_loss_weight = 1.0,

    # Weights for metric signature quality
    g_tt_mse_weight = 0.01,          # MSE push towards g_tt_target
    g_tt_sign_penalty_weight = 0.1,  # Penalty for g_tt > 0 (or g_tt > target if target is negative)

    g_spatial_diag_mse_weight = 0.01, # MSE push towards g_spatial_diag_target
    g_spatial_positivity_penalty_weight = 0.1, # Penalty for g_spatial_ii < 0

    g_off_diag_mse_weight = 0.001,   # MSE push towards g_off_diag_target (0)

    gradient_clip_value = 1.0,
    log_interval = 50 # How often to print detailed logs
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inn_phi_model.to(device)
    global eta_E_torch_for_loss
    eta_E_torch_for_loss = eta_E_torch_for_loss.to(device)

    # Initialize m_coords: spread out initial time slightly for roots/early nodes
    # This is a heuristic, could be improved with topological sort.
    initial_m_coords_map = {}
    temp_time_counter = 0.0
    time_step = 0.01 # Small initial time separation

    # Crude way to assign initial times based on order in tokens list (replace with topo sort if possible)
    # For a more principled approach, do a quick flat embedding first to get initial times.
    for i, token_id in enumerate(tokens):
        # A very simple way to get some initial time spread - can be improved
        current_t = 0.0 # Or some other scheme like i * time_step
        initial_m_coords_map[token_id] = np.array(
            [current_t] + np.random.uniform(-0.1, 0.1, inn_phi_model.input_dim - 1).tolist(), # Smaller spatial range
            dtype=np.float32
        )

    initial_m_coords_tensors_map = {
        name: torch.tensor(coords, dtype=torch.float32, device=device)
        for name, coords in initial_m_coords_map.items()
    }

    optimizer = optim.Adam(inn_phi_model.parameters(), lr=learning_rate)
    mse_loss_fn = nn.MSELoss()

    training_pairs_ids = []
    for child_id, parent_id in hierarchy_pairs:
        if parent_id is not None and child_id in initial_m_coords_tensors_map and parent_id in initial_m_coords_tensors_map:
            training_pairs_ids.append((child_id, parent_id))

    if not training_pairs_ids:
        print("No valid training pairs. Aborting.")
        # (Return logic as before)
        final_e_coords_map_on_abort = {}
        with torch.no_grad():
            for name, m_tensor in initial_m_coords_tensors_map.items():
                 final_e_coords_map_on_abort[name] = inn_phi_model(m_tensor.unsqueeze(0)).squeeze(0).cpu().numpy()
        return initial_m_coords_map, final_e_coords_map_on_abort, inn_phi_model


    print(f"Starting training (v4) for {num_epochs} epochs on {device}...")
    # Print all weights
    print(f"LR={learning_rate}, causal_w={causal_loss_weight}, interval_w={interval_loss_weight}")
    print(f"g_tt_mse_w={g_tt_mse_weight}, g_tt_sign_w={g_tt_sign_penalty_weight}")
    print(f"g_sp_diag_mse_w={g_spatial_diag_mse_weight}, g_sp_pos_w={g_spatial_positivity_penalty_weight}")
    print(f"g_off_diag_mse_w={g_off_diag_mse_weight}, clip={gradient_clip_value}")


    best_loss = float('inf')
    epochs_no_improve = 0
    patience = 100 # Increased patience

    for epoch in range(num_epochs):
        inn_phi_model.train()
        # Store all individual losses for this epoch
        epoch_losses_agg = defaultdict(list)

        for child_id, parent_id in training_pairs_ids:
            optimizer.zero_grad()

            m_coords_child_tensor_input = initial_m_coords_tensors_map[child_id]
            m_coords_parent_tensor_input = initial_m_coords_tensors_map[parent_id]

            e_coords_child = inn_phi_model(m_coords_child_tensor_input.unsqueeze(0)).squeeze(0)
            e_coords_parent = inn_phi_model(m_coords_parent_tensor_input.unsqueeze(0)).squeeze(0)

            # --- Primary Embedding Losses ---
            delta_e_t = e_coords_child[0] - e_coords_parent[0]
            loss_causal_order = torch.relu(-(delta_e_t - causal_margin)) # Target: delta_e_t > causal_margin

            delta_e_spatial_sq = torch.sum((e_coords_child[1:] - e_coords_parent[1:])**2)
            ds_sq_E = -delta_e_t**2 + delta_e_spatial_sq
            loss_interval = (ds_sq_E - (-epsilon1_target_sq))**2 # Target: ds_sq_E = -epsilon1_target_sq

            # --- Metric Regularization Losses ---
            loss_metric_reg = torch.tensor(0.0, device=device)

            # Calculate Jacobians and g_M for child and parent
            g_M_child = get_g_M_for_loss(m_coords_child_tensor_input, inn_phi_model, eta_E_torch_for_loss)
            g_M_parent = get_g_M_for_loss(m_coords_parent_tensor_input, inn_phi_model, eta_E_torch_for_loss)

            # g_tt regularization
            g_tt_c, g_tt_p = g_M_child[0,0], g_M_parent[0,0]
            loss_g_tt_m = mse_loss_fn(g_tt_c, torch.tensor(g_tt_target, device=device)) + \
                          mse_loss_fn(g_tt_p, torch.tensor(g_tt_target, device=device))
            # Penalize g_tt if it's > 0 (should be negative)
            loss_g_tt_s = torch.relu(g_tt_c) + torch.relu(g_tt_p)
            step_g_tt_loss = g_tt_mse_weight * loss_g_tt_m + g_tt_sign_penalty_weight * loss_g_tt_s
            loss_metric_reg += step_g_tt_loss

            # Spatial diagonal regularization
            spatial_diagonals_child = torch.diag(g_M_child[1:,1:])
            spatial_diagonals_parent = torch.diag(g_M_parent[1:,1:])
            target_spatial_diagonals = torch.full((inn_phi_model.input_dim - 1,), g_spatial_diag_target, device=device)

            loss_g_sp_diag_m = mse_loss_fn(spatial_diagonals_child, target_spatial_diagonals) + \
                               mse_loss_fn(spatial_diagonals_parent, target_spatial_diagonals)
            # Penalize if any g_ii (spatial) < 0
            loss_g_sp_pos = torch.sum(torch.relu(-spatial_diagonals_child)) + \
                            torch.sum(torch.relu(-spatial_diagonals_parent))
            step_g_spatial_loss = g_spatial_diag_mse_weight * loss_g_sp_diag_m + \
                                  g_spatial_positivity_penalty_weight * loss_g_sp_pos
            loss_metric_reg += step_g_spatial_loss

            # Off-diagonal regularization
            g_M_child_offdiag = g_M_child.clone(); g_M_child_offdiag.diagonal().fill_(0)
            g_M_parent_offdiag = g_M_parent.clone(); g_M_parent_offdiag.diagonal().fill_(0)
            target_off_diag = torch.zeros_like(g_M_child_offdiag) # Target is 0

            loss_g_off_diag_m = mse_loss_fn(g_M_child_offdiag, target_off_diag) + \
                                mse_loss_fn(g_M_parent_offdiag, target_off_diag)
            step_g_off_diag_loss = g_off_diag_mse_weight * loss_g_off_diag_m
            loss_metric_reg += step_g_off_diag_loss

            # Total Loss
            loss = (causal_loss_weight * loss_causal_order +
                    interval_loss_weight * loss_interval +
                    loss_metric_reg)

            loss.backward()
            if gradient_clip_value > 0:
                torch.nn.utils.clip_grad_norm_(inn_phi_model.parameters(), gradient_clip_value)
            optimizer.step()

            epoch_losses_agg['total'].append(loss.item())
            epoch_losses_agg['causal'].append(causal_loss_weight * loss_causal_order.item())
            epoch_losses_agg['interval'].append(interval_loss_weight * loss_interval.item())
            epoch_losses_agg['g_tt'].append(step_g_tt_loss.item())
            epoch_losses_agg['g_spatial_diag'].append(g_spatial_diag_mse_weight * loss_g_sp_diag_m.item()) # Log only MSE part
            epoch_losses_agg['g_spatial_pos'].append(g_spatial_positivity_penalty_weight * loss_g_sp_pos.item())
            epoch_losses_agg['g_off_diag'].append(step_g_off_diag_loss.item())

        avg_total_loss = np.mean(epoch_losses_agg['total'])
        avg_causal = np.mean(epoch_losses_agg['causal'])
        avg_interval = np.mean(epoch_losses_agg['interval'])
        avg_g_tt = np.mean(epoch_losses_agg['g_tt'])
        avg_g_spatial_diag = np.mean(epoch_losses_agg['g_spatial_diag'])
        avg_g_spatial_pos = np.mean(epoch_losses_agg['g_spatial_pos'])
        avg_g_off_diag = np.mean(epoch_losses_agg['g_off_diag'])

        if epoch % log_interval == 0 or epoch == num_epochs -1 :
            print(f"Epoch {epoch+1}/{num_epochs}, Avg Total Loss: {avg_total_loss:.4f} "
                  f"(C: {avg_causal:.4f}, I: {avg_interval:.4f} | "
                  f"g_tt: {avg_g_tt:.4f}, g_sp_diag: {avg_g_spatial_diag:.4f}, g_sp_pos: {avg_g_spatial_pos:.4f}, g_off: {avg_g_off_diag:.4f})")

            # Sanity check: print g_M for one point
            if epoch % (log_interval * 5) == 0 and len(tokens)>0 : # Less frequent
                with torch.no_grad():
                    sample_token_id = tokens[0]
                    sample_m_tensor = initial_m_coords_tensors_map[sample_token_id]
                    g_M_sample = get_g_M_for_loss(sample_m_tensor, inn_phi_model, eta_E_torch_for_loss)
                    print(f"  Sample g_M for '{sample_token_id}' (g_tt: {g_M_sample[0,0].item():.3f}, "
                          f"g_xx: {g_M_sample[1,1].item():.3f}, g_yy: {g_M_sample[2,2].item():.3f}, g_zz: {g_M_sample[3,3].item():.3f}, "
                          f"det: {torch.linalg.det(g_M_sample).item():.3e})")

        if avg_total_loss < best_loss:
            best_loss = avg_total_loss
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        if epochs_no_improve >= patience and epoch > num_epochs // 2 : # Stricter early stopping
            print(f"Early stopping at epoch {epoch+1} as total loss hasn't improved for {patience} epochs.")
            break

    print("Training finished.")
    inn_phi_model.eval()

    final_e_coords_map = {}
    with torch.no_grad():
        for name, m_tensor in initial_m_coords_tensors_map.items():
             final_e_coords_map[name] = inn_phi_model(m_tensor.unsqueeze(0)).squeeze(0).cpu().numpy()

    return initial_m_coords_map, final_e_coords_map, inn_phi_model

# Helper function to compute g_M, ensuring correct grad handling for Jacobian input
def get_g_M_for_loss(m_tensor_input, phi_model, eta_E):
    m_for_jac = m_tensor_input.detach().clone().requires_grad_(True)
    J_phi = F.jacobian(lambda x: phi_model(x.unsqueeze(0)).squeeze(0), m_for_jac)
    return J_phi.T @ eta_E @ J_phi


# --- Example Usage with v4 Metric Regularization ---
tokens_inn_reg_v4 = ['universe', 'galaxy', 'star_system', 'star', 'planet', 'sun', 'earth', 'milky_way', 'solar_system']

hierarchy_pairs_inn_reg_v4 = [
    ('galaxy', 'universe'), ('star_system', 'galaxy'), ('star', 'star_system'),
    ('planet', 'star_system'), ('sun', 'star'), ('earth', 'planet'),
    ('milky_way', 'galaxy'), ('solar_system', 'star_system'),
    ('solar_system', 'milky_way'), ('sun', 'solar_system'), ('earth', 'solar_system')
]

inn_phi_network_reg_v4 = INNPhi(input_dim=4, hidden_dim=64, num_coupling_layers=6) # Start with a slightly smaller network

print("\n--- Training INN Phi Network with v4 Metric Regularization ---")
# --- KEY: START WITH VERY SMALL REGULARIZATION WEIGHTS and a SMALL LR ---
# --- Gradually increase reg weights if the metric signature is still problematic AFTER primary losses are low ---
initial_m_coords_reg_v4, final_e_coords_reg_v4, trained_inn_phi_reg_v4 = train_inn_phi_embedding_with_metric_reg_v4(
    tokens_inn_reg_v4, hierarchy_pairs_inn_reg_v4,
    inn_phi_network_reg_v4,
    num_epochs=2000,        # Increased epochs for finer tuning
    learning_rate=1e-5,     # Very small LR
    epsilon1_target_sq=1e-6,
    causal_margin=0.001,

    g_tt_target = -1.0,
    g_spatial_diag_target = 1.0,
    g_off_diag_target = 0.0,

    causal_loss_weight = 1.0,  # Keep primary losses strong
    interval_loss_weight = 1.0,
    # if loss good but gii still positive
    g_tt_mse_weight = 1e-3,                 # slight increase
    g_tt_sign_penalty_weight = 5e-2,  # Gruadually increase to 1.0
    # If spatial g ii are often negative
    g_spatial_diag_mse_weight = 1e-3,       # push towards 1.0
    g_spatial_positivity_penalty_weight = 1e-2, # gruadually increase 2.0

    g_off_diag_mse_weight = 1e-4,     # If off-diagonal terms are large: Gradually increase g_off_diag_mse_weight

    gradient_clip_value = 1.0,
    log_interval = 100 # Print logs every 100 epochs
)

# --- Analysis (can copy the full analysis block from your previous response) ---
# For brevity, just showing a snippet for 'universe'
if final_e_coords_reg_v4 and 'universe' in final_e_coords_reg_v4:
    print("\n--- Post-Training Analysis (v4) for 'universe' ---")
    # Ensure trained_inn_phi_reg_v4 and final_e_coords_reg_v4 are used
    e_final_univ_np_v4 = final_e_coords_reg_v4['universe']
    e_final_univ_tensor_v4 = torch.tensor(e_final_univ_np_v4, dtype=torch.float32).to(next(trained_inn_phi_reg_v4.parameters()).device)
    with torch.no_grad():
        m_star_univ_tensor_v4 = trained_inn_phi_reg_v4(e_final_univ_tensor_v4.unsqueeze(0), reverse=True).squeeze(0)
    m_star_univ_np_v4 = m_star_univ_tensor_v4.cpu().numpy()

    if 'analyze_local_geometry' in locals():
        univ_geo_features_v4 = analyze_local_geometry(m_star_univ_np_v4, trained_inn_phi_reg_v4, eta_E_torch_for_loss.to(next(trained_inn_phi_reg_v4.parameters()).device))
        print(f"g_M at 'universe' (final m*): \n{univ_geo_features_v4['g_M'].round(3)}")
        print(f"det(g_M): {univ_geo_features_v4['det_g_M']:.3e}")
        print(f"g_tt: {univ_geo_features_v4['g_tt']:.3f}")
        print(f"Avg Spatial Diag: {univ_geo_features_v4['avg_spatial_diagonal_scaling']:.3f}")
        print(f"Time-Space Mixing: {univ_geo_features_v4['time_space_mixing_abs_sum_g0k']:.3f}")

    # You would then call plot_m_embedding with final_m_coords_from_inn_inverse_reg_v4
    # which needs to be calculated for all tokens similarly to the 'universe' example above.
    final_m_coords_from_inn_inverse_reg_v4 = {}
    for token_id, e_final_np_plot in final_e_coords_reg_v4.items():
        e_final_tensor_plot = torch.tensor(e_final_np_plot, dtype=torch.float32).to(next(trained_inn_phi_reg_v4.parameters()).device)
        with torch.no_grad():
            m_star_tensor_plot = trained_inn_phi_reg_v4(e_final_tensor_plot.unsqueeze(0), reverse=True).squeeze(0)
        final_m_coords_from_inn_inverse_reg_v4[token_id] = m_star_tensor_plot.cpu().numpy()

    if 'plot_m_embedding' in locals() and 'hierarchy_pairs_inn_reg_v4' in locals():
        plot_m_embedding(final_m_coords_from_inn_inverse_reg_v4, "Learned Conceptual Manifold (m* = phi_inv(e_final)) with v4 Metric Reg")


print("\n--- This v4 training aims for more stability and better metric signature. ---")
print("--- Monitor loss components closely during training and adjust weights/LR iteratively. ---")

"""Why this is "Pretty Good":

    Achieved Target Metric Signature (Locally): For 'universe', the learned
    is very close to the desired Lorentzian signature diag([-1, 1, 1, 1]), with only mild spatial anisotropy and time-space mixing. This is a huge step.
    Stable Training: The loss didn't explode, indicating the regularization weights and learning rate are in a much better regime.
    Primary Goal Met: The low causal and interval losses suggest the hierarchy is well-embedded in the
    -space.
    Interpretable Geometry: You can now start to interpret the deviations from a perfectly flat Minkowski metric on         
    as meaningful "conceptual curvature" induced by the need to embed the hierarchy.

        The slight spatial stretching/compression along different axes for 'universe' could be correlated with its role or the nature of its child concepts.
"""

if final_e_coords_reg_v4 and 'earth' in final_e_coords_reg_v4:
    print("\n--- Post-Training Analysis (v4) for 'earth' ---")
    # Ensure trained_inn_phi_reg_v4 and final_e_coords_reg_v4 are used
    e_final_univ_np_v4 = final_e_coords_reg_v4['earth']
    e_final_univ_tensor_v4 = torch.tensor(e_final_univ_np_v4, dtype=torch.float32).to(next(trained_inn_phi_reg_v4.parameters()).device)
    with torch.no_grad():
        m_star_univ_tensor_v4 = trained_inn_phi_reg_v4(e_final_univ_tensor_v4.unsqueeze(0), reverse=True).squeeze(0)
    m_star_univ_np_v4 = m_star_univ_tensor_v4.cpu().numpy()

    if 'analyze_local_geometry' in locals():
        univ_geo_features_v4 = analyze_local_geometry(m_star_univ_np_v4, trained_inn_phi_reg_v4, eta_E_torch_for_loss.to(next(trained_inn_phi_reg_v4.parameters()).device))
        print(f"g_M at 'earth' (final m*): \n{univ_geo_features_v4['g_M'].round(3)}")
        print(f"det(g_M): {univ_geo_features_v4['det_g_M']:.3e}")
        print(f"g_tt: {univ_geo_features_v4['g_tt']:.3f}")
        print(f"Avg Spatial Diag: {univ_geo_features_v4['avg_spatial_diagonal_scaling']:.3f}")
        print(f"Time-Space Mixing: {univ_geo_features_v4['time_space_mixing_abs_sum_g0k']:.3f}")

    # You would then call plot_m_embedding with final_m_coords_from_inn_inverse_reg_v4
    # which needs to be calculated for all tokens similarly to the 'universe' example above.
    final_m_coords_from_inn_inverse_reg_v4 = {}
    for token_id, e_final_np_plot in final_e_coords_reg_v4.items():
        e_final_tensor_plot = torch.tensor(e_final_np_plot, dtype=torch.float32).to(next(trained_inn_phi_reg_v4.parameters()).device)
        with torch.no_grad():
            m_star_tensor_plot = trained_inn_phi_reg_v4(e_final_tensor_plot.unsqueeze(0), reverse=True).squeeze(0)
        final_m_coords_from_inn_inverse_reg_v4[token_id] = m_star_tensor_plot.cpu().numpy()

    if 'plot_m_embedding' in locals() and 'hierarchy_pairs_inn_reg_v4' in locals():
        plot_m_embedding(final_m_coords_from_inn_inverse_reg_v4, "Learned Conceptual Manifold (m* = phi_inv(e_final)) with v4 Metric Reg")

if final_e_coords_reg_v4 and 'planet' in final_e_coords_reg_v4:
    print("\n--- Post-Training Analysis (v4) for 'planet' ---")
    # Ensure trained_inn_phi_reg_v4 and final_e_coords_reg_v4 are used
    e_final_univ_np_v4 = final_e_coords_reg_v4['planet']
    e_final_univ_tensor_v4 = torch.tensor(e_final_univ_np_v4, dtype=torch.float32).to(next(trained_inn_phi_reg_v4.parameters()).device)
    with torch.no_grad():
        m_star_univ_tensor_v4 = trained_inn_phi_reg_v4(e_final_univ_tensor_v4.unsqueeze(0), reverse=True).squeeze(0)
    m_star_univ_np_v4 = m_star_univ_tensor_v4.cpu().numpy()

    if 'analyze_local_geometry' in locals():
        univ_geo_features_v4 = analyze_local_geometry(m_star_univ_np_v4, trained_inn_phi_reg_v4, eta_E_torch_for_loss.to(next(trained_inn_phi_reg_v4.parameters()).device))
        print(f"g_M at 'planet' (final m*): \n{univ_geo_features_v4['g_M'].round(3)}")
        print(f"det(g_M): {univ_geo_features_v4['det_g_M']:.3e}")
        print(f"g_tt: {univ_geo_features_v4['g_tt']:.3f}")
        print(f"Avg Spatial Diag: {univ_geo_features_v4['avg_spatial_diagonal_scaling']:.3f}")
        print(f"Time-Space Mixing: {univ_geo_features_v4['time_space_mixing_abs_sum_g0k']:.3f}")

    # You would then call plot_m_embedding with final_m_coords_from_inn_inverse_reg_v4
    # which needs to be calculated for all tokens similarly to the 'universe' example above.
    final_m_coords_from_inn_inverse_reg_v4 = {}
    for token_id, e_final_np_plot in final_e_coords_reg_v4.items():
        e_final_tensor_plot = torch.tensor(e_final_np_plot, dtype=torch.float32).to(next(trained_inn_phi_reg_v4.parameters()).device)
        with torch.no_grad():
            m_star_tensor_plot = trained_inn_phi_reg_v4(e_final_tensor_plot.unsqueeze(0), reverse=True).squeeze(0)
        final_m_coords_from_inn_inverse_reg_v4[token_id] = m_star_tensor_plot.cpu().numpy()

    if 'plot_m_embedding' in locals() and 'hierarchy_pairs_inn_reg_v4' in locals():
        plot_m_embedding(final_m_coords_from_inn_inverse_reg_v4, "Learned Conceptual Manifold (m* = phi_inv(e_final)) with v4 Metric Reg")

if final_e_coords_reg_v4 and 'star_system' in final_e_coords_reg_v4:
    print("\n--- Post-Training Analysis (v4) for 'star_system' ---")
    # Ensure trained_inn_phi_reg_v4 and final_e_coords_reg_v4 are used
    e_final_univ_np_v4 = final_e_coords_reg_v4['star_system']
    e_final_univ_tensor_v4 = torch.tensor(e_final_univ_np_v4, dtype=torch.float32).to(next(trained_inn_phi_reg_v4.parameters()).device)
    with torch.no_grad():
        m_star_univ_tensor_v4 = trained_inn_phi_reg_v4(e_final_univ_tensor_v4.unsqueeze(0), reverse=True).squeeze(0)
    m_star_univ_np_v4 = m_star_univ_tensor_v4.cpu().numpy()

    if 'analyze_local_geometry' in locals():
        univ_geo_features_v4 = analyze_local_geometry(m_star_univ_np_v4, trained_inn_phi_reg_v4, eta_E_torch_for_loss.to(next(trained_inn_phi_reg_v4.parameters()).device))
        print(f"g_M at 'star_system' (final m*): \n{univ_geo_features_v4['g_M'].round(3)}")
        print(f"det(g_M): {univ_geo_features_v4['det_g_M']:.3e}")
        print(f"g_tt: {univ_geo_features_v4['g_tt']:.3f}")
        print(f"Avg Spatial Diag: {univ_geo_features_v4['avg_spatial_diagonal_scaling']:.3f}")
        print(f"Time-Space Mixing: {univ_geo_features_v4['time_space_mixing_abs_sum_g0k']:.3f}")

    # You would then call plot_m_embedding with final_m_coords_from_inn_inverse_reg_v4
    # which needs to be calculated for all tokens similarly to the 'universe' example above.
    final_m_coords_from_inn_inverse_reg_v4 = {}
    for token_id, e_final_np_plot in final_e_coords_reg_v4.items():
        e_final_tensor_plot = torch.tensor(e_final_np_plot, dtype=torch.float32).to(next(trained_inn_phi_reg_v4.parameters()).device)
        with torch.no_grad():
            m_star_tensor_plot = trained_inn_phi_reg_v4(e_final_tensor_plot.unsqueeze(0), reverse=True).squeeze(0)
        final_m_coords_from_inn_inverse_reg_v4[token_id] = m_star_tensor_plot.cpu().numpy()

    if 'plot_m_embedding' in locals() and 'hierarchy_pairs_inn_reg_v4' in locals():
        plot_m_embedding(final_m_coords_from_inn_inverse_reg_v4, "Learned Conceptual Manifold (m* = phi_inv(e_final)) with v4 Metric Reg")

if final_e_coords_reg_v4 and 'galaxy' in final_e_coords_reg_v4:
    print("\n--- Post-Training Analysis (v4) for 'galaxy' ---")
    # Ensure trained_inn_phi_reg_v4 and final_e_coords_reg_v4 are used
    e_final_univ_np_v4 = final_e_coords_reg_v4['galaxy']
    e_final_univ_tensor_v4 = torch.tensor(e_final_univ_np_v4, dtype=torch.float32).to(next(trained_inn_phi_reg_v4.parameters()).device)
    with torch.no_grad():
        m_star_univ_tensor_v4 = trained_inn_phi_reg_v4(e_final_univ_tensor_v4.unsqueeze(0), reverse=True).squeeze(0)
    m_star_univ_np_v4 = m_star_univ_tensor_v4.cpu().numpy()

    if 'analyze_local_geometry' in locals():
        univ_geo_features_v4 = analyze_local_geometry(m_star_univ_np_v4, trained_inn_phi_reg_v4, eta_E_torch_for_loss.to(next(trained_inn_phi_reg_v4.parameters()).device))
        print(f"g_M at 'galaxy' (final m*): \n{univ_geo_features_v4['g_M'].round(3)}")
        print(f"det(g_M): {univ_geo_features_v4['det_g_M']:.3e}")
        print(f"g_tt: {univ_geo_features_v4['g_tt']:.3f}")
        print(f"Avg Spatial Diag: {univ_geo_features_v4['avg_spatial_diagonal_scaling']:.3f}")
        print(f"Time-Space Mixing: {univ_geo_features_v4['time_space_mixing_abs_sum_g0k']:.3f}")

    # You would then call plot_m_embedding with final_m_coords_from_inn_inverse_reg_v4
    # which needs to be calculated for all tokens similarly to the 'universe' example above.
    final_m_coords_from_inn_inverse_reg_v4 = {}
    for token_id, e_final_np_plot in final_e_coords_reg_v4.items():
        e_final_tensor_plot = torch.tensor(e_final_np_plot, dtype=torch.float32).to(next(trained_inn_phi_reg_v4.parameters()).device)
        with torch.no_grad():
            m_star_tensor_plot = trained_inn_phi_reg_v4(e_final_tensor_plot.unsqueeze(0), reverse=True).squeeze(0)
        final_m_coords_from_inn_inverse_reg_v4[token_id] = m_star_tensor_plot.cpu().numpy()

    if 'plot_m_embedding' in locals() and 'hierarchy_pairs_inn_reg_v4' in locals():
        plot_m_embedding(final_m_coords_from_inn_inverse_reg_v4, "Learned Conceptual Manifold (m* = phi_inv(e_final)) with v4 Metric Reg")

# Assuming geometric_features_map is populated as in the previous analysis block

def plot_m_embedding_with_g_feature(m_coords_map_plot, hierarchy_pairs_plot,
                                    feature_map, feature_name, title="Conceptual Manifold"):
    fig = plt.figure(figsize=(12, 10)) # Increased size slightly
    ax = fig.add_subplot(111, projection='3d')

    points_list = []
    labels_list = []
    feature_values = []

    for token_id, m_coords_val in m_coords_map_plot.items():
        points_list.append(m_coords_val)
        labels_list.append(token_id)
        if token_id in feature_map and feature_name in feature_map[token_id]:
            feature_values.append(feature_map[token_id][feature_name])
        else:
            feature_values.append(np.nan) # Or some default if feature missing

    points = np.array(points_list)

    if not np.all(np.isnan(feature_values)):
        scatter = ax.scatter(points[:, 1], points[:, 2], points[:, 0], c=feature_values, cmap='coolwarm', s=100, alpha=0.8)
        cbar = fig.colorbar(scatter, label=feature_name, shrink=0.7, aspect=20)
        if feature_name == 'g_tt':
            cbar.set_label(f'{feature_name} (Target: -1.0)')
    else: # Fallback if no feature values
        ax.scatter(points[:, 1], points[:, 2], points[:, 0], s=100, alpha=0.8)


    ax.set_xlabel('Conceptual X (m_x)')
    ax.set_ylabel('Conceptual Y (m_y)')
    ax.set_zlabel('Conceptual Time (m_t)')

    min_coords = points[:, :3].min(axis=0)
    max_coords = points[:, :3].max(axis=0)
    ax.auto_scale_xyz([min_coords[1],max_coords[1]], [min_coords[2],max_coords[2]],[min_coords[0],max_coords[0]])


    for i, txt in enumerate(labels_list):
        ax.text(points[i, 1], points[i, 2], points[i, 0], f' {txt}', size=8, zorder=1, color='black')

    for child_id, parent_id in hierarchy_pairs_plot:
        if parent_id is None or child_id not in m_coords_map_plot or parent_id not in m_coords_map_plot:
            continue
        child_m_coords = m_coords_map_plot[child_id]
        parent_m_coords = m_coords_map_plot[parent_id]
        ax.plot(
            [parent_m_coords[1], child_m_coords[1]],
            [parent_m_coords[2], child_m_coords[2]],
            [parent_m_coords[0], child_m_coords[0]],
            color='gray', linestyle=':', alpha=0.5, linewidth=0.8
        )
    ax.set_title(title)
    plt.tight_layout()
    plt.show()

# After running your analysis and populating geometric_features_map:
if 'geometric_features_map' in locals() and geometric_features_map:
    plot_m_embedding_with_g_feature(
        final_m_coords_from_inn_inverse_reg_v4, # Use the m* coords
        hierarchy_pairs_inn_reg_v4,
        geometric_features_map,
        'g_tt', # Feature to color by
        "Learned Conceptual Manifold (m*) - Color by g_tt"
    )
    plot_m_embedding_with_g_feature(
        final_m_coords_from_inn_inverse_reg_v4,
        hierarchy_pairs_inn_reg_v4,
        geometric_features_map,
        'avg_spatial_diagonal_scaling',
        "Learned Conceptual Manifold (m*) - Color by Avg Spatial Scaling"
    )
else:
    print("Geometric features map not available for enhanced plotting.")
# Num children
# 0 - Universe
# 1 - galaxy, star_system, star, planet, milky way
# 2 - sun, earth, solar_system
# num parents.
# 3 - star_system
# 2 - galaxy, solar_system
# 1 - Universe, star, planet, milky_way,
# 0 - earth, sun
# gtt connections (1,2,4,2,1) in terms of total edges
# universe, planet, sun, star, star_system, solar_system, earth, milky way, galaxy

import numpy as np
import torch
import torch.autograd.functional as F
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from collections import defaultdict

# --- Ensure necessary variables are defined from previous cells ---
# trained_inn_phi = trained_inn_phi_reg_v4
# final_e_coords_map = final_e_coords_reg_v4
# initial_m_coords_map = initial_m_coords_reg_v4 # Needed for m* calculation if not already done
# hierarchy_pairs = hierarchy_pairs_inn_reg_v4
# tokens = tokens_inn_reg_v4
# analyze_local_geometry (function)
# plot_m_embedding_with_g_feature (function)

# For demonstration, if the full training wasn't run, use placeholder data
if 'trained_inn_phi_reg_v4' not in locals():
    print("Warning: `trained_inn_phi_reg_v4` not found. Using dummy INN and data.")
    class DummyPhi(torch.nn.Module):
        def __init__(self): super().__init__(); self.fc = torch.nn.Linear(4,4)
        def forward(self, x, reverse=False): return self.fc(x) if not reverse else x # very basic
    trained_inn_phi_reg_v4 = DummyPhi()
    initial_m_coords_map_reg_v4 = {t: np.random.rand(4) for t in tokens_inn_reg_v4}
    final_e_coords_reg_v4 = {t: np.random.rand(4) for t in tokens_inn_reg_v4}
    hierarchy_pairs_inn_reg_v4 = [
        ('galaxy', 'universe'), ('star_system', 'galaxy'), ('star', 'star_system'),
        ('planet', 'star_system'), ('sun', 'star'), ('earth', 'planet'),
        ('milky_way', 'galaxy'), ('solar_system', 'star_system'),
        ('solar_system', 'milky_way'), ('sun', 'solar_system'), ('earth', 'solar_system')
    ]
    tokens_inn_reg_v4 = list(final_e_coords_reg_v4.keys())


trained_inn_phi = trained_inn_phi_reg_v4 # Use the regularized model
eta_E_torch = torch.tensor(np.diag([-1.0, 1.0, 1.0, 1.0]), dtype=torch.float32).to(next(trained_inn_phi.parameters()).device)


print("--- Step 0: Calculate Final Manifold Coordinates (m*) if not already done ---")
# This step assumes `final_e_coords_reg_v4` is the output from training
# and `trained_inn_phi` is the trained model.
final_m_coords_on_manifold_map = {}
if final_e_coords_reg_v4:
    print("Calculating final m_coords on manifold using INN inverse...")
    for token_id, e_final_np in final_e_coords_reg_v4.items():
        e_final_tensor = torch.tensor(e_final_np, dtype=torch.float32).to(next(trained_inn_phi.parameters()).device)
        with torch.no_grad():
            m_star_tensor = trained_inn_phi(e_final_tensor.unsqueeze(0), reverse=True).squeeze(0)
        final_m_coords_on_manifold_map[token_id] = m_star_tensor.cpu().numpy()
    print("Finished calculating final m_coords on manifold.")
else:
    print("`final_e_coords_reg_v4` is empty. Cannot proceed with m* calculation and analysis.")
    # Create dummy for code to run
    final_m_coords_on_manifold_map = {t: np.random.rand(4) for t in tokens_inn_reg_v4}


# --- 1. Calculate Node Degrees ---
print("\n--- Step 1: Calculating Node Degrees ---")
in_degree = defaultdict(int)
out_degree = defaultdict(int)
total_degree = defaultdict(int)

for child, parent in hierarchy_pairs_inn_reg_v4:
    if parent is not None: # Exclude root's "None" parent
        if child in tokens_inn_reg_v4 and parent in tokens_inn_reg_v4: # Ensure nodes are in our token list
            out_degree[parent] += 1
            in_degree[child] += 1
            total_degree[parent] += 1
            total_degree[child] += 1

for token in tokens_inn_reg_v4: # Ensure all tokens have an entry, even if degree is 0
    _ = in_degree[token]
    _ = out_degree[token]
    _ = total_degree[token]

print("Node degrees calculated.")

# --- 2. Create Pandas DataFrame with Geometric Features and Degrees ---
print("\n--- Step 2: Creating Pandas DataFrame ---")
data_for_df = []
if final_m_coords_on_manifold_map:
    for token_id, m_star_coord_np in final_m_coords_on_manifold_map.items():
        try:
            geo_features = analyze_local_geometry(m_star_coord_np, trained_inn_phi, eta_E_torch)
            data_for_df.append({
                'token': token_id,
                'm_t': m_star_coord_np[0],
                'm_x': m_star_coord_np[1],
                'm_y': m_star_coord_np[2],
                'm_z': m_star_coord_np[3],
                'g_tt': geo_features['g_tt'],
                'g_xx': geo_features['g_M'][1,1],
                'g_yy': geo_features['g_M'][2,2],
                'g_zz': geo_features['g_M'][3,3],
                'avg_g_spatial_diag': geo_features['avg_spatial_diagonal_scaling'],
                'det_g_M': geo_features['det_g_M'],
                'time_space_mix': geo_features['time_space_mixing_abs_sum_g0k'],
                'in_degree': in_degree[token_id],
                'out_degree': out_degree[token_id],
                'total_degree': total_degree[token_id]
            })
        except Exception as e:
            print(f"Could not analyze geometry for DataFrame for '{token_id}': {e}")

df_analysis = pd.DataFrame(data_for_df)
if not df_analysis.empty:
    print("\nAnalysis DataFrame (first 5 rows):")
    print(df_analysis.head())
else:
    print("DataFrame is empty. No analysis performed.")

# --- 3. Basic Correlation ---
if not df_analysis.empty and len(df_analysis) > 1: # Correlation needs at least 2 data points
    print("\n--- Step 3: Correlation Matrix (Partial) ---")
    # Select numerical columns for correlation
    numerical_cols = ['m_t', 'm_x', 'm_y', 'm_z', 'g_tt', 'g_xx', 'g_yy', 'g_zz',
                      'avg_g_spatial_diag', 'det_g_M', 'time_space_mix',
                      'in_degree', 'out_degree', 'total_degree']
    # Ensure selected columns exist in the DataFrame
    valid_numerical_cols = [col for col in numerical_cols if col in df_analysis.columns]

    if valid_numerical_cols:
        correlation_matrix = df_analysis[valid_numerical_cols].corr()
        print("Correlations with 'total_degree':")
        print(correlation_matrix['total_degree'].sort_values(ascending=False))
        print("\nCorrelations with 'g_tt':")
        print(correlation_matrix['g_tt'].sort_values(ascending=False))
    else:
        print("No valid numerical columns for correlation.")
else:
    print("DataFrame empty or too small for correlation.")


# --- 4. Visualize Paths on M (Preimages of Straight Lines in E) ---
print("\n--- Step 4: Visualizing Paths on Conceptual Manifold M ---")

def plot_m_embedding_with_paths(m_coords_map_plot, hierarchy_pairs_plot,
                                final_e_coords_plot, phi_inverse_model,
                                title="Conceptual Manifold with Paths",
                                paths_to_plot=None): # Optional: list of (parent, child) tuples
    fig = plt.figure(figsize=(14, 12)) # Larger figure
    ax = fig.add_subplot(111, projection='3d')

    points_list = []
    labels_list = []
    for token_id, m_coords_val in m_coords_map_plot.items():
        points_list.append(m_coords_val)
        labels_list.append(token_id)
    points = np.array(points_list)

    ax.scatter(points[:, 1], points[:, 2], points[:, 0], c=points[:, 3], cmap='viridis', s=120, alpha=0.7)
    ax.set_xlabel('Conceptual X (m_x)')
    ax.set_ylabel('Conceptual Y (m_y)')
    ax.set_zlabel('Conceptual Time (m_t)')
    # fig.colorbar(scatter, label='Conceptual Z (m_z)', shrink=0.7, aspect=20) # Optional: if colors are meaningful beyond distinction

    for i, txt in enumerate(labels_list):
        ax.text(points[i, 1], points[i, 2], points[i, 0], f' {txt}', size=9, zorder=1, color='black')

    # Plot actual hierarchy links (dotted, as before)
    for child_id, parent_id in hierarchy_pairs_plot:
        if parent_id is None or child_id not in m_coords_map_plot or parent_id not in m_coords_map_plot:
            continue
        child_m_coords = m_coords_map_plot[child_id]
        parent_m_coords = m_coords_map_plot[parent_id]
        ax.plot(
            [parent_m_coords[1], child_m_coords[1]],
            [parent_m_coords[2], child_m_coords[2]],
            [parent_m_coords[0], child_m_coords[0]],
            color='gray', linestyle=':', alpha=0.4, linewidth=0.7
        )

    # Plot selected "geodesic-like" paths (preimages of straight lines in E)
    if paths_to_plot is None: # Default to a few example paths if not specified
        paths_to_plot = [('star_system', 'star'), ('galaxy', 'star_system'), ('universe', 'galaxy')]

    num_path_points = 50
    device_paths = next(phi_inverse_model.parameters()).device

    for parent_id_path, child_id_path in paths_to_plot:
        if parent_id_path in final_e_coords_plot and child_id_path in final_e_coords_plot:
            e_parent = torch.tensor(final_e_coords_plot[parent_id_path], dtype=torch.float32, device=device_paths)
            e_child = torch.tensor(final_e_coords_plot[child_id_path], dtype=torch.float32, device=device_paths)

            e_path_points = torch.stack([e_parent + (e_child - e_parent) * t
                                         for t in np.linspace(0, 1, num_path_points)])

            with torch.no_grad():
                m_path_points_tensor = phi_inverse_model(e_path_points, reverse=True)
            m_path_points_np = m_path_points_tensor.cpu().numpy()

            ax.plot(m_path_points_np[:, 1], m_path_points_np[:, 2], m_path_points_np[:, 0],
                    linewidth=1.5, alpha=0.8,
                    label=f"Path: {parent_id_path} -> {child_id_path}")

    ax.set_title(title)
    if paths_to_plot : ax.legend(fontsize='small')
    plt.tight_layout()
    plt.show()

if final_m_coords_on_manifold_map and final_e_coords_reg_v4:
    # Select a few interesting paths based on your hierarchy
    paths_to_visualize = [
        ('universe', 'galaxy'),
        ('galaxy', 'milky_way'),
        ('milky_way', 'solar_system'),
        ('solar_system', 'earth'),
        ('star_system', 'star'), # A different branch
    ]
    # Filter paths_to_visualize to only include pairs present in the hierarchy
    valid_paths_to_visualize = []
    for p,c in paths_to_visualize:
        found = False
        for ch_h, pa_h in hierarchy_pairs_inn_reg_v4:
            if ch_h == c and pa_h == p:
                found = True
                break
        if found:
            valid_paths_to_visualize.append((p,c))
        else:
            print(f"Path {p}->{c} not in hierarchy_pairs, skipping for path visualization.")


    plot_m_embedding_with_paths(
        final_m_coords_on_manifold_map,
        hierarchy_pairs_inn_reg_v4,
        final_e_coords_reg_v4, # Pass the e_coords map
        trained_inn_phi,       # Pass the trained INN model for inverse
        title="Learned Conceptual Manifold ($m^*$) with Selected Paths",
        paths_to_plot=valid_paths_to_visualize
    )
else:
    print("Cannot plot paths as m* or e_final coordinates are not available.")

print("\n--- Interpretation of Paths ---")
print("The solid colored lines in the plot are the preimages (under phi_inverse) of straight lines in the E-space.")
print("These represent the 'straightest possible paths' (geodesics) on your learned conceptual manifold M.")
print("Their curvature visually demonstrates the non-Euclidean geometry induced by your learned phi.")
print("The dotted gray lines are the direct connections in m-coordinate space, highlighting the difference.")

import numpy as np
import pandas as pd # Ensure pandas is imported if not already
import matplotlib.pyplot as plt
import seaborn as sns # For potentially nicer plots and regression lines

# --- Ensure df_analysis is available from the previous cell ---
if 'df_analysis' not in locals() or df_analysis.empty:
    print("Error: `df_analysis` DataFrame is not found or is empty.")
    print("Please ensure the previous cell that creates and populates df_analysis has been run successfully.")
    # Create a dummy DataFrame for the code to run without error if needed for testing
    if 'df_analysis' not in locals() or df_analysis.empty:
        print("Creating a dummy df_analysis for demonstration.")
        dummy_data = {
            'token': ['a', 'b', 'c', 'd', 'e'],
            'm_x': np.random.rand(5) * 2 - 1,
            'g_tt': -1 + np.random.rand(5) * 0.2 - 0.1, # Values around -1
            'total_degree': np.random.randint(1, 5, 5),
            'avg_g_spatial_diag': 1 + np.random.rand(5) * 0.2 - 0.1,
            'time_space_mix': np.random.rand(5) * 0.5
        }
        df_analysis = pd.DataFrame(dummy_data)

# --- 1. Plot m_x vs g_tt ---
print("\n--- Plotting m_x vs g_tt for all concepts ---")
if not df_analysis.empty and 'm_x' in df_analysis.columns and 'g_tt' in df_analysis.columns:
    plt.figure(figsize=(10, 6))
    sns.scatterplot(data=df_analysis, x='m_x', y='g_tt', hue='token', s=100, legend=False) # Use token for hue temporarily

    # Optional: Add a regression line to see the trend
    sns.regplot(data=df_analysis, x='m_x', y='g_tt', scatter=False, color='red', line_kws={'linestyle':'--'})

    plt.title('Conceptual $m_x$ Coordinate vs. $g_{tt}$ Metric Component')
    plt.xlabel('Conceptual X-coordinate ($m_x$) on Manifold $\mathcal{M}$')
    plt.ylabel('$g_{tt}$ component of Pullback Metric $g_{\mathcal{M}}$')
    plt.grid(True, linestyle=':', alpha=0.7)

    # Annotate points if not too many, or select key points
    if len(df_analysis) < 20: # Only annotate if few points to avoid clutter
        for i in range(df_analysis.shape[0]):
            plt.text(df_analysis['m_x'][i]+0.005, df_analysis['g_tt'][i], df_analysis['token'][i], fontsize=9)

    # Highlight the target g_tt = -1.0
    plt.axhline(-1.0, color='gray', linestyle='--', label='$g_{tt} = -1.0$ (Target)')
    plt.legend()
    plt.show()

    correlation_mx_gtt = df_analysis['m_x'].corr(df_analysis['g_tt'])
    print(f"\nCorrelation between m_x and g_tt: {correlation_mx_gtt:.4f}")
    if np.abs(correlation_mx_gtt) > 0.7:
        print("This confirms a strong linear relationship.")
    elif np.abs(correlation_mx_gtt) > 0.4:
        print("This indicates a moderate linear relationship.")
    else:
        print("The linear relationship is weak or non-existent. The visual trend might be non-linear.")

else:
    print("Could not plot m_x vs g_tt: DataFrame is empty or missing required columns.")

# --- 2. Further Discussion on Total Degrees and Path Curvature ---
print("\n--- Discussion: Total Degrees and Path Curvature ---")
print("Your observation: 'total degrees around points correlates with more curvature in the plotted lines.'")
print("This is a very insightful hypothesis! Let's explore how to quantify and test this:")

print("\nPossible Reasons for this Correlation:")
print("1.  **Learned Geometric Response to Connectivity:** The INN $\phi$, when trying to embed a highly connected concept (a hub) while maintaining causal links to many children/parents in the flat E-space, might induce stronger local curvature (more significant warping) on the M-manifold around that hub. Straight lines in E-space, when pulled back to M, would then appear more bent near these hubs.")
print("2.  **Representation of 'Influence' or 'Abstraction':** Highly connected nodes often represent more central or abstract concepts. The learned geometry might naturally make paths to/from these central concepts more 'curved' to reflect their broad influence or the significant conceptual 'distance' they span to more specific entities.")
print("3.  **Embedding Density:** Hubs are connected to many other nodes. To fit all these 'almost null' links in the E-space without violating causality, the mapping $\phi$ might need to 'bend' the M-space more significantly in the vicinity of these hubs. This is analogous to how massive objects warp spacetime more strongly.")

print("\nHow to Quantify Path Curvature (Conceptual Approaches):")
print("Directly quantifying the 'curvature' of a plotted 3D line is non-trivial. Here are some ideas:")
print("  a.  **Deviation from Straight Line in M-space:** For each path (preimage of straight E-space line), calculate the sum of squared distances of intermediate path points from the direct straight line connecting the start and end m*-coordinates. Higher values = more 'bent'.")
print("  b.  **Integrated Curvature along Path (Advanced):** If you had the full Riemann curvature tensor derived from $g_{\mathcal{M}}$, you could integrate some measure of curvature (e.g., components of Riemann tensor projected onto the path's tangent vectors) along the path.")
print("  c.  **Change in Tangent Vectors:** Calculate the angle change between successive tangent vectors along the discretized path in M-space. A larger total angle change indicates more bending.")
print("  d.  **Approximation using Local $g_{\mathcal{M}}$ Variation:** For a path from $m_A^*$ to $m_B^*$, look at how much $g_{\mathcal{M}}$ (e.g., its determinant or key components) varies for points $m^*$ along this path compared to $g_{\mathcal{M}}(m_A^*)$ and $g_{\mathcal{M}}(m_B^*)$. Rapid changes in $g_{\mathcal{M}}$ along the path suggest significant underlying curvature affecting the path's shape.")

print("\nSteps to Test Your Hypothesis:")
print("1.  **Define a Path Curvature Metric:** Choose one of the methods above (e.g., start with 'Deviation from Straight Line in M-space' as it's simpler).")
print("2.  **Calculate for All Hierarchical Paths:** For every parent-child link in your `hierarchy_pairs`:")
print("    i.   Get their `final_e_coords`.")
print("    ii.  Generate the discretized path in M-space by interpolating in E-space and applying `phi_inverse`.")
print("    iii. Calculate your chosen path curvature metric for this M-space path.")
print("3.  **Aggregate Degree Information:** For each path (link), consider the `total_degree` of its parent node, its child node, or their average/sum.")
print("4.  **Correlate:** Plot and calculate the correlation between the path curvature metric and the aggregated degree information.")

print("\nExample (Pseudo-code for Path Curvature Metric - Deviation from Straight Line):")
print("""
# For a path m_path_points_np (N x 4 array) from m_start to m_end:
# m_start = m_path_points_np[0, 1:4] # Spatial part
# m_end = m_path_points_np[-1, 1:4]
# path_vector = m_end - m_start
# path_length_sq = np.sum(path_vector**2)
# total_deviation_sq = 0
# for i in range(1, len(m_path_points_np) - 1):
#     point_spatial = m_path_points_np[i, 1:4]
#     # Project (point_spatial - m_start) onto path_vector
#     projection_factor = np.dot(point_spatial - m_start, path_vector) / path_length_sq
#     closest_point_on_line = m_start + projection_factor * path_vector
#     deviation_sq = np.sum((point_spatial - closest_point_on_line)**2)
#     total_deviation_sq += deviation_sq
# path_curvature_metric = np.sqrt(total_deviation_sq) / len(m_path_points_np) # Average deviation
""")

print("\nThis systematic analysis will help confirm or refine your insightful observation about the link between node degree and path curvature in your learned conceptual manifold!")

import nltk
from nltk.corpus import wordnet as wn
import torch
from transformers import BertTokenizer, BertModel # Assuming these are for future use or context
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from collections import defaultdict
import torch.autograd.functional as F # For Jacobian


# --- Ensure NLTK WordNet is available ---
try:
    wn.synsets('dog') # Try accessing a common synset
except LookupError:
    print("NLTK WordNet data not found. Downloading...")
    nltk.download('wordnet')
    nltk.download('omw-1.4') # For broader compatibility
    print("Download complete.")
except Exception as e:
    print(f"An error occurred with NLTK WordNet: {e}")

# --- 1. WordNet Concept and Hierarchy Extraction ---
print("\n--- Step 1: Extracting WordNet Concepts and Hierarchy ---")

def get_wordnet_subtree(start_synset_name='mammal.n.01', depth_limit=3):
    """
    Extracts concepts and hierarchy pairs from a WordNet subtree.
    Returns:
        tokens (list): List of unique concept names.
        hierarchy_pairs (list): List of (child_name, parent_name) tuples.
        concept_details (dict): Details about each concept (synset, depth).
    """
    try:
        root_synset = wn.synset(start_synset_name)
    except Exception as e:
        print(f"Error: Could not find synset '{start_synset_name}'. Please check the name. ({e})")
        return [], [], {}

    concepts_set = set()
    hierarchy_pairs_set = set()
    concept_details = {} # To store synset and depth

    queue = [(root_synset, 0, None)] # (synset, depth, parent_name)
    visited_synsets = set()

    while queue:
        current_synset, depth, parent_lemma_name = queue.pop(0)

        if current_synset in visited_synsets or depth > depth_limit:
            continue
        visited_synsets.add(current_synset)

        # Use the first lemma name as the concept identifier
        current_lemma_name = current_synset.lemmas()[0].name().replace('_', ' ') if current_synset.lemmas() else None
        if not current_lemma_name: # Should not happen for valid synsets
            continue

        concepts_set.add(current_lemma_name)
        concept_details[current_lemma_name] = {'synset': current_synset, 'depth': depth}


        if parent_lemma_name:
            hierarchy_pairs_set.add((current_lemma_name, parent_lemma_name))

        for hyponym in current_synset.hyponyms():
            if depth + 1 <= depth_limit:
                queue.append((hyponym, depth + 1, current_lemma_name))

    # Add the root of the extraction as a root in hierarchy_pairs if it has no listed parent
    root_token_name = root_synset.lemmas()[0].name().replace('_', ' ')
    is_root_explicit = any(p is None for c, p in hierarchy_pairs_set if c == root_token_name)
    if not is_root_explicit and root_token_name in concepts_set:
         # Check if it's a parent to any other node, if not, it might be an isolated start
        is_a_parent = any(p == root_token_name for c, p in hierarchy_pairs_set)
        if not is_a_parent and depth_limit == 0: # Single node extraction
             hierarchy_pairs_set.add((root_token_name, None))
        elif not any(c == root_token_name and p is not None for c,p in hierarchy_pairs_set): # Not a child of anything IN THE SUBTREE
             hierarchy_pairs_set.add((root_token_name, None))


    tokens_list = sorted(list(concepts_set))
    hierarchy_list = sorted(list(hierarchy_pairs_set))

    # Ensure all nodes in hierarchy_pairs are in tokens_list
    valid_hierarchy = []
    final_tokens_set = set(tokens_list)
    for child, parent in hierarchy_list:
        if child in final_tokens_set and (parent is None or parent in final_tokens_set):
            valid_hierarchy.append((child, parent))
        elif child in final_tokens_set and parent is not None and parent not in final_tokens_set:
            # If parent is outside the extracted depth, make child a root of this subtree
            # This can happen if depth_limit cuts off the true root's parent.
            # For simplicity, we'll just ensure the pair is valid within the token set.
            # A more robust handling might identify multiple roots or adjust depths.
            pass # Or add (child, None) if it has no other parents in the set

    # Ensure there's at least one root (None parent) if the hierarchy is not empty
    if valid_hierarchy and not any(p is None for c, p in valid_hierarchy):
        # Find highest level nodes (min depth) among those not having parents in the list
        current_children = {c for c,p in valid_hierarchy if p is not None}
        potential_roots = [t for t in final_tokens_set if t not in current_children]
        if potential_roots:
            # Choose one, e.g., the one from the start_synset if it's among them
            if root_token_name in potential_roots:
                 valid_hierarchy.append((root_token_name, None))
            elif potential_roots: # Or just the first potential root
                 valid_hierarchy.append((potential_roots[0], None))
        elif final_tokens_set: # Fallback if complex structure without clear root
            valid_hierarchy.append((list(final_tokens_set)[0], None))


    return tokens_list, valid_hierarchy, concept_details


# --- Parameters for WordNet Subtree ---
WN_START_SYNSET = 'mammal.n.01' # A good starting point
WN_DEPTH_LIMIT = 2           # Limits the size of the hierarchy

tokens_wn, hierarchy_pairs_wn, concepts_details_wn = get_wordnet_subtree(
    start_synset_name=WN_START_SYNSET,
    depth_limit=WN_DEPTH_LIMIT
)

if not tokens_wn or not hierarchy_pairs_wn:
    print(f"Could not extract a valid hierarchy from WordNet starting with '{WN_START_SYNSET}' at depth {WN_DEPTH_LIMIT}.")
    print("Skipping further WordNet-based steps.")
else:
    print(f"Extracted {len(tokens_wn)} concepts and {len(hierarchy_pairs_wn)} hierarchy pairs from WordNet.")
    print("Sample tokens:", tokens_wn[:10])
    print("Sample hierarchy pairs:", hierarchy_pairs_wn[:5])

    # --- 2. BERT Embeddings (Optional, not directly used by current INN phi input) ---
    # You might use these for initializing m_coords via PCA if desired, or for other analyses.
    # For now, train_inn_phi_... initializes m_coords randomly.
    print("\n--- Step 2: Generating BERT Embeddings (for context/potential future use) ---")
    # BERT_MODEL_NAME = 'bert-base-uncased' # Assuming defined earlier
    # tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)
    # bert_model = BertModel.from_pretrained(BERT_MODEL_NAME)
    # bert_model.eval()
    # concept_bert_embeddings_wn = {
    #     name: get_bert_embedding(name, bert_model, tokenizer) for name in tokens_wn
    # }
    # print(f"Generated BERT embeddings for {len(concept_bert_embeddings_wn)} WordNet concepts.")


    # --- 3. Train INN Phi with WordNet Data ---
    print("\n--- Step 3: Training INN Phi Network with WordNet Data ---")
    inn_phi_wn = INNPhi(input_dim=4, hidden_dim=128, num_coupling_layers=6) # Adjust architecture as needed



# initial_m_coords_reg_v4, final_e_coords_reg_v4, trained_inn_phi_reg_v4 = train_inn_phi_embedding_with_metric_reg_v4(
#     tokens_inn_reg_v4, hierarchy_pairs_inn_reg_v4,
#     inn_phi_network_reg_v4,
#     num_epochs=2000,        # Increased epochs for finer tuning
#     learning_rate=1e-5,     # Very small LR
#     epsilon1_target_sq=1e-6,
#     causal_margin=0.001,

#     g_tt_target = -1.0,
#     g_spatial_diag_target = 1.0,
#     g_off_diag_target = 0.0,

#     causal_loss_weight = 1.0,  # Keep primary losses strong
#     interval_loss_weight = 1.0,
#     # if loss good but gii still positive
#     g_tt_mse_weight = 1e-3,                 # slight increase
#     g_tt_sign_penalty_weight = 5e-2,  # Gruadually increase to 1.0
#     # If spatial g ii are often negative
#     g_spatial_diag_mse_weight = 1e-3,       # push towards 1.0
#     g_spatial_positivity_penalty_weight = 1e-2, # gruadually increase 2.0

#     g_off_diag_mse_weight = 1e-4,     # If off-diagonal terms are large: Gradually increase g_off_diag_mse_weight

#     gradient_clip_value = 1.0,
#     log_interval = 100 # Print logs every 100 epochs
# )



    # Use the v4 training function with carefully chosen regularization weights
    initial_m_coords_wn, final_e_coords_wn, trained_inn_phi_wn = train_inn_phi_embedding_with_metric_reg_v4(
        tokens_wn, hierarchy_pairs_wn,
        inn_phi_wn,
        num_epochs=1000,        # Adjust based on convergence
        learning_rate=1e-5,
        epsilon1_target_sq=1e-6,
        causal_margin=0.001,
        g_tt_target = -1.0,
        g_spatial_diag_target = 1.0,
        g_off_diag_target = 0.0,
        causal_loss_weight = 1.0,
        interval_loss_weight = 1.0,
        g_tt_mse_weight = 1e-3, # Start with moderate weights
        g_tt_sign_penalty_weight = 0.1,
        g_spatial_diag_mse_weight = 1e-3,
        g_spatial_positivity_penalty_weight = 1e-2,
        g_off_diag_mse_weight = 1e-4,
        gradient_clip_value = 1.0,
        log_interval = 100
    )

    # --- 4. Analysis of Learned Geometry for WordNet ---
    # Calculate final m* coordinates
    final_m_coords_wn_map = {}
    if final_e_coords_wn:
        print("\nCalculating final WordNet m_coords on manifold using INN inverse...")
        device_wn_analysis = next(trained_inn_phi_wn.parameters()).device
        for token_id, e_final_np in final_e_coords_wn.items():
            e_final_tensor = torch.tensor(e_final_np, dtype=torch.float32).to(device_wn_analysis)
            with torch.no_grad():
                m_star_tensor = trained_inn_phi_wn(e_final_tensor.unsqueeze(0), reverse=True).squeeze(0)
            final_m_coords_wn_map[token_id] = m_star_tensor.cpu().numpy()
        print("Finished calculating final WordNet m_coords.")

    # Calculate Node Degrees
    in_degree_wn = defaultdict(int)
    out_degree_wn = defaultdict(int)
    total_degree_wn = defaultdict(int)
    for child, parent in hierarchy_pairs_wn:
        if parent is not None:
            if child in tokens_wn and parent in tokens_wn:
                out_degree_wn[parent] += 1
                in_degree_wn[child] += 1
                total_degree_wn[parent] += 1
                total_degree_wn[child] += 1
    for token in tokens_wn: _ = total_degree_wn[token] # Ensure all have entry

    # Create Pandas DataFrame
    data_for_df_wn = []
    if final_m_coords_wn_map:
        eta_E_torch_wn = torch.tensor(np.diag([-1.0, 1.0, 1.0, 1.0]), dtype=torch.float32).to(device_wn_analysis)
        for token_id, m_star_coord_np in final_m_coords_wn_map.items():
            try:
                geo_features = analyze_local_geometry(m_star_coord_np, trained_inn_phi_wn, eta_E_torch_wn)
                data_for_df_wn.append({
                    'token': token_id,
                    'm_t': m_star_coord_np[0], 'm_x': m_star_coord_np[1], 'm_y': m_star_coord_np[2], 'm_z': m_star_coord_np[3],
                    'g_tt': geo_features['g_tt'], 'g_xx': geo_features['g_M'][1,1],
                    'g_yy': geo_features['g_M'][2,2], 'g_zz': geo_features['g_M'][3,3],
                    'avg_g_spatial_diag': geo_features['avg_spatial_diagonal_scaling'],
                    'det_g_M': geo_features['det_g_M'],
                    'time_space_mix': geo_features['time_space_mixing_abs_sum_g0k'],
                    'depth': concepts_details_wn.get(token_id, {}).get('depth', -1),
                    'in_degree': in_degree_wn[token_id], 'out_degree': out_degree_wn[token_id],
                    'total_degree': total_degree_wn[token_id]
                })
            except Exception as e:
                print(f"Could not analyze geometry for WN DataFrame for '{token_id}': {e}")

    df_analysis_wn = pd.DataFrame(data_for_df_wn)
    if not df_analysis_wn.empty:
        print("\nWordNet Analysis DataFrame (sample):")
        print(df_analysis_wn.sample(min(5, len(df_analysis_wn))))

        # Plot m_x vs g_tt for WordNet
        plt.figure(figsize=(10, 6))
        sns.scatterplot(data=df_analysis_wn, x='m_x', y='g_tt', hue='depth', size='total_degree', palette='viridis', sizes=(50,250))
        plt.title('WordNet: Conceptual $m_x$ vs. $g_{tt}$ (Color by Depth, Size by Degree)')
        plt.xlabel('Conceptual X-coordinate ($m_x$) on Manifold $\mathcal{M}$')
        plt.ylabel('$g_{tt}$ component of Pullback Metric $g_{\mathcal{M}}$')
        plt.axhline(-1.0, color='gray', linestyle='--', label='$g_{tt} = -1.0$ (Target)')
        plt.grid(True, linestyle=':', alpha=0.7)
        plt.legend(title='Depth / Degree')
        plt.show()

        # Plot Path Curvature vs. Degrees for WordNet
        path_curvature_data_wn = []
        for parent_id, child_id in hierarchy_pairs_wn:
            if parent_id is None or child_id not in final_e_coords_wn or parent_id not in final_e_coords_wn:
                continue
            e_parent = torch.tensor(final_e_coords_wn[parent_id], dtype=torch.float32, device=device_wn_analysis)
            e_child = torch.tensor(final_e_coords_wn[child_id], dtype=torch.float32, device=device_wn_analysis)
            e_path_points_tensor = torch.stack([e_parent + (e_child - e_parent) * t for t in np.linspace(0, 1, 30)])
            with torch.no_grad():
                m_path_points_tensor = trained_inn_phi_wn(e_path_points_tensor, reverse=True)
            curvature_metric = calculate_spatial_path_deviation(m_path_points_tensor.cpu().numpy())
            path_curvature_data_wn.append({
                'parent': parent_id, 'child': child_id, 'curvature_metric': curvature_metric,
                'avg_degree_of_pair': (total_degree_wn[parent_id] + total_degree_wn[child_id]) / 2.0,
                'max_degree_of_pair': max(total_degree_wn[parent_id], total_degree_wn[child_id])
            })
        df_paths_wn = pd.DataFrame(path_curvature_data_wn)
        if not df_paths_wn.empty:
            plt.figure(figsize=(12, 6))
            plt.subplot(1, 2, 1)
            sns.scatterplot(data=df_paths_wn, x='avg_degree_of_pair', y='curvature_metric')
            sns.regplot(data=df_paths_wn, x='avg_degree_of_pair', y='curvature_metric', scatter=False, color='red', order=2)
            plt.title('WN: Path Curvature vs. Avg Degree')
            plt.xlabel('Average Total Degree'); plt.ylabel('Path Curvature Metric')
            plt.grid(True, linestyle=':', alpha=0.7)

            plt.subplot(1, 2, 2)
            sns.scatterplot(data=df_paths_wn, x='max_degree_of_pair', y='curvature_metric')
            sns.regplot(data=df_paths_wn, x='max_degree_of_pair', y='curvature_metric', scatter=False, color='blue', order=2)
            plt.title('WN: Path Curvature vs. Max Degree')
            plt.xlabel('Maximum Total Degree'); plt.ylabel('Path Curvature Metric')
            plt.grid(True, linestyle=':', alpha=0.7)
            plt.tight_layout(); plt.show()

            print("\nCorrelations with WordNet Path Curvature Metric:")
            if len(df_paths_wn) > 1:
                print(f"  vs. Avg Degree: {df_paths_wn['curvature_metric'].corr(df_paths_wn['avg_degree_of_pair']):.4f}")
                print(f"  vs. Max Degree: {df_paths_wn['curvature_metric'].corr(df_paths_wn['max_degree_of_pair']):.4f}")

    # Visualize the Learned WordNet Conceptual Manifold with Paths

"""Interpretation: This strongly supports your initial hypothesis that "total degrees around points correlates with more curvature." More specifically, it suggests that paths involving moderately connected hubs (concepts that are neither too peripheral nor globally central like 'entity.n.01' would be if included) experience the most "bending" or distortion in the learned conceptual manifold.
Low Degree Pairs: Links between concepts with few connections (e.g., two leaf nodes, or a leaf and its direct parent) tend to be "straighter" on the manifold. This makes sense, as the phi doesnt need as much  space to accomadate them in Ecludiean space. Very High Degree Pairs (if present and distinct from moderate): The downturn at the high end of average degree could imply that extremely broad concepts (if your subtree captures them and they are distinct from the "peak" hubs) might also have less "local path bending" perhaps because their influence is so pervasive that the geometry around them is more uniformly scaled, or they are mapped further out in Ecludiean space.
"""

if final_m_coords_wn_map and final_e_coords_wn and not df_paths_wn.empty:
        plot_m_embedding_paths_colored_by_curvature(
            m_coords_map_plot=final_e_coords_wn,
            path_data_df = df_paths_wn,
            final_e_coords_map_plot=final_m_coords_wn_map,
            phi_inverse_model_plot= trained_inn_phi_wn,

            title="Learned WordNet Conceptual Manifold ($m^*$) - Paths Colored by Spatial Deviation"
        )
else:
    print("Cannot plot WordNet manifold with paths as necessary data is missing.")

import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from collections import defaultdict

# --- Ensure necessary variables are defined from previous WordNet cells ---
# trained_inn_phi_wn
# final_e_coords_wn
# final_m_coords_wn_map
# hierarchy_pairs_wn
# df_paths_wn
# plot_m_embedding_paths_colored_by_curvature (function)
# tokens_wn (list of all tokens in the WordNet subset)

# trained_inn_phi_wn = trained_inn_phi_wn
# final_e_coords_wn = final_e_coords_wn
# final_m_coords_wn_map = final_m_coords_wn_map
# hierarchy_pairs_wn = hierarchy_pairs_wn
# df_paths_wn = df_paths_wn
# plot_m_embedding_paths_colored_by_curvature = plot_m_embedding_paths_colored_by_curvature
# tokens_wn = tokens_wn

# For demonstration, if the full WordNet setup wasn't run, create minimal dummy data
if 'trained_inn_phi_wn' not in locals() or 'final_m_coords_wn_map' not in locals() or 'df_paths_wn' not in locals():
    print("Warning: WordNet specific trained model/data not found. Using generic dummy data for plotting.")
    # Use the dummy phi and data from the previous path curvature cell if needed
    if 'trained_inn_phi' not in locals(): # If even the generic dummy is not there
        class DummyPhi(torch.nn.Module):
            def __init__(self): super().__init__(); self.fc = torch.nn.Linear(4,4)
            def forward(self, x, reverse=False): return self.fc(x) + 0.1 * torch.sin(x) if not reverse else x - 0.1*torch.sin(x)
        trained_inn_phi_wn = DummyPhi()
    else:
        trained_inn_phi_wn = trained_inn_phi # Fallback to generic if WN specific one isn't there

    if 'final_m_coords_on_manifold_map' in locals() and final_m_coords_on_manifold_map:
         final_m_coords_wn_map = final_m_coords_on_manifold_map
         final_e_coords_wn = {k: trained_inn_phi_wn(torch.tensor(v, dtype=torch.float32).unsqueeze(0)).squeeze(0).detach().numpy() for k,v in final_m_coords_wn_map.items()} # Reconstruct e_coords
    else: # Absolute fallback
        _tokens_dummy_wn = ['mammal', 'placental', 'carnivore', 'canine', 'dog', 'cat']
        final_e_coords_wn = {
            'mammal': np.array([0,0,0,0]), 'placental': np.array([0.1,0.5,0,0]),
            'carnivore': np.array([0.2,0.5,0.5,0]), 'canine': np.array([0.3,1,0.5,0]),
            'dog': np.array([0.4,1,0.2,0]), 'cat': np.array([0.3, 0.2, 0.8, 0])
        }
        with torch.no_grad():
            final_m_coords_wn_map = {
                k: trained_inn_phi_wn(torch.tensor(v, dtype=torch.float32).unsqueeze(0), reverse=True).squeeze(0).numpy()
                for k,v in final_e_coords_wn.items()
            }
    if 'hierarchy_pairs_wn' not in locals() or not hierarchy_pairs_wn:
        hierarchy_pairs_wn = [('placental','mammal'), ('carnivore','placental'), ('canine','carnivore'), ('dog','canine'), ('cat','placental')]
    if 'df_paths_wn' not in locals() or df_paths_wn.empty:
        _dummy_path_data = [{'parent': p, 'child': c, 'curvature_metric': np.random.rand()*0.0005} for c,p in hierarchy_pairs_wn if p is not None]
        df_paths_wn = pd.DataFrame(_dummy_path_data)
    if 'tokens_wn' not in locals() or not tokens_wn:
        tokens_wn = list(final_m_coords_wn_map.keys())


print("\n--- Visualizing WordNet Conceptual Manifold with Paths Colored by Curvature ---")

# Select a subset of concepts and their connecting paths for clarity
# This depends on your WN_START_SYNSET and WN_DEPTH_LIMIT
# Let's try to pick a few interesting branches if 'mammal.n.01' depth 2 was used.
# Example: mammal -> placental -> carnivore -> canine -> dog
#          mammal -> placental -> feline -> cat (if feline/cat exist)
#          mammal -> marsupial -> opossum (if they exist)

# Create a set of all valid nodes for quick lookup
valid_nodes_in_plot = set(final_m_coords_wn_map.keys())

# Define specific paths you want to highlight.
# These should be actual parent->child links present in hierarchy_pairs_wn
# And both parent and child must be in final_m_coords_wn_map
paths_to_highlight_wn = []
candidate_paths = [
    ('mammal', 'placental mammal'), # WordNet often uses "placental mammal"
    ('placental mammal', 'carnivore'),
    ('carnivore', 'canine'),
    ('canine', 'dog'),
    ('carnivore', 'feline'), # If 'feline' was extracted
    ('feline', 'cat'),      # If 'cat' was extracted as child of feline
    ('mammal', 'marsupial'), # If 'marsupial' was extracted
    ('marsupial', 'opossum') # If 'opossum' was extracted
]
# A more general way if the exact names above are not in your tokens_wn:
# Pick some paths from df_paths_wn, e.g., ones with high/low curvature or involving hubs.
if not df_paths_wn.empty:
    # Pick some paths from those for which curvature was calculated
    # For example, the top 3 most curved and bottom 3 least curved (if enough paths exist)
    df_paths_wn_sorted = df_paths_wn.sort_values(by='curvature_metric', ascending=False)
    highlight_from_df = []
    if len(df_paths_wn_sorted) >= 1:
        highlight_from_df.extend(list(zip(df_paths_wn_sorted['parent'].head(3), df_paths_wn_sorted['child'].head(3))))
        highlight_from_df.extend(list(zip(df_paths_wn_sorted['parent'].tail(3), df_paths_wn_sorted['child'].tail(3))))
    paths_to_highlight_wn = list(set(highlight_from_df)) # Unique paths
else: # Fallback if df_paths_wn is empty
    # Try to find a few valid paths from hierarchy_pairs_wn
    count = 0
    for p, c in hierarchy_pairs_wn:
        if p and c and p in valid_nodes_in_plot and c in valid_nodes_in_plot:
            paths_to_highlight_wn.append((p,c))
            count += 1
            if count >= 5: break


# Filter paths_to_highlight_wn to ensure they are in df_paths_wn (so they have curvature_metric)
# and that both nodes are in the plot
if not df_paths_wn.empty:
    plotted_paths_df_subset = df_paths_wn[
        df_paths_wn.apply(lambda row: (row['parent'], row['child']) in paths_to_highlight_wn and \
                                    row['parent'] in valid_nodes_in_plot and \
                                    row['child'] in valid_nodes_in_plot, axis=1)
    ]
else:
    plotted_paths_df_subset = pd.DataFrame() # Empty df if no path data


# Decide which tokens to label to reduce clutter for larger graphs
tokens_to_label_wn = set()
if not plotted_paths_df_subset.empty:
    for _, row in plotted_paths_df_subset.iterrows():
        tokens_to_label_wn.add(row['parent'])
        tokens_to_label_wn.add(row['child'])
else: # If no specific paths, label a general subset
    tokens_to_label_wn = set(tokens_wn[:15]) # Label first 15 or so

# Call the plotting function
if final_m_coords_wn_map and final_e_coords_wn and not plotted_paths_df_subset.empty:
    # Redefine plot_m_embedding_paths_colored_by_curvature to accept tokens_to_label
    def plot_m_embedding_paths_colored_by_curvature_labeled(
        m_coords_map_plot,
        path_data_df,
        tokens_to_label,
        title="Conceptual Manifold - Paths Colored by Curvature"
    ):
        fig = plt.figure(figsize=(16, 14)) # Even larger for WordNet
        ax = fig.add_subplot(111, projection='3d')

        points_list = []
        labels_all = [] # Store all labels for node plotting
        for token_id, m_coords_val in m_coords_map_plot.items():
            points_list.append(m_coords_val)
            labels_all.append(token_id)
        points = np.array(points_list)

        # Node scatter plot
        node_colors = [concepts_details_wn.get(token, {}).get('depth', 0) for token in labels_all] # Color by depth
        node_scatter = ax.scatter(points[:, 1], points[:, 2], points[:, 0],
                                  c=node_colors, cmap='cool', s=80, alpha=0.7, edgecolors='grey', linewidth=0.1)
        cbar_nodes = fig.colorbar(node_scatter, label='WordNet Depth', shrink=0.5, aspect=15, pad=0.15)


        ax.set_xlabel('Conceptual X ($m_x$)')
        ax.set_ylabel('Conceptual Y ($m_y$)')
        ax.set_zlabel('Conceptual Time ($m_t$)')

        for i, txt in enumerate(labels_all):
            if txt in tokens_to_label: # Only label selected tokens
                ax.text(points[i, 1], points[i, 2], points[i, 0], f' {txt}', size=9, zorder=10, color='black')

        if not path_data_df.empty and 'curvature_metric' in path_data_df.columns:
            curvatures = path_data_df['curvature_metric'].values
            if len(curvatures) > 0 and not np.all(np.isnan(curvatures)):
                 # Handle potential NaNs or single value for normalization
                min_curv = np.nanmin(curvatures)
                max_curv = np.nanmax(curvatures)
                if min_curv == max_curv: # Avoid division by zero if all curvatures are same
                    norm = plt.Normalize(vmin=min_curv - 0.00001, vmax=max_curv + 0.00001)
                else:
                    norm = plt.Normalize(vmin=min_curv, vmax=max_curv)
                cmap_paths = plt.cm.get_cmap('autumn_r') # Use a different cmap for paths

                num_path_interpolation_points_plot = 30
                device_plot = next(trained_inn_phi_wn.parameters()).device

                for idx_path, row_path in path_data_df.iterrows():
                    parent_id_p = row_path['parent']
                    child_id_p = row_path['child']
                    path_curvature_val = row_path['curvature_metric']

                    if parent_id_p in final_e_coords_wn and child_id_p in final_e_coords_wn:
                        e_parent = torch.tensor(final_e_coords_wn[parent_id_p], dtype=torch.float32, device=device_plot)
                        e_child = torch.tensor(final_e_coords_wn[child_id_p], dtype=torch.float32, device=device_plot)

                        e_path_pts = torch.stack([e_parent + (e_child - e_parent) * t
                                                     for t in np.linspace(0, 1, num_path_interpolation_points_plot)])
                        with torch.no_grad():
                            m_path_pts_tensor = trained_inn_phi_wn(e_path_pts, reverse=True)
                        m_path_pts_np = m_path_pts_tensor.cpu().numpy()

                        ax.plot(m_path_pts_np[:, 1], m_path_pts_np[:, 2], m_path_pts_np[:, 0],
                                linewidth=1.0 + path_curvature_val * 5000, # Scale linewidth by curvature
                                alpha=0.8,
                                color=cmap_paths(norm(path_curvature_val)),
                                zorder=5) # Plot paths under nodes for clarity if nodes are opaque

                sm_paths = plt.cm.ScalarMappable(cmap=cmap_paths, norm=norm)
                sm_paths.set_array([])
                cbar_paths = fig.colorbar(sm_paths, ax=ax, label='Path Spatial Deviation', shrink=0.5, aspect=15, pad=0.02)
            else:
                print("Path curvature values are NaN or singular, skipping path coloring.")


        ax.set_title(title)
        plt.tight_layout()
        plt.show()

    plot_m_embedding_paths_colored_by_curvature_labeled(
        final_m_coords_wn_map,
        plotted_paths_df_subset, # Use the subset of paths with curvature data
        tokens_to_label_wn,
        title="WordNet Conceptual Manifold ($m^*$) - Paths Colored by Spatial Deviation"
    )
else:
    print("Cannot plot WordNet manifold with paths as necessary data (m_coords, e_coords, or path_df) is missing.")

import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns # For easier color mapping in some cases

# --- Ensure necessary variables are defined from previous WordNet cells ---
# For demonstration, if not fully run, create minimal dummy data
if 'trained_inn_phi_wn' not in locals() or \
   'final_m_coords_wn_map' not in locals() or \
   'df_analysis_wn' not in locals() or df_analysis_wn.empty or \
   'df_paths_wn' not in locals() or df_paths_wn.empty or \
   'hierarchy_pairs_wn' not in locals() or \
   'final_e_coords_wn' not in locals():

    print("Warning: One or more required WordNet variables not found. Using dummy data for plotting.")
    class DummyPhi(torch.nn.Module):
        def __init__(self): super().__init__(); self.fc = torch.nn.Linear(4,4)
        def forward(self, x, reverse=False): return self.fc(x) + 0.1 * torch.sin(x) if not reverse else x - 0.1*torch.sin(x)
    trained_inn_phi_wn = DummyPhi()
    _tokens_dummy_wn = ['mammal', 'placental', 'carnivore', 'canine', 'dog', 'cat', 'rodent']

    final_e_coords_wn = { # Dummy e_coords
        t: np.array([i*0.05] + (np.random.rand(3) * 0.2 - 0.1).tolist())
        for i, t in enumerate(_tokens_dummy_wn)
    }
    with torch.no_grad(): # Dummy m_coords from dummy e_coords
        final_m_coords_wn_map = {
            k: trained_inn_phi_wn(torch.tensor(v, dtype=torch.float32).unsqueeze(0), reverse=True).squeeze(0).numpy()
            for k,v in final_e_coords_wn.items()
        }
    if 'hierarchy_pairs_wn' not in locals() or not hierarchy_pairs_wn:
        hierarchy_pairs_wn = [('placental','mammal'), ('carnivore','placental'), ('canine','carnivore'),
                              ('dog','canine'), ('cat','placental'), ('rodent', 'placental')]

    if 'df_analysis_wn' not in locals() or df_analysis_wn.empty:
        _dummy_analysis_data = []
        for t in _tokens_dummy_wn:
            _dummy_analysis_data.append({
                'token': t, 'm_t': final_m_coords_wn_map[t][0], 'm_x': final_m_coords_wn_map[t][1],
                'm_y': final_m_coords_wn_map[t][2], 'm_z': final_m_coords_wn_map[t][3],
                'g_tt': -0.8 - np.random.rand()*0.4,
                'avg_g_spatial_diag': 0.8 + np.random.rand()*0.4,
                'depth': np.random.randint(0,3),
                'total_degree': np.random.randint(1,4)
            })
        df_analysis_wn = pd.DataFrame(_dummy_analysis_data)

    if 'df_paths_wn' not in locals() or df_paths_wn.empty:
        _dummy_path_data = []
        for p,c in hierarchy_pairs_wn:
            if p is not None:
                 _dummy_path_data.append({'parent': p, 'child': c, 'curvature_metric': np.random.rand()*0.0005})
        df_paths_wn = pd.DataFrame(_dummy_path_data)

# Helper function for consistent 3D scatter plots with annotations and links
def plot_3d_manifold(m_coords_map, hierarchy_pairs,
                     color_values=None, color_label="Feature Value", cmap='viridis',
                     title="Conceptual Manifold", point_size=100, alpha=0.8,
                     tokens_to_label=None, label_size=8):
    fig = plt.figure(figsize=(14, 11))
    ax = fig.add_subplot(111, projection='3d')

    token_ids = list(m_coords_map.keys())
    points = np.array([m_coords_map[tid] for tid in token_ids])

    if color_values is None: # Default to m_z if no specific color values provided
        color_values_plot = points[:, 3]
        actual_color_label = "Conceptual Z (m_z)"
    else:
        color_values_plot = color_values
        actual_color_label = color_label

    # Handle potential NaNs in color_values for normalization
    valid_color_values = color_values_plot[~np.isnan(color_values_plot)]
    if len(valid_color_values) == 0: # All NaNs or empty
        norm = None # No normalization if all are NaN
        colors_for_scatter = 'blue' # Default color
    else:
        min_val = np.min(valid_color_values)
        max_val = np.max(valid_color_values)
        if min_val == max_val:
            norm = plt.Normalize(vmin=min_val - 0.1, vmax=max_val + 0.1)
        else:
            norm = plt.Normalize(vmin=min_val, vmax=max_val)
        colors_for_scatter = cmap if norm else 'blue'


    scatter = ax.scatter(points[:, 1], points[:, 2], points[:, 0],
                         c=color_values_plot, cmap=colors_for_scatter, norm=norm,
                         s=point_size, alpha=alpha, edgecolors='grey', linewidth=0.5)

    if norm : # Only add colorbar if normalization was possible
        cbar = fig.colorbar(scatter, label=actual_color_label, shrink=0.6, aspect=20, pad=0.1)

    ax.set_xlabel('Conceptual X ($m_x$)')
    ax.set_ylabel('Conceptual Y ($m_y$)')
    ax.set_zlabel('Conceptual Time ($m_t$)')

    # Auto-adjust axis limits for better visibility
    if len(points) > 0:
        min_coords_plot = points[:, :3].min(axis=0)
        max_coords_plot = points[:, :3].max(axis=0)
        ax.auto_scale_xyz([min_coords_plot[1],max_coords_plot[1]],
                          [min_coords_plot[2],max_coords_plot[2]],
                          [min_coords_plot[0],max_coords_plot[0]])


    if tokens_to_label is None: # If None, label all
        tokens_to_label = set(token_ids)

    for i, token_id in enumerate(token_ids):
        if token_id in tokens_to_label:
            ax.text(points[i, 1], points[i, 2], points[i, 0], f' {token_id}', size=label_size, zorder=10, color='black')

    for child_id, parent_id in hierarchy_pairs:
        if parent_id is None or child_id not in m_coords_map or parent_id not in m_coords_map:
            continue
        p_coords = m_coords_map[parent_id]
        c_coords = m_coords_map[child_id]
        ax.plot([p_coords[1], c_coords[1]], [p_coords[2], c_coords[2]], [p_coords[0], c_coords[0]],
                color='gray', linestyle=':', alpha=0.4, linewidth=0.7, zorder=1)

    ax.set_title(title, fontsize=16)
    plt.tight_layout()
    plt.show()

# --- 1. Visualize Nodes Colored by Different Features ---
if not df_analysis_wn.empty:
    # Plot 1: Color by WordNet Depth
    plot_3d_manifold(final_m_coords_wn_map, hierarchy_pairs_wn,
                     color_values=df_analysis_wn.set_index('token')['depth'].reindex(final_m_coords_wn_map.keys()).values,
                     color_label="WordNet Depth", cmap='cool',
                     title="Learned WordNet Manifold ($m^*$) - Nodes Colored by Depth",
                     tokens_to_label=set(tokens_wn)) # Label all for WordNet if not too many

    # Plot 2: Color by Total Node Degree
    plot_3d_manifold(final_m_coords_wn_map, hierarchy_pairs_wn,
                     color_values=df_analysis_wn.set_index('token')['total_degree'].reindex(final_m_coords_wn_map.keys()).values,
                     color_label="Total Node Degree", cmap='plasma',
                     title="Learned WordNet Manifold ($m^*$) - Nodes Colored by Total Degree",
                     tokens_to_label=set(tokens_wn))

    # Plot 3: Color by g_tt
    plot_3d_manifold(final_m_coords_wn_map, hierarchy_pairs_wn,
                     color_values=df_analysis_wn.set_index('token')['g_tt'].reindex(final_m_coords_wn_map.keys()).values,
                     color_label="$g_{tt}$ (Time Component)", cmap='plasma', # _r reverses coolwarm
                     title="Learned WordNet Manifold ($m^*$) - Nodes Colored by $g_{tt}$",
                     tokens_to_label=set(tokens_wn))

    # Plot 4: Color by Average Spatial Diagonal Scaling
    plot_3d_manifold(final_m_coords_wn_map, hierarchy_pairs_wn,
                     color_values=df_analysis_wn.set_index('token')['avg_g_spatial_diag'].reindex(final_m_coords_wn_map.keys()).values,
                     color_label="Avg. Spatial Diagonal ($g_{ii}$)", cmap='viridis',
                     title="Learned WordNet Manifold ($m^*$) - Nodes Colored by Avg. Spatial Scaling",
                     tokens_to_label=set(tokens_wn))
else:
    print("df_analysis_wn is empty. Skipping node feature visualizations.")


# --- 2. Visualize Selected Paths Colored by their Curvature Metric ---
# (Re-using the function from the previous response, ensure it's defined)
if 'plot_m_embedding_paths_colored_by_curvature_labeled' not in locals():
    # Define a fallback or simplified version if it's missing
    def plot_m_embedding_paths_colored_by_curvature_labeled(*args, **kwargs):
        print("Warning: `plot_m_embedding_paths_colored_by_curvature_labeled` not defined. Skipping detailed path plot.")
        # You could call the simpler plot_3d_manifold here as a fallback
        if args: plot_3d_manifold(args[0], hierarchy_pairs_wn, title=kwargs.get('title', "Manifold Plot"))


if final_m_coords_wn_map and final_e_coords_wn and not df_paths_wn.empty:
    # Determine tokens_to_label for this plot
    tokens_for_path_plot_labels = set()
    # Select a few paths with highest and lowest curvature for labeling clarity
    df_paths_wn_sorted_for_plot = df_paths_wn.sort_values(by='curvature_metric', ascending=False)
    paths_to_include_for_labeling = pd.concat([df_paths_wn_sorted_for_plot.head(3), df_paths_wn_sorted_for_plot.tail(3)])

    for _, row in paths_to_include_for_labeling.iterrows():
        if row['parent'] in final_m_coords_wn_map: tokens_for_path_plot_labels.add(row['parent'])
        if row['child'] in final_m_coords_wn_map: tokens_for_path_plot_labels.add(row['child'])

    # If still too few labels, add some general ones (e.g., roots or high-degree nodes)
    if len(tokens_for_path_plot_labels) < 5 and not df_analysis_wn.empty:
         # Add top 3 highest degree nodes if not already included
        top_degree_nodes = df_analysis_wn.sort_values(by='total_degree', ascending=False)['token'].head(3).tolist()
        for node in top_degree_nodes:
            if node in final_m_coords_wn_map: tokens_for_path_plot_labels.add(node)
            if len(tokens_for_path_plot_labels) >= 7: break # Limit total labels


    plot_m_embedding_paths_colored_by_curvature_labeled( # Assumes this function is correctly defined
        final_m_coords_wn_map,
        df_paths_wn, # Use all paths from df_paths_wn for coloring/thickness
        tokens_for_path_plot_labels, # Only label selected tokens
        # df_analysis_wn, # Pass this if the func uses it for node coloring by a feature
        # 'depth',        # Example: color nodes by depth
        title="WordNet Manifold ($m^*$) - Paths Colored by Spatial Deviation"
    )
else:
    print("Cannot plot WordNet manifold with paths colored by curvature as necessary data is missing.")

"""Super interesting how the curves go around the top word in the hierarchy. Like that word has "weight"
"""

import numpy as np
import pandas as pd
import torch
import torch.autograd.functional as F
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from collections import defaultdict
import seaborn as sns

# --- Ensure necessary variables and functions are defined from previous cells ---
# trained_inn_phi_wn, final_e_coords_wn, final_m_coords_wn_map, hierarchy_pairs_wn,
# df_analysis_wn, calculate_spatial_path_deviation, analyze_local_geometry
# eta_E_torch (should be defined and on the correct device as the model)

# For demonstration, if the full WordNet setup wasn't run, create minimal dummy data
if 'trained_inn_phi_wn' not in locals() or \
   'final_m_coords_wn_map' not in locals() or \
   'df_analysis_wn' not in locals() or df_analysis_wn.empty or \
   'hierarchy_pairs_wn' not in locals() or \
   'final_e_coords_wn' not in locals():

    print("Warning: One or more required WordNet variables not found. Using dummy data for plotting.")
    class DummyPhi(torch.nn.Module):
        def __init__(self): super().__init__(); self.fc = torch.nn.Linear(4,4)
        def forward(self, x, reverse=False): return self.fc(x) + 0.1 * torch.sin(x) if not reverse else x - 0.1*torch.sin(x)
    trained_inn_phi_wn = DummyPhi()
    device_wn_analysis = torch.device("cpu") # ensure device consistency for dummy
    trained_inn_phi_wn.to(device_wn_analysis)
    eta_E_torch = torch.tensor(np.diag([-1.0, 1.0, 1.0, 1.0]), dtype=torch.float32).to(device_wn_analysis)


    _tokens_dummy_wn = ['mammal', 'placental', 'carnivore', 'canine', 'dog', 'cat', 'rodent']
    final_e_coords_wn = {
        t: np.array([i*0.05] + (np.random.rand(3) * 0.2 - 0.1).tolist())
        for i, t in enumerate(_tokens_dummy_wn)
    }
    with torch.no_grad():
        final_m_coords_wn_map = {
            k: trained_inn_phi_wn(torch.tensor(v, dtype=torch.float32).unsqueeze(0).to(device_wn_analysis), reverse=True).squeeze(0).cpu().numpy()
            for k,v in final_e_coords_wn.items()
        }
    if 'hierarchy_pairs_wn' not in locals() or not hierarchy_pairs_wn:
        hierarchy_pairs_wn = [('placental','mammal'), ('carnivore','placental'), ('canine','carnivore'),
                              ('dog','canine'), ('cat','placental'), ('rodent', 'placental')]
    if 'df_analysis_wn' not in locals() or df_analysis_wn.empty:
        _dummy_analysis_data = []
        for t in _tokens_dummy_wn:
            # Dummy g_M components for analyze_local_geometry to work
            _g_M_dummy = np.diag([-0.9, 0.9, 0.9, 0.9]) + (np.random.rand(4,4)-0.5)*0.1
            _dummy_analysis_data.append({
                'token': t, 'm_t': final_m_coords_wn_map[t][0], 'm_x': final_m_coords_wn_map[t][1],
                'm_y': final_m_coords_wn_map[t][2], 'm_z': final_m_coords_wn_map[t][3],
                'g_tt': _g_M_dummy[0,0], 'g_xx': _g_M_dummy[1,1],
                'g_yy': _g_M_dummy[2,2], 'g_zz': _g_M_dummy[3,3],
                'avg_g_spatial_diag': np.mean(np.diag(_g_M_dummy[1:,1:])),
                'det_g_M': -0.9,
                'time_space_mix': np.sum(np.abs(_g_M_dummy[0,1:])),
                'depth': np.random.randint(0,3),
                'total_degree': np.random.randint(1,4)
            })
        df_analysis_wn = pd.DataFrame(_dummy_analysis_data)
    if 'df_paths_wn' not in locals() or df_paths_wn.empty: # df_paths_wn should be recreated
        df_paths_wn = pd.DataFrame() # Ensure it's an empty df if not properly generated

# Ensure analyze_local_geometry is defined (copied from your previous good response)
def analyze_local_geometry(m_coord_np, phi_network_model, eta_E):
    m_coord_torch = torch.tensor(m_coord_np, dtype=torch.float32).to(eta_E.device)
    def phi_for_jacobian(m_input_tensor):
        return phi_network_model(m_input_tensor.unsqueeze(0), reverse=False).squeeze(0)
    J_phi = F.jacobian(phi_for_jacobian, m_coord_torch)
    g_M_tensor = J_phi.T @ eta_E @ J_phi
    g_M_np = g_M_tensor.detach().cpu().numpy()
    det_g_M = np.linalg.det(g_M_np)
    g_tt = g_M_np[0, 0]
    time_distortion_factor_val = np.sqrt(np.abs(g_tt))
    avg_spatial_scaling = np.mean(np.diag(g_M_np[1:, 1:]))
    spatial_submatrix = g_M_np[1:, 1:]
    det_g_spatial = np.linalg.det(spatial_submatrix)
    spatial_volume_factor = np.sqrt(np.abs(det_g_spatial)) if (det_g_spatial > 0 or not np.isclose(det_g_spatial,0)) else 0.0
    time_space_mixing = (np.sum(np.abs(g_M_np[0, 1:])) + np.sum(np.abs(g_M_np[1:, 0]))) / 2
    return {
        "g_M": g_M_np, "det_g_M": det_g_M, "g_tt": g_tt,
        "time_distortion_sqrt_abs_g_tt": time_distortion_factor_val,
        "avg_spatial_diagonal_scaling": avg_spatial_scaling,
        "spatial_volume_sqrt_det_g_spatial": spatial_volume_factor,
        "time_space_mixing_abs_sum_g0k": time_space_mixing
    }


print("\n--- Step 1: Calculating Path Lengths and Re-calculating Spatial Deviation ---")

def calculate_m_path_length_and_deviation(m_path_points_np, phi_model_for_gM, eta_E_tensor_device, path_name=""): # Added path_name for debug
    """
    Calculates the approximate proper length of a path on manifold M
    and its spatial deviation from a straight line in M-coordinates.
    """
    if len(m_path_points_np) < 2:
        return 0.0, 0.0

    path_length_m_approx = 0.0
    # print(f"\nAnalyzing path: {path_name}") # Debug
    for i in range(len(m_path_points_np) - 1):
        m_start_segment = m_path_points_np[i]
        m_end_segment = m_path_points_np[i+1]
        delta_m_segment = m_end_segment - m_start_segment

        m_midpoint_segment = (m_start_segment + m_end_segment) / 2.0
        geo_features_mid = analyze_local_geometry(m_midpoint_segment, phi_model_for_gM, eta_E_tensor_device)
        g_M_mid = geo_features_mid['g_M']

        ds_sq_segment_m = delta_m_segment @ g_M_mid @ delta_m_segment.T

        # --- DEBUG PRINT ---
        # if i < 5 or i > len(m_path_points_np) - 6 : # Print for first/last 5 segments
        #     print(f"  Path {path_name} - Segment {i}: ds_sq_M = {ds_sq_segment_m:.3e}, "
        #           f"delta_m_t={delta_m_segment[0]:.2e}, "
        #           f"g_tt_mid={g_M_mid[0,0]:.2f}, "
        #           f"g_xx_mid={g_M_mid[1,1]:.2f}, g_yy_mid={g_M_mid[2,2]:.2f}, g_zz_mid={g_M_mid[3,3]:.2f}, "
        #           f"g_tx_mid={g_M_mid[0,1]:.2f}")
        # --- END DEBUG ---

        if ds_sq_segment_m < -1e-9: # Timelike segment on M (add small tolerance for strict negativity)
            path_length_m_approx += np.sqrt(-ds_sq_segment_m)
        # else: # Spacelike or null on M
            # If you want to sum magnitudes for spacelike paths too for some notion of "length"
            # path_length_m_approx += np.sqrt(np.abs(ds_sq_segment_m))
            # However, for "proper time," only timelike contributions count.
            # If consistently spacelike, it's a strong indicator of metric signature issues.

    spatial_deviation = calculate_spatial_path_deviation(m_path_points_np)

    return path_length_m_approx, spatial_deviation


path_analysis_data_wn = []
num_path_interp_pts = 30 # For discretizing path
device_for_analysis = next(trained_inn_phi_wn.parameters()).device
eta_E_torch_device = eta_E_torch.to(device_for_analysis)


if final_e_coords_wn and final_m_coords_wn_map:
    print("Calculating path lengths and deviations for WordNet links...")
    for parent_id, child_id in hierarchy_pairs_wn:
        if parent_id is None or child_id not in final_e_coords_wn or parent_id not in final_e_coords_wn:
            continue

        e_parent = torch.tensor(final_e_coords_wn[parent_id], dtype=torch.float32, device=device_for_analysis)
        e_child = torch.tensor(final_e_coords_wn[child_id], dtype=torch.float32, device=device_for_analysis)

        e_path_pts_tensor = torch.stack([e_parent + (e_child - e_parent) * t
                                         for t in np.linspace(0, 1, num_path_interp_pts)])
        with torch.no_grad():
            m_path_pts_tensor = trained_inn_phi_wn(e_path_pts_tensor, reverse=True)
        m_path_pts_np = m_path_pts_tensor.cpu().numpy()

        path_len_m, spatial_dev = calculate_m_path_length_and_deviation(m_path_pts_np, trained_inn_phi_wn, eta_E_torch_device)

        degree_parent = df_analysis_wn.set_index('token').loc[parent_id, 'total_degree'] if parent_id in df_analysis_wn.set_index('token').index else 0
        degree_child = df_analysis_wn.set_index('token').loc[child_id, 'total_degree'] if child_id in df_analysis_wn.set_index('token').index else 0

        path_analysis_data_wn.append({
            'parent': parent_id, 'child': child_id,
            'path_length_m': path_len_m,
            'spatial_deviation_m': spatial_dev, # This is our "curvature_metric"
            'avg_degree_of_pair': (degree_parent + degree_child) / 2.0,
            'max_degree_of_pair': max(degree_parent, degree_child),
            'parent_g_tt': df_analysis_wn.set_index('token').loc[parent_id, 'g_tt'] if parent_id in df_analysis_wn.set_index('token').index else np.nan,
            'child_g_tt': df_analysis_wn.set_index('token').loc[child_id, 'g_tt'] if child_id in df_analysis_wn.set_index('token').index else np.nan,
        })

    df_paths_analysis_wn = pd.DataFrame(path_analysis_data_wn)
    if not df_paths_analysis_wn.empty:
        print("\nPath Analysis Data for WordNet (first 5 rows):")
        print(df_paths_analysis_wn.head())

        # Update df_paths_wn if it was used by the plotting function, or use df_paths_analysis_wn
        df_paths_wn = df_paths_analysis_wn.rename(columns={'spatial_deviation_m': 'curvature_metric'})

    else:
        print("No path analysis data generated for WordNet.")
else:
    print("final_e_coords_wn or final_m_coords_wn_map is empty. Skipping path analysis.")

# --- 2. Enhanced Visualization: Paths colored by curvature, thickness by length ---

def plot_m_embedding_paths_len_curve(
    m_coords_map_plot,
    path_data_df, # DataFrame with parent, child, path_length_m, curvature_metric
    tokens_to_label,
    node_color_feature_df=None, # Optional: df_analysis_wn to color nodes by a feature
    node_color_by='depth',    # Feature name in node_color_feature_df
    title="Conceptual Manifold - Paths by Length & Curvature"
):
    fig = plt.figure(figsize=(16, 14))
    ax = fig.add_subplot(111, projection='3d')

    token_ids_plot = list(m_coords_map_plot.keys())
    points_plot = np.array([m_coords_map_plot[tid] for tid in token_ids_plot])

    # Node coloring
    node_feature_values = None
    if node_color_feature_df is not None and node_color_by in node_color_feature_df.columns:
        # Align feature values with the order of points_plot
        node_feature_values_series = node_color_feature_df.set_index('token')[node_color_by].reindex(token_ids_plot)
        node_feature_values = node_feature_values_series.values

    if node_feature_values is not None and not np.all(np.isnan(node_feature_values)):
        node_cmap = 'cool' if node_color_by == 'depth' else 'cividis'
        node_scatter = ax.scatter(points_plot[:, 1], points_plot[:, 2], points_plot[:, 0],
                                  c=node_feature_values, cmap=node_cmap, s=100, alpha=0.7, edgecolors='k', linewidth=0.5)
        fig.colorbar(node_scatter, label=f'Node: {node_color_by}', shrink=0.5, aspect=15, pad=0.15)
    else:
        ax.scatter(points_plot[:, 1], points_plot[:, 2], points_plot[:, 0], s=100, alpha=0.7, color='skyblue', edgecolors='k', linewidth=0.5)


    ax.set_xlabel('Conceptual X ($m_x$)')
    ax.set_ylabel('Conceptual Y ($m_y$)')
    ax.set_zlabel('Conceptual Time ($m_t$)')

    for i, txt in enumerate(token_ids_plot):
        if txt in tokens_to_label:
            ax.text(points_plot[i, 1], points_plot[i, 2], points_plot[i, 0], f' {txt}', size=9, zorder=10, color='black')

    # Path plotting
    if not path_data_df.empty and 'curvature_metric' in path_data_df.columns and 'path_length_m' in path_data_df.columns:
        curvatures = path_data_df['curvature_metric'].values
        lengths = path_data_df['path_length_m'].values

        # Normalize curvature for color
        norm_curvature = plt.Normalize(vmin=np.nanmin(curvatures) if not np.all(np.isnan(curvatures)) else 0,
                                       vmax=np.nanmax(curvatures) if not np.all(np.isnan(curvatures)) else 1)
        cmap_curvature = plt.cm.get_cmap('autumn_r')

        # Normalize length for linewidth (be careful with extreme values)
        min_len, max_len = np.nanmin(lengths), np.nanmax(lengths)
        if min_len == max_len : max_len = min_len + 1 # Avoid division by zero for norm

        num_path_interp_plot = 30
        path_device = next(trained_inn_phi_wn.parameters()).device

        for idx, row in path_data_df.iterrows():
            parent_id = row['parent']
            child_id = row['child']
            path_curv = row['curvature_metric']
            path_len = row['path_length_m']

            if parent_id in final_e_coords_wn and child_id in final_e_coords_wn:
                e_p = torch.tensor(final_e_coords_wn[parent_id], dtype=torch.float32, device=path_device)
                e_c = torch.tensor(final_e_coords_wn[child_id], dtype=torch.float32, device=path_device)

                e_path = torch.stack([e_p + (e_c - e_p) * t for t in np.linspace(0, 1, num_path_interp_plot)])
                with torch.no_grad():
                    m_path = trained_inn_phi_wn(e_path, reverse=True).cpu().numpy()

                # Scale linewidth: 0.5 to 3.0, for example
                linewidth = 0.5 + 2.5 * ( (path_len - min_len) / (max_len - min_len + 1e-9) ) if max_len > min_len else 1.5

                ax.plot(m_path[:, 1], m_path[:, 2], m_path[:, 0],
                        linewidth=max(0.5, min(linewidth, 3.5)), # Cap linewidth
                        alpha=0.8,
                        color=cmap_curvature(norm_curvature(path_curv)),
                        zorder=5)

        sm_curvature = plt.cm.ScalarMappable(cmap=cmap_curvature, norm=norm_curvature)
        sm_curvature.set_array([])
        fig.colorbar(sm_curvature, ax=ax, label='Path Spatial Deviation (Curvature)', shrink=0.5, aspect=15, pad=0.02)
        # Could add a pseudo-legend for linewidth if desired, but it's trickier

    ax.set_title(title, fontsize=16)
    plt.tight_layout()
    plt.show()

# --- Call the Enhanced Visualization ---
if final_m_coords_wn_map and final_e_coords_wn and not df_paths_wn.empty and not df_analysis_wn.empty:
    # Select tokens to label for clarity
    tokens_to_label_plot_wn = set()
    if not df_paths_wn.empty:
        df_paths_wn_sorted = df_paths_wn.sort_values(by='curvature_metric', ascending=False)
        paths_to_include = pd.concat([df_paths_wn_sorted.head(2), df_paths_wn_sorted.tail(2)]) # Most & least curved
        for _, row in paths_to_include.iterrows():
            if row['parent'] in final_m_coords_wn_map: tokens_to_label_plot_wn.add(row['parent'])
            if row['child'] in final_m_coords_wn_map: tokens_to_label_plot_wn.add(row['child'])
    if len(tokens_to_label_plot_wn) < 7: # Ensure some labels if few paths chosen
         tokens_to_label_plot_wn.update(list(df_analysis_wn.sort_values(by='total_degree', ascending=False)['token'].head(max(0,7-len(tokens_to_label_plot_wn)))))
    if not tokens_to_label_plot_wn and tokens_wn: tokens_to_label_plot_wn.add(tokens_wn[0]) # At least one label


    plot_m_embedding_paths_len_curve(
        final_m_coords_wn_map,
        df_paths_wn, # DataFrame now contains path_length_m and curvature_metric
        tokens_to_label_plot_wn,
        node_color_feature_df=df_analysis_wn,
        node_color_by='total_degree', # Example: color nodes by total degree
        title="WordNet Manifold ($m^*$): Paths Colored by Curvature, Thickness by M-Path Length"
    )
else:
    print("Skipping enhanced path visualization for WordNet due to missing data.")

# --- Further Scatter Plots for Hypotheses ---
if not df_paths_analysis_wn.empty:
    plt.figure(figsize=(18, 6))

    plt.subplot(1, 3, 1)

    sns.scatterplot(data=df_paths_analysis_wn, x='avg_degree_of_pair', y='path_length_m')
    sns.regplot(data=df_paths_analysis_wn, x='avg_degree_of_pair', y='path_length_m', scatter=False, color='green', order=2)
    plt.title('M-Path Length vs. Avg Degree')
    plt.xlabel('Average Total Degree'); plt.ylabel('Path Length on M')
    plt.grid(True, linestyle=':', alpha=0.7)

    plt.subplot(1, 3, 2)
    sns.scatterplot(data=df_paths_analysis_wn, x='spatial_deviation_m', y='path_length_m')
    sns.regplot(data=df_paths_analysis_wn, x='spatial_deviation_m', y='path_length_m', scatter=False, color='purple')
    plt.title('M-Path Length vs. Spatial Deviation')
    plt.xlabel('Path Spatial Deviation (Curvature)'); plt.ylabel('Path Length on M')
    plt.grid(True, linestyle=':', alpha=0.7)

    plt.subplot(1, 3, 3)
    # Example: Path length vs. average g_tt of parent and child
    df_paths_analysis_wn['avg_g_tt_of_pair'] = (df_paths_analysis_wn['parent_g_tt'] + df_paths_analysis_wn['child_g_tt']) / 2.0
    sns.scatterplot(data=df_paths_analysis_wn, x='avg_g_tt_of_pair', y='path_length_m')
    sns.regplot(data=df_paths_analysis_wn, x='avg_g_tt_of_pair', y='path_length_m', scatter=False, color='orange')
    plt.title('M-Path Length vs. Avg $g_{tt}$ of Pair')
    plt.xlabel('Average $g_{tt}$ of Parent and Child'); plt.ylabel('Path Length on M')
    plt.grid(True, linestyle=':', alpha=0.7)

    plt.tight_layout()
    plt.show()

    if len(df_paths_analysis_wn) > 1:
        print("\nCorrelations with M-Path Length:")
        print(f"  vs. Avg Degree: {df_paths_analysis_wn['path_length_m'].corr(df_paths_analysis_wn['avg_degree_of_pair']):.4f}")
        print(f"  vs. Spatial Deviation: {df_paths_analysis_wn['path_length_m'].corr(df_paths_analysis_wn['spatial_deviation_m']):.4f}")
        print(f"  vs. Avg g_tt of Pair: {df_paths_analysis_wn['path_length_m'].corr(df_paths_analysis_wn['avg_g_tt_of_pair']):.4f}")

# ... (previous imports and helper functions) ...

def train_inn_phi_embedding_with_metric_reg_v5( # Renamed for clarity
    tokens, hierarchy_pairs,
    inn_phi_model,
    num_epochs=1000,
    learning_rate=5e-6, # Potentially even smaller
    epsilon1_target_sq=1e-6,
    causal_margin = 0.001,

    g_tt_target = -1.0,
    g_spatial_diag_target = 1.0,
    g_off_diag_target = 0.0,

    causal_loss_weight = 1.0,
    interval_loss_weight = 1.0,

    g_tt_mse_weight = 0.1,      # MSE push towards g_tt_target
    g_tt_negativity_penalty_weight = 10.0,  # INCREASED: Strong penalty for g_tt > g_tt_target (e.g. > -1)

    g_spatial_diag_mse_weight = 0.1,
    g_spatial_positivity_penalty_weight = 1.0, # Keep this strong

    g_off_diag_mse_weight = 0.01,

    gradient_clip_value = 0.5, # Potentially smaller clip value
    log_interval = 100,
    regularize_path_midpoints = False, # New flag: whether to regularize g_M on path midpoints
    num_midpoints_to_reg = 1 # Number of midpoints on path to regularize
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inn_phi_model.to(device)
    global eta_E_torch_for_loss
    eta_E_torch_for_loss = eta_E_torch_for_loss.to(device)

    # ... (initial_m_coords_map and initial_m_coords_tensors_map setup as before) ...
    initial_m_coords_map = {
        token_id: np.array([0.0] + np.random.uniform(-0.1, 0.1, inn_phi_model.input_dim - 1).tolist(), dtype=np.float32)
        for token_id in tokens
    }
    initial_m_coords_tensors_map = {
        name: torch.tensor(coords, dtype=torch.float32, device=device)
        for name, coords in initial_m_coords_map.items()
    }
    optimizer = optim.Adam(inn_phi_model.parameters(), lr=learning_rate)
    mse_loss_fn = nn.MSELoss()

    training_pairs_ids = []
    for child_id, parent_id in hierarchy_pairs:
        if parent_id is not None and child_id in initial_m_coords_tensors_map and parent_id in initial_m_coords_tensors_map:
            training_pairs_ids.append((child_id, parent_id))

    if not training_pairs_ids: # same abort logic
        print("No valid training pairs...")
        # ...
        final_e_coords_map_on_abort = {}
        with torch.no_grad():
            for name, m_tensor in initial_m_coords_tensors_map.items():
                 final_e_coords_map_on_abort[name] = inn_phi_model(m_tensor.unsqueeze(0)).squeeze(0).cpu().numpy()
        return initial_m_coords_map, final_e_coords_map_on_abort, inn_phi_model


    print(f"Starting training (v5) for {num_epochs} epochs on {device}...")
    # ... (print LR and weights) ...
    print(f"LR={learning_rate}, causal_w={causal_loss_weight}, interval_w={interval_loss_weight}")
    print(f"g_tt_mse_w={g_tt_mse_weight}, g_tt_neg_w={g_tt_negativity_penalty_weight}") # Updated name
    print(f"g_sp_diag_mse_w={g_spatial_diag_mse_weight}, g_sp_pos_w={g_spatial_positivity_penalty_weight}")
    print(f"g_off_diag_mse_w={g_off_diag_mse_weight}, clip={gradient_clip_value}, reg_midpoints={regularize_path_midpoints}")


    best_loss = float('inf')
    epochs_no_improve = 0
    patience = 100

    for epoch in range(num_epochs):
        inn_phi_model.train()
        epoch_losses_agg = defaultdict(list)

        for child_id, parent_id in training_pairs_ids:
            optimizer.zero_grad()

            m_coords_child_tensor_input = initial_m_coords_tensors_map[child_id]
            m_coords_parent_tensor_input = initial_m_coords_tensors_map[parent_id]

            e_coords_child = inn_phi_model(m_coords_child_tensor_input.unsqueeze(0)).squeeze(0)
            e_coords_parent = inn_phi_model(m_coords_parent_tensor_input.unsqueeze(0)).squeeze(0)

            delta_e_t = e_coords_child[0] - e_coords_parent[0]
            loss_causal_order = torch.relu(-(delta_e_t - causal_margin))
            delta_e_spatial_sq = torch.sum((e_coords_child[1:] - e_coords_parent[1:])**2)
            ds_sq_E = -delta_e_t**2 + delta_e_spatial_sq
            loss_interval = (ds_sq_E - (-epsilon1_target_sq))**2

            loss_metric_reg = torch.tensor(0.0, device=device)
            # Initialize per-component losses for this step to ensure they are always float/tensor
            current_g_tt_loss_val = 0.0
            current_g_spatial_diag_loss_val = 0.0
            current_g_spatial_pos_loss_val = 0.0
            current_g_off_diag_loss_val = 0.0

            # Points for metric regularization: parent, child, and optionally midpoints
            m_points_for_gM_reg = [m_coords_child_tensor_input, m_coords_parent_tensor_input]

            if regularize_path_midpoints:
                with torch.no_grad(): # Inverse pass doesn't need grad for this purpose
                    # Get a few m-space midpoints by inverting e-space midpoints
                    for k in range(1, num_midpoints_to_reg + 1):
                        t_interp = k / (num_midpoints_to_reg + 1)
                        e_mid = e_coords_parent + (e_coords_child - e_coords_parent) * t_interp
                        m_mid = inn_phi_model(e_mid.unsqueeze(0), reverse=True).squeeze(0)
                        m_points_for_gM_reg.append(m_mid)

            step_g_tt_loss_sum = torch.tensor(0.0, device=device)
            step_g_sp_diag_loss_sum = torch.tensor(0.0, device=device)
            step_g_sp_pos_loss_sum = torch.tensor(0.0, device=device)
            step_g_off_diag_loss_sum = torch.tensor(0.0, device=device)

            for m_point_tensor in m_points_for_gM_reg:
                g_M = get_g_M_for_loss(m_point_tensor, inn_phi_model, eta_E_torch_for_loss)

                # g_tt regularization
                g_tt_val = g_M[0,0]
                loss_g_tt_m_pt = mse_loss_fn(g_tt_val, torch.tensor(g_tt_target, device=device))
                # Penalize if g_tt > g_tt_target (e.g., if g_tt = -0.5 and target is -1, penalty is relu(-0.5 - (-1)) = relu(0.5) = 0.5)
                # This will push g_tt to be *more negative* or equal to the target.
                loss_g_tt_s_pt = torch.relu(g_tt_val - torch.tensor(g_tt_target, device=device))
                step_g_tt_loss_sum += g_tt_mse_weight * loss_g_tt_m_pt + g_tt_negativity_penalty_weight * loss_g_tt_s_pt

                # Spatial diagonal regularization
                spatial_diagonals = torch.diag(g_M[1:,1:])
                target_spatial_diag_tensor = torch.full((inn_phi_model.input_dim - 1,), g_spatial_diag_target, device=device)
                loss_g_sp_diag_m_pt = mse_loss_fn(spatial_diagonals, target_spatial_diag_tensor)
                loss_g_sp_pos_pt = torch.sum(torch.relu(-spatial_diagonals)) # Penalize if any g_ii < 0
                step_g_sp_diag_loss_sum += g_spatial_diag_mse_weight * loss_g_sp_diag_m_pt
                step_g_sp_pos_loss_sum += g_spatial_positivity_penalty_weight * loss_g_sp_pos_pt

                # Off-diagonal regularization
                g_M_offdiag_pt = g_M.clone(); g_M_offdiag_pt.diagonal().fill_(0)
                loss_g_off_diag_m_pt = mse_loss_fn(g_M_offdiag_pt, torch.zeros_like(g_M_offdiag_pt))
                step_g_off_diag_loss_sum += g_off_diag_mse_weight * loss_g_off_diag_m_pt

            # Average per-point losses if midpoints were regularized
            num_reg_points = len(m_points_for_gM_reg)
            current_g_tt_loss_val = (step_g_tt_loss_sum / num_reg_points).item()
            current_g_spatial_diag_loss_val = (step_g_sp_diag_loss_sum / num_reg_points).item()
            current_g_spatial_pos_loss_val = (step_g_sp_pos_loss_sum / num_reg_points).item()
            current_g_off_diag_loss_val = (step_g_off_diag_loss_sum / num_reg_points).item()

            loss_metric_reg = (step_g_tt_loss_sum + step_g_sp_diag_loss_sum +
                               step_g_sp_pos_loss_sum + step_g_off_diag_loss_sum) / num_reg_points


            loss = (causal_loss_weight * loss_causal_order +
                    interval_loss_weight * loss_interval +
                    loss_metric_reg)

            loss.backward()
            if gradient_clip_value > 0:
                torch.nn.utils.clip_grad_norm_(inn_phi_model.parameters(), gradient_clip_value)
            optimizer.step()

            epoch_losses_agg['total'].append(loss.item())
            epoch_losses_agg['causal'].append(causal_loss_weight * loss_causal_order.item())
            epoch_losses_agg['interval'].append(interval_loss_weight * loss_interval.item())
            epoch_losses_agg['g_tt'].append(current_g_tt_loss_val)
            epoch_losses_agg['g_spatial_diag'].append(current_g_spatial_diag_loss_val)
            epoch_losses_agg['g_spatial_pos'].append(current_g_spatial_pos_loss_val)
            epoch_losses_agg['g_off_diag'].append(current_g_off_diag_loss_val)

        # ... (Averaging and printing logic as before, ensure to use the updated epoch_losses_agg keys) ...
        avg_total_loss = np.mean(epoch_losses_agg['total'])
        avg_causal = np.mean(epoch_losses_agg['causal'])
        avg_interval = np.mean(epoch_losses_agg['interval'])
        avg_g_tt = np.mean(epoch_losses_agg['g_tt'])
        avg_g_spatial_diag = np.mean(epoch_losses_agg['g_spatial_diag']) # MSE part for spatial diag
        avg_g_spatial_pos = np.mean(epoch_losses_agg['g_spatial_pos'])   # Positivity penalty for spatial diag
        avg_g_off_diag = np.mean(epoch_losses_agg['g_off_diag'])

        if epoch % log_interval == 0 or epoch == num_epochs -1 :
            print(f"Epoch {epoch+1}/{num_epochs}, Avg Total Loss: {avg_total_loss:.4f} "
                  f"(C: {avg_causal:.4f}, I: {avg_interval:.4f} | "
                  f"g_tt: {avg_g_tt:.4f}, g_sp_diag: {avg_g_spatial_diag:.4f}, g_sp_pos: {avg_g_spatial_pos:.4f}, g_off: {avg_g_off_diag:.4f})")

            if len(tokens)>0:
                with torch.no_grad():
                    sample_token_id_log = tokens[0]
                    sample_m_tensor_log = initial_m_coords_tensors_map[sample_token_id_log]
                    g_M_sample_log = get_g_M_for_loss(sample_m_tensor_log, inn_phi_model, eta_E_torch_for_loss)
                    print(f"  Sample g_M for '{sample_token_id_log}' (g_tt: {g_M_sample_log[0,0].item():.3f}, "
                          f"g_xx: {g_M_sample_log[1,1].item():.3f}, g_yy: {g_M_sample_log[2,2].item():.3f}, g_zz: {g_M_sample_log[3,3].item():.3f}, "
                          f"det: {torch.linalg.det(g_M_sample_log).item():.3e})")

        # ... (Early stopping logic as before) ...
        if avg_total_loss < best_loss:
            best_loss = avg_total_loss
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        if epochs_no_improve >= patience and epoch > num_epochs // 2 :
            print(f"Early stopping at epoch {epoch+1}.")
            break


    print("Training finished.")
    inn_phi_model.eval()

    final_e_coords_map = {}
    with torch.no_grad():
        for name, m_tensor in initial_m_coords_tensors_map.items():
             final_e_coords_map[name] = inn_phi_model(m_tensor.unsqueeze(0)).squeeze(0).cpu().numpy()

    return initial_m_coords_map, final_e_coords_map, inn_phi_model


# --- Example Usage with v5 Metric Regularization ---
# (Tokens and hierarchy pairs definition as before, e.g., tokens_inn_reg_v4, hierarchy_pairs_inn_reg_v4)
# Ensure tokens_inn_reg_v4 and hierarchy_pairs_inn_reg_v4 are defined.
if 'tokens_inn_reg_v4' not in locals(): # Basic check
    tokens_inn_reg_v4 = ['a','b','c']; hierarchy_pairs_inn_reg_v4 = [('b','a'),('c','b')]


inn_phi_network_reg_v5 = INNPhi(input_dim=4, hidden_dim=64, num_coupling_layers=6)

print("\n--- Training INN Phi Network with v5 Metric Regularization ---")
initial_m_coords_reg_v5, final_e_coords_reg_v5, trained_inn_phi_reg_v5 = train_inn_phi_embedding_with_metric_reg_v5(
    tokens_inn_reg_v4, hierarchy_pairs_inn_reg_v4, # Use existing token/pair lists
    inn_phi_network_reg_v5,
    num_epochs=1500,
    learning_rate=1e-5,
    epsilon1_target_sq=1e-6,
    causal_margin=0.001,

    g_tt_target = -1.0,
    g_spatial_diag_target = 1.0,
    g_off_diag_target = 0.0,

    causal_loss_weight = 1.0,
    interval_loss_weight = 1.0,

    g_tt_mse_weight = 0.1, # Moderate weight
    g_tt_negativity_penalty_weight = 1.0, # Stronger penalty for positive g_tt

    g_spatial_diag_mse_weight = 0.1, # Moderate weight
    g_spatial_positivity_penalty_weight = 1.0, # Strong penalty for negative g_ii

    g_off_diag_mse_weight = 0.01,

    gradient_clip_value = 0.5,
    log_interval = 100,
    regularize_path_midpoints = False # Set to True to try midpoint regularization (slower)
)

# --- Perform Analysis and Visualization with the new results (trained_inn_phi_reg_v5, final_e_coords_reg_v5) ---
# (Copy the analysis and plotting code from your previous response,
#  ensuring to use the `_v5` suffixed variables)
# e.g., calculate final_m_coords_from_inn_inverse_reg_v5
# then analyze_local_geometry for these points, then plot.

# For brevity, a snippet for analysis of 'universe' with v5 results:
if final_e_coords_reg_v5 and 'universe' in final_e_coords_reg_v5 and 'analyze_local_geometry' in locals():
    print("\n--- Post-Training Analysis (v5) for 'universe' ---")
    _device = next(trained_inn_phi_reg_v5.parameters()).device
    e_final_univ_np_v5 = final_e_coords_reg_v5['universe']
    e_final_univ_tensor_v5 = torch.tensor(e_final_univ_np_v5, dtype=torch.float32).to(_device)
    with torch.no_grad():
        m_star_univ_tensor_v5 = trained_inn_phi_reg_v5(e_final_univ_tensor_v5.unsqueeze(0), reverse=True).squeeze(0)
    m_star_univ_np_v5 = m_star_univ_tensor_v5.cpu().numpy()

    univ_geo_features_v5 = analyze_local_geometry(m_star_univ_np_v5, trained_inn_phi_reg_v5, eta_E_torch_for_loss.to(_device))
    print(f"g_M at 'universe' (final m*): \n{univ_geo_features_v5['g_M'].round(3)}")
    print(f"det(g_M): {univ_geo_features_v5['det_g_M']:.3e}")
    print(f"g_tt: {univ_geo_features_v5['g_tt']:.3f}")

import numpy as np
import pandas as pd
import torch
import torch.autograd.functional as F
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from collections import defaultdict
import seaborn as sns

# --- Ensure necessary variables and functions are defined from previous cells ---
# trained_inn_phi_wn, final_e_coords_wn, final_m_coords_wn_map, hierarchy_pairs_wn,
# df_analysis_wn, calculate_spatial_path_deviation, analyze_local_geometry
# eta_E_torch (should be defined and on the correct device as the model)

# For demonstration, if the full WordNet setup wasn't run, create minimal dummy data
if 'trained_inn_phi_wn' not in locals() or \
   'final_m_coords_wn_map' not in locals() or \
   'df_analysis_wn' not in locals() or df_analysis_wn.empty or \
   'hierarchy_pairs_wn' not in locals() or \
   'final_e_coords_wn' not in locals():

    print("Warning: One or more required WordNet variables not found. Using dummy data for plotting.")
    class DummyPhi(torch.nn.Module):
        def __init__(self): super().__init__(); self.fc = torch.nn.Linear(4,4)
        def forward(self, x, reverse=False): return self.fc(x) + 0.1 * torch.sin(x) if not reverse else x - 0.1*torch.sin(x)
    trained_inn_phi_wn = DummyPhi()
    device_wn_analysis = torch.device("cpu") # ensure device consistency for dummy
    trained_inn_phi_wn.to(device_wn_analysis)
    eta_E_torch = torch.tensor(np.diag([-1.0, 1.0, 1.0, 1.0]), dtype=torch.float32).to(device_wn_analysis)


    _tokens_dummy_wn = ['mammal', 'placental', 'carnivore', 'canine', 'dog', 'cat', 'rodent']
    final_e_coords_wn = {
        t: np.array([i*0.05] + (np.random.rand(3) * 0.2 - 0.1).tolist())
        for i, t in enumerate(_tokens_dummy_wn)
    }
    with torch.no_grad():
        final_m_coords_wn_map = {
            k: trained_inn_phi_wn(torch.tensor(v, dtype=torch.float32).unsqueeze(0).to(device_wn_analysis), reverse=True).squeeze(0).cpu().numpy()
            for k,v in final_e_coords_wn.items()
        }
    if 'hierarchy_pairs_wn' not in locals() or not hierarchy_pairs_wn:
        hierarchy_pairs_wn = [('placental','mammal'), ('carnivore','placental'), ('canine','carnivore'),
                              ('dog','canine'), ('cat','placental'), ('rodent', 'placental')]
    if 'df_analysis_wn' not in locals() or df_analysis_wn.empty:
        _dummy_analysis_data = []
        for t in _tokens_dummy_wn:
            # Dummy g_M components for analyze_local_geometry to work
            _g_M_dummy = np.diag([-0.9, 0.9, 0.9, 0.9]) + (np.random.rand(4,4)-0.5)*0.1
            _dummy_analysis_data.append({
                'token': t, 'm_t': final_m_coords_wn_map[t][0], 'm_x': final_m_coords_wn_map[t][1],
                'm_y': final_m_coords_wn_map[t][2], 'm_z': final_m_coords_wn_map[t][3],
                'g_tt': _g_M_dummy[0,0], 'g_xx': _g_M_dummy[1,1],
                'g_yy': _g_M_dummy[2,2], 'g_zz': _g_M_dummy[3,3],
                'avg_g_spatial_diag': np.mean(np.diag(_g_M_dummy[1:,1:])),
                'det_g_M': -0.9,
                'time_space_mix': np.sum(np.abs(_g_M_dummy[0,1:])),
                'depth': np.random.randint(0,3),
                'total_degree': np.random.randint(1,4)
            })
        df_analysis_wn = pd.DataFrame(_dummy_analysis_data)
    if 'df_paths_wn' not in locals() or df_paths_wn.empty: # df_paths_wn should be recreated
        df_paths_wn = pd.DataFrame() # Ensure it's an empty df if not properly generated

# Ensure analyze_local_geometry is defined (copied from your previous good response)
def analyze_local_geometry(m_coord_np, phi_network_model, eta_E):
    m_coord_torch = torch.tensor(m_coord_np, dtype=torch.float32).to(eta_E.device)
    def phi_for_jacobian(m_input_tensor):
        return phi_network_model(m_input_tensor.unsqueeze(0), reverse=False).squeeze(0)
    J_phi = F.jacobian(phi_for_jacobian, m_coord_torch)
    g_M_tensor = J_phi.T @ eta_E @ J_phi
    g_M_np = g_M_tensor.detach().cpu().numpy()
    det_g_M = np.linalg.det(g_M_np)
    g_tt = g_M_np[0, 0]
    time_distortion_factor_val = np.sqrt(np.abs(g_tt))
    avg_spatial_scaling = np.mean(np.diag(g_M_np[1:, 1:]))
    spatial_submatrix = g_M_np[1:, 1:]
    det_g_spatial = np.linalg.det(spatial_submatrix)
    spatial_volume_factor = np.sqrt(np.abs(det_g_spatial)) if (det_g_spatial > 0 or not np.isclose(det_g_spatial,0)) else 0.0
    time_space_mixing = (np.sum(np.abs(g_M_np[0, 1:])) + np.sum(np.abs(g_M_np[1:, 0]))) / 2
    return {
        "g_M": g_M_np, "det_g_M": det_g_M, "g_tt": g_tt,
        "time_distortion_sqrt_abs_g_tt": time_distortion_factor_val,
        "avg_spatial_diagonal_scaling": avg_spatial_scaling,
        "spatial_volume_sqrt_det_g_spatial": spatial_volume_factor,
        "time_space_mixing_abs_sum_g0k": time_space_mixing
    }


print("\n--- Step 1: Calculating Path Lengths and Re-calculating Spatial Deviation ---")

def calculate_m_path_length_and_deviation(m_path_points_np, phi_model_for_gM, eta_E_tensor_device, path_name=""): # Added path_name for debug
    """
    Calculates the approximate proper length of a path on manifold M
    and its spatial deviation from a straight line in M-coordinates.
    """
    if len(m_path_points_np) < 2:
        return 0.0, 0.0

    path_length_m_approx = 0.0
    # print(f"\nAnalyzing path: {path_name}") # Debug
    for i in range(len(m_path_points_np) - 1):
        m_start_segment = m_path_points_np[i]
        m_end_segment = m_path_points_np[i+1]
        delta_m_segment = m_end_segment - m_start_segment

        m_midpoint_segment = (m_start_segment + m_end_segment) / 2.0
        geo_features_mid = analyze_local_geometry(m_midpoint_segment, phi_model_for_gM, eta_E_tensor_device)
        g_M_mid = geo_features_mid['g_M']

        ds_sq_segment_m = delta_m_segment @ g_M_mid @ delta_m_segment.T

        # --- DEBUG PRINT ---
        # if i < 5 or i > len(m_path_points_np) - 6 : # Print for first/last 5 segments
        #     print(f"  Path {path_name} - Segment {i}: ds_sq_M = {ds_sq_segment_m:.3e}, "
        #           f"delta_m_t={delta_m_segment[0]:.2e}, "
        #           f"g_tt_mid={g_M_mid[0,0]:.2f}, "
        #           f"g_xx_mid={g_M_mid[1,1]:.2f}, g_yy_mid={g_M_mid[2,2]:.2f}, g_zz_mid={g_M_mid[3,3]:.2f}, "
        #           f"g_tx_mid={g_M_mid[0,1]:.2f}")
        # --- END DEBUG ---

        if ds_sq_segment_m < -1e-9: # Timelike segment on M (add small tolerance for strict negativity)
            path_length_m_approx += np.sqrt(-ds_sq_segment_m)
        # else: # Spacelike or null on M
            # If you want to sum magnitudes for spacelike paths too for some notion of "length"
            # path_length_m_approx += np.sqrt(np.abs(ds_sq_segment_m))
            # However, for "proper time," only timelike contributions count.
            # If consistently spacelike, it's a strong indicator of metric signature issues.

    spatial_deviation = calculate_spatial_path_deviation(m_path_points_np)

    return path_length_m_approx, spatial_deviation


path_analysis_data_wn = []
num_path_interp_pts = 30 # For discretizing path
device_for_analysis = next(trained_inn_phi_wn.parameters()).device
eta_E_torch_device = eta_E_torch.to(device_for_analysis)


if final_e_coords_wn and final_m_coords_wn_map:
    print("Calculating path lengths and deviations for WordNet links...")
    for parent_id, child_id in hierarchy_pairs_wn:
        if parent_id is None or child_id not in final_e_coords_wn or parent_id not in final_e_coords_wn:
            continue

        e_parent = torch.tensor(final_e_coords_wn[parent_id], dtype=torch.float32, device=device_for_analysis)
        e_child = torch.tensor(final_e_coords_wn[child_id], dtype=torch.float32, device=device_for_analysis)

        e_path_pts_tensor = torch.stack([e_parent + (e_child - e_parent) * t
                                         for t in np.linspace(0, 1, num_path_interp_pts)])
        with torch.no_grad():
            m_path_pts_tensor = trained_inn_phi_wn(e_path_pts_tensor, reverse=True)
        m_path_pts_np = m_path_pts_tensor.cpu().numpy()

        path_len_m, spatial_dev = calculate_m_path_length_and_deviation(m_path_pts_np, trained_inn_phi_wn, eta_E_torch_device)

        degree_parent = df_analysis_wn.set_index('token').loc[parent_id, 'total_degree'] if parent_id in df_analysis_wn.set_index('token').index else 0
        degree_child = df_analysis_wn.set_index('token').loc[child_id, 'total_degree'] if child_id in df_analysis_wn.set_index('token').index else 0

        path_analysis_data_wn.append({
            'parent': parent_id, 'child': child_id,
            'path_length_m': path_len_m,
            'spatial_deviation_m': spatial_dev, # This is our "curvature_metric"
            'avg_degree_of_pair': (degree_parent + degree_child) / 2.0,
            'max_degree_of_pair': max(degree_parent, degree_child),
            'parent_g_tt': df_analysis_wn.set_index('token').loc[parent_id, 'g_tt'] if parent_id in df_analysis_wn.set_index('token').index else np.nan,
            'child_g_tt': df_analysis_wn.set_index('token').loc[child_id, 'g_tt'] if child_id in df_analysis_wn.set_index('token').index else np.nan,
        })

    df_paths_analysis_wn = pd.DataFrame(path_analysis_data_wn)
    if not df_paths_analysis_wn.empty:
        print("\nPath Analysis Data for WordNet (first 5 rows):")
        print(df_paths_analysis_wn.head())

        # Update df_paths_wn if it was used by the plotting function, or use df_paths_analysis_wn
        df_paths_wn = df_paths_analysis_wn.rename(columns={'spatial_deviation_m': 'curvature_metric'})

    else:
        print("No path analysis data generated for WordNet.")
else:
    print("final_e_coords_wn or final_m_coords_wn_map is empty. Skipping path analysis.")

# --- 2. Enhanced Visualization: Paths colored by curvature, thickness by length ---

def plot_m_embedding_paths_len_curve(
    m_coords_map_plot,
    path_data_df, # DataFrame with parent, child, path_length_m, curvature_metric
    tokens_to_label,
    node_color_feature_df=None, # Optional: df_analysis_wn to color nodes by a feature
    node_color_by='depth',    # Feature name in node_color_feature_df
    title="Conceptual Manifold - Paths by Length & Curvature"
):
    fig = plt.figure(figsize=(16, 14))
    ax = fig.add_subplot(111, projection='3d')

    token_ids_plot = list(m_coords_map_plot.keys())
    points_plot = np.array([m_coords_map_plot[tid] for tid in token_ids_plot])

    # Node coloring
    node_feature_values = None
    if node_color_feature_df is not None and node_color_by in node_color_feature_df.columns:
        # Align feature values with the order of points_plot
        node_feature_values_series = node_color_feature_df.set_index('token')[node_color_by].reindex(token_ids_plot)
        node_feature_values = node_feature_values_series.values

    if node_feature_values is not None and not np.all(np.isnan(node_feature_values)):
        node_cmap = 'cool' if node_color_by == 'depth' else 'cividis'
        node_scatter = ax.scatter(points_plot[:, 1], points_plot[:, 2], points_plot[:, 0],
                                  c=node_feature_values, cmap=node_cmap, s=100, alpha=0.7, edgecolors='k', linewidth=0.5)
        fig.colorbar(node_scatter, label=f'Node: {node_color_by}', shrink=0.5, aspect=15, pad=0.15)
    else:
        ax.scatter(points_plot[:, 1], points_plot[:, 2], points_plot[:, 0], s=100, alpha=0.7, color='skyblue', edgecolors='k', linewidth=0.5)


    ax.set_xlabel('Conceptual X ($m_x$)')
    ax.set_ylabel('Conceptual Y ($m_y$)')
    ax.set_zlabel('Conceptual Time ($m_t$)')

    for i, txt in enumerate(token_ids_plot):
        if txt in tokens_to_label:
            ax.text(points_plot[i, 1], points_plot[i, 2], points_plot[i, 0], f' {txt}', size=9, zorder=10, color='black')

    # Path plotting
    if not path_data_df.empty and 'curvature_metric' in path_data_df.columns and 'path_length_m' in path_data_df.columns:
        curvatures = path_data_df['curvature_metric'].values
        lengths = path_data_df['path_length_m'].values

        # Normalize curvature for color
        norm_curvature = plt.Normalize(vmin=np.nanmin(curvatures) if not np.all(np.isnan(curvatures)) else 0,
                                       vmax=np.nanmax(curvatures) if not np.all(np.isnan(curvatures)) else 1)
        cmap_curvature = plt.cm.get_cmap('autumn_r')

        # Normalize length for linewidth (be careful with extreme values)
        min_len, max_len = np.nanmin(lengths), np.nanmax(lengths)
        if min_len == max_len : max_len = min_len + 1 # Avoid division by zero for norm

        num_path_interp_plot = 30
        path_device = next(trained_inn_phi_wn.parameters()).device

        for idx, row in path_data_df.iterrows():
            parent_id = row['parent']
            child_id = row['child']
            path_curv = row['curvature_metric']
            path_len = row['path_length_m']

            if parent_id in final_e_coords_wn and child_id in final_e_coords_wn:
                e_p = torch.tensor(final_e_coords_wn[parent_id], dtype=torch.float32, device=path_device)
                e_c = torch.tensor(final_e_coords_wn[child_id], dtype=torch.float32, device=path_device)

                e_path = torch.stack([e_p + (e_c - e_p) * t for t in np.linspace(0, 1, num_path_interp_plot)])
                with torch.no_grad():
                    m_path = trained_inn_phi_wn(e_path, reverse=True).cpu().numpy()

                # Scale linewidth: 0.5 to 3.0, for example
                linewidth = 0.5 + 2.5 * ( (path_len - min_len) / (max_len - min_len + 1e-9) ) if max_len > min_len else 1.5

                ax.plot(m_path[:, 1], m_path[:, 2], m_path[:, 0],
                        linewidth=max(0.5, min(linewidth, 3.5)), # Cap linewidth
                        alpha=0.8,
                        color=cmap_curvature(norm_curvature(path_curv)),
                        zorder=5)

        sm_curvature = plt.cm.ScalarMappable(cmap=cmap_curvature, norm=norm_curvature)
        sm_curvature.set_array([])
        fig.colorbar(sm_curvature, ax=ax, label='Path Spatial Deviation (Curvature)', shrink=0.5, aspect=15, pad=0.02)
        # Could add a pseudo-legend for linewidth if desired, but it's trickier

    ax.set_title(title, fontsize=16)
    plt.tight_layout()
    plt.show()

# --- Call the Enhanced Visualization ---
if final_m_coords_wn_map and final_e_coords_wn and not df_paths_wn.empty and not df_analysis_wn.empty:
    # Select tokens to label for clarity
    tokens_to_label_plot_wn = set()
    if not df_paths_wn.empty:
        df_paths_wn_sorted = df_paths_wn.sort_values(by='curvature_metric', ascending=False)
        paths_to_include = pd.concat([df_paths_wn_sorted.head(2), df_paths_wn_sorted.tail(2)]) # Most & least curved
        for _, row in paths_to_include.iterrows():
            if row['parent'] in final_m_coords_wn_map: tokens_to_label_plot_wn.add(row['parent'])
            if row['child'] in final_m_coords_wn_map: tokens_to_label_plot_wn.add(row['child'])
    if len(tokens_to_label_plot_wn) < 7: # Ensure some labels if few paths chosen
         tokens_to_label_plot_wn.update(list(df_analysis_wn.sort_values(by='total_degree', ascending=False)['token'].head(max(0,7-len(tokens_to_label_plot_wn)))))
    if not tokens_to_label_plot_wn and tokens_wn: tokens_to_label_plot_wn.add(tokens_wn[0]) # At least one label


    plot_m_embedding_paths_len_curve(
        final_m_coords_wn_map,
        df_paths_wn, # DataFrame now contains path_length_m and curvature_metric
        tokens_to_label_plot_wn,
        node_color_feature_df=df_analysis_wn,
        node_color_by='total_degree', # Example: color nodes by total degree
        title="WordNet Manifold ($m^*$): Paths Colored by Curvature, Thickness by M-Path Length"
    )
else:
    print("Skipping enhanced path visualization for WordNet due to missing data.")

# --- Further Scatter Plots for Hypotheses ---
if not df_paths_analysis_wn.empty:
    plt.figure(figsize=(18, 6))

    plt.subplot(1, 3, 1)

    sns.scatterplot(data=df_paths_analysis_wn, x='avg_degree_of_pair', y='path_length_m')
    sns.regplot(data=df_paths_analysis_wn, x='avg_degree_of_pair', y='path_length_m', scatter=False, color='green', order=2)
    plt.title('M-Path Length vs. Avg Degree')
    plt.xlabel('Average Total Degree'); plt.ylabel('Path Length on M')
    plt.grid(True, linestyle=':', alpha=0.7)

    plt.subplot(1, 3, 2)
    sns.scatterplot(data=df_paths_analysis_wn, x='spatial_deviation_m', y='path_length_m')
    sns.regplot(data=df_paths_analysis_wn, x='spatial_deviation_m', y='path_length_m', scatter=False, color='purple')
    plt.title('M-Path Length vs. Spatial Deviation')
    plt.xlabel('Path Spatial Deviation (Curvature)'); plt.ylabel('Path Length on M')
    plt.grid(True, linestyle=':', alpha=0.7)

    plt.subplot(1, 3, 3)
    # Example: Path length vs. average g_tt of parent and child
    df_paths_analysis_wn['avg_g_tt_of_pair'] = (df_paths_analysis_wn['parent_g_tt'] + df_paths_analysis_wn['child_g_tt']) / 2.0
    sns.scatterplot(data=df_paths_analysis_wn, x='avg_g_tt_of_pair', y='path_length_m')
    sns.regplot(data=df_paths_analysis_wn, x='avg_g_tt_of_pair', y='path_length_m', scatter=False, color='orange')
    plt.title('M-Path Length vs. Avg $g_{tt}$ of Pair')
    plt.xlabel('Average $g_{tt}$ of Parent and Child'); plt.ylabel('Path Length on M')
    plt.grid(True, linestyle=':', alpha=0.7)

    plt.tight_layout()
    plt.show()

    if len(df_paths_analysis_wn) > 1:
        print("\nCorrelations with M-Path Length:")
        print(f"  vs. Avg Degree: {df_paths_analysis_wn['path_length_m'].corr(df_paths_analysis_wn['avg_degree_of_pair']):.4f}")
        print(f"  vs. Spatial Deviation: {df_paths_analysis_wn['path_length_m'].corr(df_paths_analysis_wn['spatial_deviation_m']):.4f}")
        print(f"  vs. Avg g_tt of Pair: {df_paths_analysis_wn['path_length_m'].corr(df_paths_analysis_wn['avg_g_tt_of_pair']):.4f}")

# ======================
# BERT Latent Spacetime Demo Cell
# ======================
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from scipy.stats import gaussian_kde
import torch

# HuggingFace Transformers
from transformers import AutoTokenizer, AutoModel

# 1. Define some example sentences
sentences = [
    "Gravity curves the fabric of spacetime.",
    "Neural networks map inputs to latent spaces.",
    "Cats chase mice.",
    "Black holes warp geometry.",
    "Machine learning discovers structure.",
    "Earth orbits the Sun.",
    "The apple fell from the tree.",
    "A quick brown fox jumps over the lazy dog.",
    "Quantum mechanics predicts probabilities.",
    "Transformers attend to every token."
]

# 2. Extract BERT [CLS] embeddings
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")
inputs = tokenizer(sentences, return_tensors="pt", padding=True, truncation=True)
with torch.no_grad():
    outputs = model(**inputs)
# Use [CLS] token embedding for each sentence
embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # (batch, hidden_dim)

# 3. Reduce to 2D
pca = PCA(n_components=2)
emb2d = pca.fit_transform(embeddings)

# 4. Estimate "gravity" (potential) field using kernel density
kde = gaussian_kde(emb2d.T)
xmin, ymin = emb2d.min(axis=0) - 1
xmax, ymax = emb2d.max(axis=0) + 1
X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]
positions = np.vstack([X.ravel(), Y.ravel()])
density = kde(positions).reshape(X.shape)
potential = -np.log(density + 1e-9)  # gravity = negative log-density

# 5. Plot points and potential field
plt.figure(figsize=(8, 6))
plt.contourf(X, Y, potential, levels=30, cmap='Blues', alpha=0.7)
plt.scatter(emb2d[:,0], emb2d[:,1], color='red', s=60, label='Embeddings')
for i, s in enumerate(sentences):
    plt.text(emb2d[i,0]+0.02, emb2d[i,1]+0.02, f"{i+1}", fontsize=9)
plt.colorbar(label="Gravity Potential (âˆ’log density)")
plt.title("BERT Latent Space with Gravity Analogy")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()

# (Bonus) Show geodesic-like gradient descent from random points
def descend_path(start, steps=40, lr=0.15):
    pt = np.array(start, dtype=float)
    path = [pt.copy()]
    for _ in range(steps):
        # Simple gradient approximation
        ix = np.clip(((pt[0] - xmin) / (xmax-xmin) * 99).astype(int), 0, 99)
        iy = np.clip(((pt[1] - ymin) / (ymax-ymin) * 99).astype(int), 0, 99)
        # Numerical gradient
        g = np.zeros(2)
        eps = 1e-2
        for d in range(2):
            pt_f = pt.copy(); pt_f[d] += eps
            ix_f = np.clip(((pt_f[0] - xmin) / (xmax-xmin) * 99).astype(int), 0, 99)
            iy_f = np.clip(((pt_f[1] - ymin) / (ymax-ymin) * 99).astype(int), 0, 99)
            g[d] = (potential[ix_f, iy_f] - potential[ix, iy]) / eps
        pt -= lr * g
        path.append(pt.copy())
    return np.stack(path)

# Draw a few descent paths
np.random.seed(42)
for _ in range(4):
    start = np.random.uniform([xmin, ymin], [xmax, ymax])
    path = descend_path(start)
    plt.plot(path[:,0], path[:,1], 'k--', alpha=0.7)

plt.tight_layout()
plt.show()