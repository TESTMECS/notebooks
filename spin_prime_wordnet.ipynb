{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae092566",
   "metadata": {},
   "source": [
    "# Spinâ€‘Prime Encoding Demo ðŸŒŒðŸ”¢\n",
    "\n",
    "This Colabâ€‘ready notebook shows how to map **word vectors** into an **indivisible prime number** representation using a toy *spinor* twist encoding.\n",
    "\n",
    "**Pipeline**\n",
    "1. Load small GloVe vectors (50â€‘D)\n",
    "2. Select a handful of nouns & verbs from **WordNet**\n",
    "3. *Twistâ€‘encode* each vector (simulate SU(2) double cover)\n",
    "4. Map the vector norm â†’ nearest **prime** (indivisible magnitude key)\n",
    "5. Visualize the original vectors (PCAâ€‘2D) with prime labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (lightweight)\n",
    "!pip -q install gensim nltk sympy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6454b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim.downloader as api\n",
    "import nltk, math, hashlib\n",
    "from sympy import nextprime\n",
    "from sklearn.decomposition import PCA\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979fd4df",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f17cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twist_encode(vec: np.ndarray):\n",
    "    \"\"\"Return simulated spinor doubleâ€‘cover (v, âˆ’v).\"\"\"\n",
    "    return vec, -vec\n",
    "\n",
    "def encode_magnitude_to_prime(mag: float, scale: int = 10_000) -> int:\n",
    "    \"\"\"Quantize magnitude and map to nearest prime.\"\"\"\n",
    "    scaled = max(2, int(round(mag * scale)))\n",
    "    return int(nextprime(scaled))\n",
    "\n",
    "def spin_prime_encode(vec: np.ndarray):\n",
    "    spin_pos, spin_neg = twist_encode(vec)\n",
    "    prime_code = encode_magnitude_to_prime(np.linalg.norm(vec))\n",
    "    return spin_pos, spin_neg, prime_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b817f4",
   "metadata": {},
   "source": [
    "## Load word vectors & sample WordNet terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c75c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small 50â€‘dimensional GloVe model (~70â€¯MB, quick)\n",
    "model = api.load('glove-wiki-gigaword-50')\n",
    "\n",
    "# Choose 10 illustrative synset lemmas from WordNet\n",
    "sample_lemmas = ['cat', 'dog', 'car', 'vehicle', 'run', 'walk', 'music', 'art', 'computer', 'science']\n",
    "\n",
    "vecs = []\n",
    "words = []\n",
    "for w in sample_lemmas:\n",
    "    if w in model:\n",
    "        words.append(w)\n",
    "        vecs.append(model[w])\n",
    "\n",
    "vecs = np.stack(vecs)\n",
    "print(f\"Loaded {len(vecs)} word vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a45358",
   "metadata": {},
   "source": [
    "## Spinâ€‘Prime encode each vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebc37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for word, vec in zip(words, vecs):\n",
    "    spin_pos, spin_neg, prime_code = spin_prime_encode(vec)\n",
    "    records.append({\n",
    "        'word': word,\n",
    "        'prime': prime_code,\n",
    "        'norm': np.linalg.norm(vec),\n",
    "        'spin_pos_head': spin_pos[:5],  # preview first 5 dims\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2fe71",
   "metadata": {},
   "source": [
    "## Visualize in 2â€‘D PCA with Prime Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b278b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(vecs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "for (x, y), word, prime in zip(coords, words, df['prime']):\n",
    "    ax.scatter(x, y, s=60)\n",
    "    ax.text(x+0.02, y+0.02, f\"{word}\\n{prime}\", fontsize=9)\n",
    "\n",
    "ax.set_xlabel('PCAâ€‘1')\n",
    "ax.set_ylabel('PCAâ€‘2')\n",
    "ax.set_title('Word Vectors â†’ Spinâ€‘Prime Encoding')\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e492bf6b",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "* Try bigger models (e.g. word2vecâ€‘300D)\n",
    "* Explore **Gaussian primes** or multiâ€‘prime tuples for richer encodings\n",
    "* Investigate whether spin consistency helps analogy tasks\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
